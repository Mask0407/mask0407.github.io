<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://mask0407.github.io</id>
    <title>个人博客</title>
    <updated>2020-07-15T09:41:46.406Z</updated>
    <generator>https://github.com/jpmonette/feed</generator>
    <link rel="alternate" href="https://mask0407.github.io"/>
    <link rel="self" href="https://mask0407.github.io/atom.xml"/>
    <subtitle>温故而知新</subtitle>
    <logo>https://mask0407.github.io/images/avatar.png</logo>
    <icon>https://mask0407.github.io/favicon.ico</icon>
    <rights>All rights reserved 2020, 个人博客</rights>
    <entry>
        <title type="html"><![CDATA[40行代码换1000文章阅读量？]]></title>
        <id>https://mask0407.github.io/40-xing-dai-ma-huan-1000-wen-zhang-yue-du-liang/</id>
        <link href="https://mask0407.github.io/40-xing-dai-ma-huan-1000-wen-zhang-yue-du-liang/">
        </link>
        <updated>2020-07-15T09:39:15.000Z</updated>
        <content type="html"><![CDATA[<h2 id="前言">前言</h2>
<ol>
<li>这篇文章虽说可以教你刷文章阅读量，但是仅供学习测试使用。</li>
<li>不要拿来恶意刷CSDN访问量，账号出问题概不负责。</li>
</ol>
<h2 id="代码">代码</h2>
<p>话不多说直接上Python代码，代码很简单，注释都写在了代码里。</p>
<pre><code class="language-python"># coding:utf-8
import re
import time

import requests

payload = &quot;&quot;
# 请求头
headers = {
    &quot;Accept&quot;: &quot;*/*&quot;,
    &quot;Accept-Encoding&quot;: &quot;gzip, deflate, br&quot;,
    &quot;Accept-Language&quot;: &quot;zh-CN,zh;q=0.8,en-US;q=0.5,en;q=0.3&quot;,
    &quot;Cookie&quot;: &quot;l=AurqcPuigwQdnQv7WvAfCoR1OlrRQW7h; isg=BHp6mNB79CHqYXpVEiRteXyyyKNcg8YEwjgLqoRvCI3ddxqxbLtOFUBGwwOrZ3ad; thw=cn; cna=VsJQERAypn0CATrXFEIahcz8; t=0eed37629fe7ef5ec0b8ecb6cd3a3577; tracknick=tb830309_22; _cc_=UtASsssmfA%3D%3D; tg=0; ubn=p; ucn=unzbyun; x=e%3D1%26p%3D*%26s%3D0%26c%3D0%26f%3D0%26g%3D0%26t%3D0%26__ll%3D-1%26_ato%3D0; miid=981798063989731689; hng=CN%7Czh-CN%7CCNY%7C156; um=0712F33290AB8A6D01951C8161A2DF2CDC7C5278664EE3E02F8F6195B27229B88A7470FD7B89F7FACD43AD3E795C914CC2A8BEB1FA88729A3A74257D8EE4FBBC; enc=1UeyOeN0l7Fkx0yPu7l6BuiPkT%2BdSxE0EqUM26jcSMdi1LtYaZbjQCMj5dKU3P0qfGwJn8QqYXc6oJugH%2FhFRA%3D%3D; ali_ab=58.215.20.66.1516409089271.6; mt=ci%3D-1_1; cookie2=104f8fc9c13eb24c296768a50cabdd6e; _tb_token_=ee7e1e1e7dbe7; v=0&quot;,
    &quot;User-Agent&quot;: &quot;Mozilla/5.0 (Windows NT 10.0; Win64; x64;` rv:47.0) Gecko/20100101 Firefox/47.0&quot;
}

# 获得文章列表urls
def getUrls(url):

    resp = requests.request(&quot;GET&quot;, url, data=payload, headers=headers)
    resp.encoding=resp.apparent_encoding
    html_source = resp.text
    # 正则表达式，取出主页中的博客链接
    urls = re.findall(&quot;https://[^&gt;\&quot;;\']*\d&quot;,html_source)
    new_urls=[]
    for url in urls:
        if 'details' in url:
            if url not in new_urls:
                new_urls.append(url)
    return new_urls

# 通过博客首页url获取所有文章url集合(这里换上自己博客主页)
urls = getUrls(&quot;https://blog.csdn.net/M283592338&quot;)
i=0
while True:
    # 遍历集合并向该url发送get请求
    for url in urls:
        requests.request(&quot;GET&quot;, url, data=payload, headers=headers)
        print(url, &quot;request--OK！&quot;)
        print (&quot;第&quot;, i, &quot;次访问&quot;)
        i+=1
        # 访问后休息一段时间，不要连续发起求，某些网站短时间内多次请求记为1次
        time.sleep(6)
    # 请求完所有文章继续发起下一轮请求
    time.sleep(6)
</code></pre>
<h2 id="写在最后">写在最后</h2>
<p>这个代码刷1000次貌似就会被限流，所以大家控制好自己的欲望。</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Spark ML计算皮尔逊相似度案例]]></title>
        <id>https://mask0407.github.io/spark-ml-ji-suan-pi-er-xun-xiang-si-du-an-li/</id>
        <link href="https://mask0407.github.io/spark-ml-ji-suan-pi-er-xun-xiang-si-du-an-li/">
        </link>
        <updated>2020-07-15T09:38:54.000Z</updated>
        <content type="html"><![CDATA[<p><ul class="markdownIt-TOC">
<li>
<ul>
<li><a href="#%E5%8A%9F%E8%83%BD%E8%AF%B4%E6%98%8E">功能说明</a></li>
<li><a href="#%E6%95%B0%E6%8D%AE%E6%A0%BC%E5%BC%8F%E5%8F%8A%E8%AF%B4%E6%98%8E">数据格式及说明</a></li>
<li><a href="#%E8%AE%BE%E8%AE%A1%E6%80%9D%E8%B7%AF">设计思路</a>
<ul>
<li><a href="#spark-ml-%E8%AE%A1%E7%AE%97%E7%9A%AE%E5%B0%94%E9%80%8A%E7%9B%B8%E5%85%B3%E7%B3%BB%E6%95%B0">Spark ML 计算皮尔逊相关系数</a></li>
</ul>
</li>
<li><a href="#%E4%BB%A3%E7%A0%81scala">代码(Scala)</a></li>
</ul>
</li>
</ul>
(Spark ML 计算两用户间的皮尔逊相似度)</p>
<h2 id="功能说明">功能说明</h2>
<p>该程序根据不同用户对不同电影的评分情况，通过<code>Spark ML</code>中<code>Correlation.corr</code>函数计算用户之间的皮尔逊相关矩阵。</p>
<h2 id="数据格式及说明">数据格式及说明</h2>
<p>该数据为模拟数据。</p>
<pre><code>X   A:5.0,B:1.0,C:2.0,D:0
Y   A:3.0,B:1.0,C:2.0,D:3.00
</code></pre>
<blockquote>
<ol>
<li>其中X、Y分别代表两个用户</li>
<li>A-D代表电影名称，电影名后面代表该用户的评分</li>
</ol>
</blockquote>
<blockquote>
<p>注意：实际情况下数据并没有这么理想，所以本次测试仅供学习。</p>
</blockquote>
<h2 id="设计思路">设计思路</h2>
<h3 id="spark-ml-计算皮尔逊相关系数">Spark ML 计算皮尔逊相关系数</h3>
<p>Spark 计算皮尔逊相关系数使用的是<code>Correlation.corr()</code>函数，函数具体使用方法可参考之前博客。</p>
<ul>
<li>使用皮尔逊相关系数公式计算XY的相关系数，用来验证代码的正确性<br>
<img src="https://img-blog.csdnimg.cn/20200708174052648.png" alt="在这里插入图片描述" loading="lazy"><br>
<img src="https://img-blog.csdnimg.cn/20200708174224365.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L00yODM1OTIzMzg=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述" loading="lazy"></li>
</ul>
<h2 id="代码scala">代码(Scala)</h2>
<pre><code class="language-java">object Pearson {
	def main(args: Array[String]): Unit = {
		// 屏蔽日志
		Logger.getLogger(&quot;org.apache.spark&quot;).setLevel(Level.WARN)
		Logger.getLogger(&quot;org.apache.jetty.server&quot;).setLevel(Level.OFF)

		val spark = SparkSession
			.builder()
			.master(&quot;local[*]&quot;)
			.appName(Demo03.getClass.getName)
			.getOrCreate()

		import spark.implicits._

		// 将数据加载为DataFrame
		val data: DataFrame = spark.read.format(&quot;text&quot;).load(&quot;/Users/mashikang/IdeaProjects/spark-mllib/src/main/resources/input/test&quot;)

		/**
		 * 将数据清洗格式并创建表结构
 		 */
		val frame = data.select($&quot;value&quot;)
			.map(_.getAs[String](&quot;value&quot;))
			.rdd
			.flatMap(line =&gt; {
				val strs = line.split(&quot;\\s+&quot;)
				val movie = strs(1).split(&quot;,&quot;)
				val tuples = movie.map(m =&gt; {
					val strings = m.split(&quot;:&quot;)
					(strs(0), strings(0), strings(1).toDouble)
				})
				tuples.toList
			})
			.toDF(&quot;user&quot;, &quot;movie&quot;, &quot;score&quot;)

		// 展示清洗数据 并创建该表的视图
		println(&quot;清洗后数据：&quot;)
		frame.show()
		frame.createOrReplaceTempView(&quot;user&quot;)

		/**
		 * 将数据进行 行、列颠倒
		 */
		val sql =
			&quot;&quot;&quot;
			  |  select movie,
			  |  max(case user when 'X' then score else 0 end) as X,
			  |  max(case user when 'Y' then score else 0 end) as Y
			  |  from user group by movie
			&quot;&quot;&quot;.stripMargin

		println(&quot;行转列后：&quot;)
		spark.sql(sql).show()

		/**
		 * 将数据封装为向量
		 */
		val vectors = spark.sql(sql)
			.drop(&quot;movie&quot;)
			.map(row =&gt; {
				val list = new ListBuffer[Double]
				for (i &lt;- 0 until row.size) {
					list.+=(row.getDouble(i))
				}
				list.toArray
			})
			.rdd
			.map(Vectors.dense(_))

		// 将rdd中的向量放入Seq中
		val seq: Seq[linalg.Vector] = vectors.collect().toSeq
		val df = seq.map(Tuple1.apply).toDF(&quot;features&quot;)

		// 计算df features列的相关性
		val Row(coef: Matrix) = Correlation.corr(df, &quot;features&quot;).head
		println(s&quot;用户X和Y的Pearson相关矩阵为:\n $coef&quot;)

	}
}
</code></pre>
<ul>
<li>结果</li>
</ul>
<pre><code>清洗后数据：
+----+-----+-----+
|user|movie|score|
+----+-----+-----+
|   X|    A|  5.0|
|   X|    B|  1.0|
|   X|    C|  2.0|
|   X|    D|  0.0|
|   Y|    A|  3.0|
|   Y|    B|  1.0|
|   Y|    C|  2.0|
|   Y|    D|  3.0|
+----+-----+-----+

行转列后：
+-----+---+---+
|movie|  X|  Y|
+-----+---+---+
|    B|1.0|1.0|
|    D|0.0|3.0|
|    C|2.0|2.0|
|    A|5.0|3.0|
+-----+---+---+

数据封装为向量后：
[5.0,3.0]
[2.0,2.0]
[0.0,3.0]
[1.0,1.0]
20/07/08 17:47:39 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS
20/07/08 17:47:39 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS
用户X和Y的Pearson相关矩阵为:
 1.0                 0.3223291856101522  
0.3223291856101522  1.0                 
</code></pre>
<p>该程序打印为一个<code>2*2</code>的的相关矩阵，矩阵的元素是由元数据单个向量的长度决定的。矩阵<code>11</code>位置为X用户与X用户的相关系数，<code>22</code>位置为Y用户与Y用户的相关系数，<code>12</code>和<code>21</code>位置分别代表X、Y和Y、X的相关系数，明显可以看出X、Y的相关系数与我们上面算的基本一致。</p>
<p>如果我们的用户为三个，也就是说行转列并且封装为向量后，每个向量的长度为3，那么皮尔逊相关矩阵的大小就为<code>3*3</code>。</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Spark修改几行源码，解决Kafka数据积压]]></title>
        <id>https://mask0407.github.io/spark-xiu-gai-ji-xing-yuan-ma-jie-jue-kafka-shu-ju-ji-ya/</id>
        <link href="https://mask0407.github.io/spark-xiu-gai-ji-xing-yuan-ma-jie-jue-kafka-shu-ju-ji-ya/">
        </link>
        <updated>2020-07-15T09:38:33.000Z</updated>
        <content type="html"><![CDATA[<p><ul class="markdownIt-TOC">
<li>
<ul>
<li><a href="#%E5%AF%BC%E8%87%B4kafka%E6%95%B0%E6%8D%AE%E7%A7%AF%E5%8E%8B%E7%9A%84%E5%87%A0%E7%A7%8D%E6%83%85%E5%86%B5">导致Kafka数据积压的几种情况</a></li>
<li><a href="#spark-streaming%E6%B6%88%E8%B4%B9kafka%E7%9A%84%E6%96%B9%E5%BC%8F">Spark Streaming消费Kafka的方式</a></li>
<li><a href="#%E5%B8%B8%E8%A7%84%E8%A7%A3%E5%86%B3%E4%B8%8A%E8%BF%B0%E4%B8%89%E7%A7%8D%E5%9C%BA%E6%99%AF%E7%9A%84%E6%96%B9%E5%BC%8F">常规解决上述三种场景的方式</a>
<ul>
<li><a href="#%E9%92%88%E5%AF%B9%E7%AC%AC%E4%B8%80%E7%A7%8D%E7%94%B1%E4%BA%8E%E6%95%B0%E6%8D%AE%E9%87%8F%E8%BE%83%E5%A4%A7%E5%88%86%E5%8C%BA%E8%BE%83%E5%B0%8F%E7%9A%84%E6%83%85%E5%86%B5%E4%BA%A7%E7%94%9F%E6%95%B0%E6%8D%AE%E7%A7%AF%E5%8E%8B">针对第一种由于数据量较大分区较小的情况产生数据积压</a></li>
<li><a href="#%E9%92%88%E5%AF%B9%E7%AC%AC%E4%BA%8C%E7%A7%8D%E7%A8%8B%E5%BA%8F%E5%AE%95%E6%9C%BA%E5%AF%BC%E8%87%B4%E7%9A%84%E6%B6%88%E8%B4%B9%E6%BB%9E%E5%90%8E">针对第二种程序宕机导致的消费滞后</a></li>
<li><a href="#%E9%92%88%E5%AF%B9%E7%AC%AC%E4%B8%89%E7%A7%8Dkafka%E6%95%B0%E6%8D%AE%E5%88%86%E5%B8%83%E4%B8%8D%E5%9D%87%E5%8C%80%E7%9A%84%E6%83%85%E5%86%B5">针对第三种Kafka数据分布不均匀的情况</a></li>
</ul>
</li>
<li><a href="#%E9%80%9A%E8%BF%87%E4%BF%AE%E6%94%B9%E6%BA%90%E7%A0%81%E6%9D%A5%E8%A7%A3%E5%86%B3%E6%95%B0%E6%8D%AE%E7%A7%AF%E5%8E%8B">通过修改源码来解决数据积压</a>
<ul>
<li><a href="#spark%E5%A6%82%E4%BD%95%E7%A1%AE%E5%AE%9A%E5%88%86%E5%8C%BA%E6%95%B0">Spark如何确定分区数</a></li>
<li><a href="#%E4%BF%AE%E6%94%B9%E5%88%86%E5%8C%BA%E8%A7%84%E5%88%99">修改分区规则</a></li>
</ul>
</li>
</ul>
</li>
</ul>
(Spark 增加几行源代码，解决棘手Kafka消息堆积问题)</p>
<h2 id="导致kafka数据积压的几种情况">导致Kafka数据积压的几种情况</h2>
<ul>
<li>
<p>第一种情况，SparkStreaming 通过receivers(或者Direct方式)以生产者生产数据的速率接收数据。当<code>Batch procecing time &gt; batch interval</code> 的时候，也就是每个批次数据处理的时间要比SparkStreaming批处理间隔时间长；越来越多的数据被接收，但是数据的处理速度没有跟上，导致系统开会出现数据堆积，可能进一步导致Excutor端OOM问题而出现失败的情况。</p>
</li>
<li>
<p>第二种情况，就是你的流计算程序没有自动拉起脚本，或者是自动拉起脚本设计不合理，导致服务较长时间处于停止状态。</p>
</li>
<li>
<p>第三种情况，是由于kafka内数据自定义Key导致Kafka分区内数据分布不均匀。</p>
</li>
</ul>
<h2 id="spark-streaming消费kafka的方式">Spark Streaming消费Kafka的方式</h2>
<ol>
<li>基于Receiver的方式</li>
</ol>
<blockquote>
<p>这种方式使用Receiver来获取数据。Receiver是使用Kafka的高级Consumer API来实现的。receiver从Kafka中获取的数据都是存储在Spark Executor的内存中的（如果突然数据暴增，大量batch堆积，很容易出现内存溢出的问题），然后Spark Streaming启动的job会去处理那些数据。<br>
然而，在默认的配置下，这种方式可能会因为底层的失败而丢失数据。如果要启用高可靠机制，让数据零丢失，就必须启用Spark Streaming的预写日志机制（Write Ahead Log，WAL）。该机制会同步地将接收到的Kafka数据写入分布式文件系统（比如HDFS）上的预写日志中。所以，即使底层节点出现了失败，也可以使用预写日志中的数据进行恢复。</p>
</blockquote>
<p><img src="https://img-blog.csdnimg.cn/20200703161455757.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L00yODM1OTIzMzg=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述" loading="lazy">2. 基于Direct的方式</p>
<blockquote>
<p>这种新的直接方式，是在Spark 1.3中引入的，从而能够确保更加健壮的机制。替代掉使用Receiver来接收数据后，这种方式会周期性地查询Kafka，来获得每个<code>topic+partition的最新的offset</code>，从而定义每个batch的offset的范围。当处理数据的job启动时，就会使用Kafka的简单Consumer api来获取Kafka指定offset范围的数据。</p>
</blockquote>
<figure data-type="image" tabindex="1"><img src="https://img-blog.csdnimg.cn/20200703161605622.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L00yODM1OTIzMzg=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述" loading="lazy"></figure>
<h2 id="常规解决上述三种场景的方式">常规解决上述三种场景的方式</h2>
<h3 id="针对第一种由于数据量较大分区较小的情况产生数据积压">针对第一种由于数据量较大分区较小的情况产生数据积压</h3>
<blockquote>
<ol>
<li>最常见的就是开启Spark Streaming的反压制机制。</li>
<li>再有就是通过shuffle，在Spark消费阶段进行repartition</li>
<li>最根本的方法就是增加Kafka分区数</li>
</ol>
</blockquote>
<h3 id="针对第二种程序宕机导致的消费滞后">针对第二种程序宕机导致的消费滞后</h3>
<blockquote>
<ol>
<li>最常见的方式任务启动从最新的消费，历史数据采用离线修补。</li>
<li>还可以再次启动时加大资源，提升消费速度</li>
</ol>
</blockquote>
<h3 id="针对第三种kafka数据分布不均匀的情况">针对第三种Kafka数据分布不均匀的情况</h3>
<blockquote>
<ol>
<li>最常见的就是给Key增加随机后缀，尽可能让数据散列均匀避免数据倾斜</li>
</ol>
</blockquote>
<h2 id="通过修改源码来解决数据积压">通过修改源码来解决数据积压</h2>
<p>如果按常规的模式来解决这些问题，就太平凡了，根本不够骚。</p>
<h3 id="spark如何确定分区数">Spark如何确定分区数</h3>
<blockquote>
<p>Spark Streaming生产KafkaRDD-Rdd的分区数，完全可以是<code>大于kakfa分区数</code>的！</p>
</blockquote>
<p>其实，经常阅读源码应该了解，RDD的分区数，是由RDD的getPartitions函数决定。比如KafkaRDD的getPartitions方法实现如下：</p>
<pre><code class="language-java">val offsetRanges: Array[OffsetRange]

override def getPartitions: Array[Partition] = {
    offsetRanges.zipWithIndex.map { case (o, i) =&gt;
        new KafkaRDDPartition(i, o.topic, o.partition, o.fromOffset, o.untilOffset)
    }.toArray
  }
</code></pre>
<p>具体位置：<br>
KafkaRDD --&gt; getPartitions</p>
<blockquote>
<p>OffsetRange存储一个kafka分区元数据及其offset范围，然后进行map操作，转化为KafkaRDDPartition。实际上，我们可以在这里下手，将map改为flatmap，然后对offsetrange的范围进行拆分，但是这个会引发一个问题</p>
</blockquote>
<h3 id="修改分区规则">修改分区规则</h3>
<p>其实，我们可以在offsetRange生成的时候做下转换。位置是DirectKafkaInputDstream的compute方法。具体实现：<br>
<img src="https://img-blog.csdnimg.cn/20200703173916685.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L00yODM1OTIzMzg=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述" loading="lazy"></p>
<pre><code class="language-java">// 是否开启自动重分区分区
sparkConf.set(&quot;enable.auto.repartition&quot;,&quot;true&quot;)

// 避免不必要的重分区操作，增加个阈值，只有该批次要消费的kafka的分区内数据大于该阈值才进行拆分
sparkConf.set(&quot;per.partition.offsetrange.threshold&quot;,&quot;300&quot;)

// 拆分后，每个kafkardd 的分区数据量。
sparkConf.set(&quot;per.partition.after.partition.size&quot;,&quot;100&quot;)
</code></pre>
<p>然后，在DirectKafkaInputDstream里获取着三个配置</p>
<pre><code class="language-java">val repartitionStep = _ssc.conf.getInt(&quot;per.partition.offsetrange.size&quot;,1000)
val repartitionThreshold = _ssc.conf.getLong(&quot;per.partition.offsetrange.threshold&quot;,1000)
val enableRepartition = _ssc.conf.getBoolean(&quot;enable.auto.repartition&quot;,false)
</code></pre>
<p>对offsetRanges生成的过程进行改造，只需要增加几行源码即可</p>
<pre><code class="language-java">val offsetRanges = untilOffsets.flatMap{ case (tp, uo) =&gt;
  val fo = currentOffsets(tp)
  val delta = uo -fo
  if(enableRepartition&amp;&amp;(repartitionThreshold &lt; delta)){
    val offsets = fo to uo by repartitionStep
    offsets.map(each =&gt;{
      val tmpOffset = each + repartitionStep
      OffsetRange(tp.topic, tp.partition, each, Math.min(tmpOffset,uo))
    }).toList
  }else{
    Array(OffsetRange(tp.topic, tp.partition, fo, uo))
  }
}
</code></pre>
<p>修改后：<br>
<img src="https://img-blog.csdnimg.cn/20200703174018186.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L00yODM1OTIzMzg=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述" loading="lazy"></p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Spark MLlib机器学习 Pipelines]]></title>
        <id>https://mask0407.github.io/spark-mllib-ji-qi-xue-xi-pipelines/</id>
        <link href="https://mask0407.github.io/spark-mllib-ji-qi-xue-xi-pipelines/">
        </link>
        <updated>2020-07-15T09:38:12.000Z</updated>
        <content type="html"><![CDATA[<p><ul class="markdownIt-TOC">
<li><a href="#ml%E7%AE%A1%E9%81%93">ML管道</a>
<ul>
<li><a href="#%E7%AE%A1%E9%81%93%E7%9A%84%E4%B8%BB%E8%A6%81%E6%A6%82%E5%BF%B5">管道的主要概念</a></li>
<li><a href="#dataframe">DataFrame</a></li>
<li><a href="#pipeline-components%E7%AE%A1%E9%81%93%E7%BB%84%E4%BB%B6">Pipeline components(管道组件)</a>
<ul>
<li><a href="#transformers%E8%BD%AC%E6%8D%A2%E5%99%A8">Transformers(转换器)</a></li>
<li><a href="#estimators%E4%BC%B0%E7%AE%97%E5%99%A8">Estimators(估算器)</a></li>
<li><a href="#properties-of-pipeline-components%E7%AE%A1%E9%81%93%E7%BB%84%E4%BB%B6%E5%B1%9E%E6%80%A7">Properties of pipeline components(管道组件属性)</a></li>
</ul>
</li>
<li><a href="#pipeline%E7%AE%A1%E9%81%93">Pipeline(管道)</a>
<ul>
<li><a href="#%E5%B7%A5%E4%BD%9C%E6%B5%81%E7%A8%8B">工作流程</a></li>
<li><a href="#%E8%AF%A6%E7%BB%86">详细</a></li>
<li><a href="#%E5%8F%82%E6%95%B0">参数</a></li>
</ul>
</li>
<li><a href="#ml%E6%8C%81%E4%B9%85%E6%80%A7saving-and-loading-pipelines">ML持久性：Saving and Loading Pipelines</a>
<ul>
<li><a href="#%E6%8C%81%E4%B9%85%E6%80%A7%E7%9A%84%E5%90%91%E5%90%8E%E5%85%BC%E5%AE%B9">持久性的向后兼容</a></li>
</ul>
</li>
<li><a href="#%E4%BB%A3%E7%A0%81%E7%A4%BA%E4%BE%8B">代码示例</a>
<ul>
<li><a href="#estimator-transformer-and-param">Estimator, Transformer, and Param</a></li>
<li><a href="#pipeline">Pipeline</a></li>
</ul>
</li>
</ul>
</li>
</ul>
(Spark ML Pipelines)</p>
<h1 id="ml管道">ML管道</h1>
<h2 id="管道的主要概念">管道的主要概念</h2>
<p>MLlib对用于机器学习算法的API进行了标准化，从而使将多种算法组合到单个管道或工作流中变得更加容易。<br>
-<code>DataFrame</code>：此ML API使用DataFrameSpark SQL作为ML数据集，可以保存各种数据类型。例如，一个DataFrame可能有不同的列，用于存储文本，特征向量，真实标签和预测。</p>
<ul>
<li>
<p><code>Transformer</code>：一个Transformer是一种算法，其可以将一个DataFrame到另一个DataFrame。例如，ML模型是一种Transformer将DataFrame具有特征的a转换为DataFrame具有预测的a的模型。</p>
</li>
<li>
<p><code>Estimator</code>：An Estimator是一种算法，可以适合DataFrame产生Transformer。例如，学习算法是在上Estimator进行训练DataFrame并生成模型的算法。</p>
</li>
<li>
<p><code>Pipeline</code>：将Pipeline多个Transformer和链接Estimator在一起以指定ML工作流程。</p>
</li>
<li>
<p><code>Parameter</code>：所有Transformer和Estimator现在共享一个用于指定参数的通用API。</p>
</li>
</ul>
<h2 id="dataframe">DataFrame</h2>
<p>Machine learning可以应用于多种数据类型，例如矢量，文本，图像和结构化数据。该API采用Spark SQL中的DataFrame，以支持多种数据类型。</p>
<p>DataFrame支持许多基本类型和结构化类型。请参考Spark SQL数据类型参。除了Spark SQL指南中列出的类型之外，DataFrame还可使用ML Vector类型。</p>
<p>可以从常规RDD隐式或显式创建DataFrame。</p>
<p>命名DataFrame中的列。下面的代码示例使用诸如“文本”，“功能”和“标签”之类的名称。</p>
<h2 id="pipeline-components管道组件">Pipeline components(管道组件)</h2>
<h3 id="transformers转换器">Transformers(转换器)</h3>
<p><code>Transformers</code>是一种抽象，其中包括特征转换器和学习的模型。从技术上讲，Transformer实现了<code>transform()</code>方法，该方法通常通过附加一个或多个列将一个DataFrame转换为另一个DataFrame。例如：</p>
<ul>
<li>
<p>特征转换器可以获取一个DataFrame，读取一列（例如：文本），将其映射到一个新列（例如：特征向量），然后输出一个新的DataFrame并附加映射的列。</p>
</li>
<li>
<p>学习模型可能需要一个DataFrame，读取包含特征向量的列，预测每个特征向量的标签，然后输出带有预测标签的新DataFrame作为列添加。</p>
</li>
</ul>
<h3 id="estimators估算器">Estimators(估算器)</h3>
<p>一个Estimator抽象学习算法的概念或算法适合或数据串。从技术上讲，Estimator实现是一种方法fit()，该方法接收一个DataFrame并产生一个 Model，即一个Transformer。例如，学习算法（例如为LogisticRegression）Estimator和调用 fit()训练一个LogisticRegressionModel，即为Model，因此为Transformer。</p>
<h3 id="properties-of-pipeline-components管道组件属性">Properties of pipeline components(管道组件属性)</h3>
<p>Transformer.transform()和Estimator.fit()都是无状态的。将来，可通过替代概念来支持有状态算法。</p>
<p>每个Transformer或Estimator实例都有一个唯一的ID，该ID在指定参数中很有用。</p>
<h2 id="pipeline管道">Pipeline(管道)</h2>
<p>在machine learning中，通常需要运行一系列算法来处理数据并从中学习。例如，简单的文本文档处理工作流程可能包括几个阶段：</p>
<ol>
<li>将每个文档的文本拆分为单词。</li>
<li>将每个文档的单词转换成数字特征向量。</li>
<li>使用特征向量和标签学习预测模型。</li>
</ol>
<p>MLlib将这样的工作流表示为“Pipeline”，它由要按特定顺序运行的一系列<code>PipelineStages</code>（<code>Transformer和Estimator</code>）组成。</p>
<h3 id="工作流程">工作流程</h3>
<p>Pipeline被指定为阶段序列，每个阶段可以是一个<code>Transformer</code>或<code>Estimator</code>。这些阶段按顺序运行，并且输入DataFrame在通过每个阶段时都会进行转换。对于<code>Transformer</code>阶段，在DataFrame上调用<code>transform()方法</code>。对于<code>Estimator</code>阶段，调用<code>fit()</code>方法以生成一个Transformer（它将成为<code>PipelineModel</code>或已拟合<code>Pipeline</code>的一部分），并且在DataFrame上调用该<code>Transformer的transform()</code>方法。</p>
<p>下图为简单的文本文档工作流程管道的培训时间使用情况。<br>
<img src="https://img-blog.csdnimg.cn/20200702141955735.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L00yODM1OTIzMzg=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述" loading="lazy"><br>
上图的第一行代表Pipeline的三个阶段。前面两个蓝色区域(<code>Tokenizer</code>和<code>HashingTF</code>)为<code>Transformers</code>，第三个(<code>LogisticRegression</code>)是<code>Estimator</code>。第二行表示流经管道的数据，其中第一个表示DataFrames。<code>Pipeline.fit()</code>在原始DataFrame文件上调用此方法，原始文件包含原始文本文档和标签。该<code>Tokenizer.transform()</code>方法将原始文本文档拆分为单词，然后向添加带有单词的新列DataFrame。该<code>HashingTF.transform()</code>方法将words列转换为特征向量，并将带有这些向量的新列添加到DataFrame。现在，由于LogisticRegression为Estimator，因此Pipeline第一个调用<code>LogisticRegression.fit()</code>产生一个<code>LogisticRegressionModel</code>。如果管道中有更多Estimator，则在将DataFrame传递到下一阶段之前，将在DataFrame上调用<code>LogisticRegressionModel的transform()</code>方法。</p>
<p>当Pipeline只有Estimator，因此，运行Pipeline的fit()方法后，它会生成PipelineModel，它是一个Transformer。该PipelineModel在测试时使用，用法如下图。<br>
<img src="https://img-blog.csdnimg.cn/202007021453216.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L00yODM1OTIzMzg=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述" loading="lazy"><br>
在上图中，PipelineModel具有与原始Pipeline相同的阶段数，但是原始Pipeline中的所有Estimator都已变为Transformers。在测试数据集上调用<code>PipelineModel的transform()</code>方法时，数据将按顺序通过拟合的管道。每个阶段的<code>transform()</code>方法都会更新数据集，并将其传递到下一个阶段。</p>
<p>Pipelines 和 PipelineModels有助于确保训练和测试数据经过相同的特征处理步骤。</p>
<h3 id="详细">详细</h3>
<ul>
<li>
<p><code>DAG Pipeline</code>：Pipeline的阶段被指定为有序数组。此处给出的所有示例均适用于线性管道，即每个阶段使用前一阶段产生的数据的管道。只要数据流图形成<code>有向无环图</code>（DAG），就可以创建非线性管道。当前基于每个阶段的输入和输出列名称（通常指定为参数）隐式指定该图。如果管道形成DAG，则必须按拓扑顺序指定阶段。</p>
</li>
<li>
<p><code>运行时检查</code>：由于管道可以在具有各种类型的DataFrame上运行，因此它们不能使用编译时类型检查。Pipelines和PipelineModels会在实际运行Pipeline之前进行运行时检查。此类型检查使用DataFrame架构完成，该架构是对DataFrame中列的数据类型的描述。</p>
</li>
<li>
<p><code>唯一的Pipeline阶段</code>：管道的阶段应该是唯一的实例。例如，同一实例myHashingTF不应两次插入到管道中，因为管道阶段必须具有唯一的ID。但是，可以将不同的实例myHashingTF1和myHashingTF2（均为HashingTF类型）放置到同一管道中，因为将使用不同的ID创建不同的实例。</p>
</li>
</ul>
<h3 id="参数">参数</h3>
<p>MLlib Estimators和Transformers使用统一的API来指定参数。</p>
<p>参数是具有独立文件的命名参数。 ParamMap是一组（参数, 值）对。</p>
<p>将参数传递给算法的主要方法有两种：</p>
<ol>
<li>设置实例的参数。例如，如果lr是LogisticRegression的实例，则可以调用<code>lr.setMaxIter(10)</code>以使<code>lr.fit()</code>最多使用10次迭代。该API与<code>spark.mllib</code>软件包中使用的API相似。</li>
<li>将ParamMap传递给<code>fit()</code>或<code>transform()</code>。 ParamMap中的任何参数都将覆盖以前通过<code>setter</code>方法指定的参数。</li>
</ol>
<p>参数属于Estimators和Transformers的特定实例。例如，如果我们有两个LogisticRegression实例lr1和lr2，则可以使用指定的两个maxIter参数构建ParamMap：<code>ParamMap(lr1.maxIter-&gt; 10, lr2.maxIter-&gt; 20)</code>。如果Pipeline中有两个算法的<code>maxIter</code>参数，这种方法就很适用。</p>
<h2 id="ml持久性saving-and-loading-pipelines">ML持久性：Saving and Loading Pipelines</h2>
<p>ML 通常将模型或管道保存到磁盘以供以后使用。在<code>Spark 1.6</code>中，模型导入/导出功能已添加到管道API。从<code>Spark 2.3</code>开始，spark.ml和pyspark.ml中基于DataFrame的API已有完整介绍。</p>
<p>ML持久性适用于Scala，Java和Python。但是，R当前使用修改后的格式，因此保存在R中的模型只能重新加载到R中。R语言用户可等待后续官方修复。</p>
<h3 id="持久性的向后兼容">持久性的向后兼容</h3>
<p>通常，MLlib为ML持久性保持向后兼容性。也就是说，如果您将ML模型或管道保存在一个版本的Spark中，则应该能够将其重新加载并在以后的Spark版本中使用。但是，有极少数例外，如下所述。</p>
<p><code>模型持久性</code>：是否可以通过Y版本的Spark加载在Apache X版本中使用Apache Spark ML持久性保存的模型或管道？</p>
<ul>
<li><code>主要版本</code>：无保证，但尽力而为。</li>
<li><code>次要版本和补丁程序版本</code>：是，这些是向后兼容的。</li>
<li><code>关于格式的注意事项</code>：不能保证稳定的持久性格式，但是模型加载本身被设计为向后兼容。</li>
</ul>
<p><code>模型行为</code>：Spark版本X中的模型或管道在Spark版本Y中的行为是否相同？</p>
<ul>
<li><code>主要版本</code>：无保证，但尽力而为。</li>
<li><code>次要版本和修补程序版本</code>：除错误修复外，行为相同。</li>
</ul>
<p>对于模型持久性和模型行为，Spark版本发行说明中都会报告次要版本或修补程序版本中的所有重大更改。如果发行说明中未报告损坏，则应将其视为要修复的错误。</p>
<h2 id="代码示例">代码示例</h2>
<h3 id="estimator-transformer-and-param">Estimator, Transformer, and Param</h3>
<pre><code class="language-java">def main(args: Array[String]): Unit = {


		// 屏蔽日志
		Logger.getLogger(&quot;org.apache.spark&quot;).setLevel(Level.WARN)
		Logger.getLogger(&quot;org.apache.jetty.server&quot;).setLevel(Level.OFF)

		val spark = SparkSession
			.builder()
			.master(&quot;local[*]&quot;)
			.appName(Demo02.getClass.getName)
			.getOrCreate()

		// 从(标签, 特征)元组列表准备训练数据
		val training = spark.createDataFrame(Seq(
			(1.0, Vectors.dense(0.0, 1.1, 0.1)),
			(0.0, Vectors.dense(2.0, 1.0, -1.0)),
			(0.0, Vectors.dense(2.0, 1.3, 1.0)),
			(1.0, Vectors.dense(0.0, 1.2, -0.5))
		)).toDF(&quot;label&quot;, &quot;features&quot;)

		// 创建一个LogisticRegression实例。该实例是一个Estimator.
		val lr = new LogisticRegression()
		// 打印出参数，文档和任何默认值
		println(s&quot;LogisticRegression parameters:\n ${lr.explainParams()}\n&quot;)

		// 可以使用setter方法设置参数
		lr.setMaxIter(10)
			.setRegParam(0.01)

		// 了解LogisticRegression模型。这使用存储在lr中的参数
		val model1 = lr.fit(training)

		// 由于model1是模型(即Estimator生产的Transformer)
		// 我们可以查看它在fit()中使用的参数。
		// 打印参数(名称：值)对，其中名称是为此的唯一ID
		// LogisticRegression 实例.
		println(s&quot;Model 1 was fit using parameters: ${model1.parent.extractParamMap}&quot;)

		// 使用ParamMap指定参数,
		// 支持几种制定参数的方法.
		val paramMap = ParamMap(lr.maxIter -&gt; 20)
			.put(lr.maxIter, 30) //指定一个参数， 这样就会覆盖之前的maxIter.
			.put(lr.regParam -&gt; 0.1, lr.threshold -&gt; 0.55) // Specify multiple Params.

		// 也可以综合使用 ParamMaps.
		val paramMap2 = ParamMap(lr.probabilityCol -&gt; &quot;myProbability&quot;) // Change output column name.
		val paramMapCombined = paramMap ++ paramMap2

		// paramMapCombined 参数的新模型
		// paramMapCombined 会覆盖之前的所有参数.
		val model2 = lr.fit(training, paramMapCombined)
		println(s&quot;Model 2 was fit using parameters: ${model2.parent.extractParamMap}&quot;)

		// 测试数据.
		val test = spark.createDataFrame(Seq(
			(1.0, Vectors.dense(-1.0, 1.5, 1.3)),
			(0.0, Vectors.dense(3.0, 2.0, -0.1)),
			(1.0, Vectors.dense(0.0, 2.2, -1.5))
		)).toDF(&quot;label&quot;, &quot;features&quot;)

		// 使用Transformer.transform() 方法对数据进行预测.
		// LogisticRegression.transform 应用到'features'列.
		// model2.transform() 输出 'myProbability'
		// 因为我们之前已重命名了lr.probabilityCol参数，所以使用了'probability'列
		model2.transform(test)
			.select(&quot;features&quot;, &quot;label&quot;, &quot;myProbability&quot;, &quot;prediction&quot;)
			.collect()
			.foreach { case Row(features: Vector, label: Double, prob: Vector, prediction: Double) =&gt;
				println(s&quot;($features, $label) -&gt; prob=$prob, prediction=$prediction&quot;)
			}

	}
</code></pre>
<h3 id="pipeline">Pipeline</h3>
<pre><code class="language-java">def main(args: Array[String]): Unit = {
		// 屏蔽日志
		Logger.getLogger(&quot;org.apache.spark&quot;).setLevel(Level.WARN)
		Logger.getLogger(&quot;org.apache.jetty.server&quot;).setLevel(Level.OFF)

		val spark = SparkSession
			.builder()
			.master(&quot;local[*]&quot;)
			.appName(Demo02.getClass.getName)
			.getOrCreate()

		import org.apache.spark.ml.{Pipeline, PipelineModel}
		import org.apache.spark.ml.classification.LogisticRegression
		import org.apache.spark.ml.feature.{HashingTF, Tokenizer}
		import org.apache.spark.ml.linalg.Vector
		import org.apache.spark.sql.Row

		// 从 (id, text, label) 元组列表准备培训文档.
		val training = spark.createDataFrame(Seq(
			(0L, &quot;a b c d e spark&quot;, 1.0),
			(1L, &quot;b d&quot;, 0.0),
			(2L, &quot;spark f g h&quot;, 1.0),
			(3L, &quot;hadoop mapreduce&quot;, 0.0)
		)).toDF(&quot;id&quot;, &quot;text&quot;, &quot;label&quot;)

		// 配置ML管道，该管道包括三个阶段：tokenizer，HashingTF和lr
		val tokenizer = new Tokenizer()
			.setInputCol(&quot;text&quot;)
			.setOutputCol(&quot;words&quot;)
		val hashingTF = new HashingTF()
			.setNumFeatures(1000)
			.setInputCol(tokenizer.getOutputCol)
			.setOutputCol(&quot;features&quot;)
		val lr = new LogisticRegression()
			.setMaxIter(10)
			.setRegParam(0.001)
		val pipeline = new Pipeline()
			.setStages(Array(tokenizer, hashingTF, lr))

		// 使pipeline适合培训文档
		val model = pipeline.fit(training)

		// 选择将已拟合的管道保存到磁盘
		model.write.overwrite().save(&quot;/Users/mashikang/IdeaProjects/spark-mllib/src/main/resources/spark-logistic-regression-model&quot;)

		// 将不合适的管道保存到磁盘
		pipeline.write.overwrite().save(&quot;/Users/mashikang/IdeaProjects/spark-mllib/src/main/resources/unfit-lr-model&quot;)

		// 在生产期间将其加载回
		val sameModel = PipelineModel.load(&quot;/Users/mashikang/IdeaProjects/spark-mllib/src/main/resources/spark-logistic-regression-model&quot;)

		// 准备未标记（id，text）元组的测试文档
		val test = spark.createDataFrame(Seq(
			(4L, &quot;spark i j k&quot;),
			(5L, &quot;l m n&quot;),
			(6L, &quot;spark hadoop spark&quot;),
			(7L, &quot;apache hadoop&quot;)
		)).toDF(&quot;id&quot;, &quot;text&quot;)

		// 对测试文件进行预测
		model.transform(test)
			.select(&quot;id&quot;, &quot;text&quot;, &quot;probability&quot;, &quot;prediction&quot;)
			.collect()
			.foreach { case Row(id: Long, text: String, prob: Vector, prediction: Double) =&gt;
				println(s&quot;($id, $text) --&gt; prob=$prob, prediction=$prediction&quot;)
			}
	}
</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hive安装(超详细)]]></title>
        <id>https://mask0407.github.io/hive-an-zhuang-chao-xiang-xi/</id>
        <link href="https://mask0407.github.io/hive-an-zhuang-chao-xiang-xi/">
        </link>
        <updated>2020-07-15T09:37:28.000Z</updated>
        <content type="html"><![CDATA[<!-- more -->
<p><ul class="markdownIt-TOC">
<li><a href="#%E5%89%8D%E6%8F%90%E7%8E%AF%E5%A2%83">前提环境</a></li>
<li><a href="#hive%E5%AE%89%E8%A3%85">Hive安装</a>
<ul>
<li><a href="#%E8%A7%A3%E5%8E%8B%E6%96%87%E4%BB%B6">解压文件</a></li>
<li><a href="#%E4%BF%AE%E6%94%B9%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6">修改配置文件</a></li>
<li><a href="#hdfs%E5%88%9B%E5%BB%BAhive%E5%B7%A5%E4%BD%9C%E7%A9%BA%E9%97%B4">HDFS创建Hive工作空间</a></li>
<li><a href="#%E9%85%8D%E7%BD%AEhive%E7%8E%AF%E5%A2%83%E5%8F%98%E9%87%8F">配置Hive环境变量</a></li>
</ul>
</li>
<li><a href="#hive%E5%90%AF%E5%8A%A8">Hive启动</a></li>
<li><a href="#%E6%9B%BF%E6%8D%A2hive%E5%8E%9F%E7%94%9Fmetastore">替换Hive原生MetaStore</a></li>
</ul>
(Hive安装)</p>
<h1 id="前提环境">前提环境</h1>
<blockquote>
<p>Linux基础设置以及Hadoop环境安装请看上一篇文章</p>
</blockquote>
<h1 id="hive安装">Hive安装</h1>
<h2 id="解压文件">解压文件</h2>
<pre><code class="language-shell">[root@localhost ~]# tar -zxvf apache-hive-1.2.2-bin.tar.gz -C /opt/install/
</code></pre>
<h2 id="修改配置文件">修改配置文件</h2>
<pre><code class="language-shell">[root@localhost apache-hive-1.2.2-bin]# cd /opt/install/apache-hive-1.2.2-bin/conf
</code></pre>
<ul>
<li>hive-env.sh<br>
拷贝创建hive-env.sh文件</li>
</ul>
<pre><code class="language-shell">[root@localhost conf]# cp hive-env.sh.template hive-env.sh
</code></pre>
<p>加入配置</p>
<pre><code class="language-xml">HADOOP_HOME=/opt/install/hadoop-2.9.2
export HIVE_CONF_DIR=/opt/install/apache-hive-1.2.2-bin/conf
</code></pre>
<figure data-type="image" tabindex="1"><img src="https://img-blog.csdnimg.cn/20200624175446767.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L00yODM1OTIzMzg=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述" loading="lazy"></figure>
<h2 id="hdfs创建hive工作空间">HDFS创建Hive工作空间</h2>
<ul>
<li>hdfs创建2个目录</li>
</ul>
<pre><code class="language-shell">       hadoop fs -mkdir /tmp
       hadoop fs -mkdir -p /user/hive/warehouse
</code></pre>
<ul>
<li>查看是否创建成功</li>
</ul>
<pre><code class="language-shell">[root@localhost conf]# hadoop fs -ls /
</code></pre>
<figure data-type="image" tabindex="2"><img src="https://img-blog.csdnimg.cn/20200624175742567.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L00yODM1OTIzMzg=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述" loading="lazy"></figure>
<h2 id="配置hive环境变量">配置Hive环境变量</h2>
<pre><code class="language-shell">[root@localhost apache-hive-1.2.2-bin]# vim /etc/profile

# 加入
export HIVE_HOME=/opt/install/apache-hive-1.2.2-bin
export PATH=$PATH:$JAVA_HOME/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$HIVE_HOME/bin

# 刷新
[root@localhost apache-hive-1.2.2-bin]# source /etc/profile
</code></pre>
<figure data-type="image" tabindex="3"><img src="https://img-blog.csdnimg.cn/20200624180016856.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L00yODM1OTIzMzg=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述" loading="lazy"></figure>
<h1 id="hive启动">Hive启动</h1>
<ul>
<li>进入Hive交互界面</li>
</ul>
<pre><code class="language-shell">hive
</code></pre>
<p><img src="https://img-blog.csdnimg.cn/20200624180120378.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L00yODM1OTIzMzg=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述" loading="lazy"><br>
hive基本操作指令参考之前博客</p>
<h1 id="替换hive原生metastore">替换Hive原生MetaStore</h1>
<p>未完待续。。。。</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hadoop 伪分布式搭建(超详细)]]></title>
        <id>https://mask0407.github.io/hadoop-wei-fen-bu-shi-da-jian-chao-xiang-xi/</id>
        <link href="https://mask0407.github.io/hadoop-wei-fen-bu-shi-da-jian-chao-xiang-xi/">
        </link>
        <updated>2020-07-15T09:36:39.000Z</updated>
        <content type="html"><![CDATA[<!-- more -->
<p><ul class="markdownIt-TOC">
<li><a href="#%E8%99%9A%E6%8B%9F%E6%9C%BA%E5%87%86%E5%A4%87%E9%98%B6%E6%AE%B5%E6%93%8D%E4%BD%9C">虚拟机准备阶段操作</a>
<ul>
<li><a href="#%E5%AE%89%E5%85%A8%E8%AE%BE%E7%BD%AE">安全设置</a>
<ul>
<li><a href="#%E9%98%B2%E7%81%AB%E5%A2%99%E7%9B%B8%E5%85%B3%E6%8C%87%E4%BB%A4">防火墙相关指令</a></li>
<li><a href="#%E5%85%B3%E9%97%AD%E5%85%B3%E9%97%ADselinux">关闭关闭selinux</a></li>
</ul>
</li>
<li><a href="#ip%E8%AE%BE%E7%BD%AE">IP设置</a>
<ul>
<li><a href="#%E6%9F%A5%E7%9C%8B%E6%9C%BA%E5%99%A8ip">查看机器IP</a></li>
<li><a href="#%E4%BF%AE%E6%94%B9%E4%B8%BB%E6%9C%BA%E5%90%8D">修改主机名</a></li>
<li><a href="#%E4%BF%AE%E6%94%B9ip%E5%8F%8A%E4%B8%BB%E6%9C%BA%E5%90%8D%E6%98%A0%E5%B0%84">修改IP及主机名映射</a></li>
</ul>
</li>
<li><a href="#ssh%E5%85%8D%E5%AF%86%E7%99%BB%E9%99%86">SSH免密登陆</a></li>
</ul>
</li>
<li><a href="#hadoop%E4%BC%AA%E5%88%86%E5%B8%83%E5%BC%8F%E6%90%AD%E5%BB%BA">Hadoop伪分布式搭建</a>
<ul>
<li><a href="#jdk%E9%85%8D%E7%BD%AE">JDK配置</a>
<ul>
<li><a href="#%E8%A7%A3%E5%8E%8B">解压</a></li>
<li><a href="#%E9%85%8D%E7%BD%AE%E7%8E%AF%E5%A2%83%E5%8F%98%E9%87%8F">配置环境变量</a></li>
</ul>
</li>
<li><a href="#hadoop%E9%85%8D%E7%BD%AE">Hadoop配置</a>
<ul>
<li><a href="#%E8%A7%A3%E5%8E%8B%E6%96%87%E4%BB%B6">解压文件</a></li>
<li><a href="#%E4%BF%AE%E6%94%B9%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6">修改配置文件</a></li>
<li><a href="#%E9%85%8D%E7%BD%AEhadoop%E7%8E%AF%E5%A2%83%E5%8F%98%E9%87%8F">配置Hadoop环境变量</a></li>
<li><a href="#%E9%AA%8C%E8%AF%81%E7%8E%AF%E5%A2%83%E5%8F%98%E9%87%8F%E6%98%AF%E5%90%A6%E9%85%8D%E7%BD%AE%E6%88%90%E5%8A%9F">验证环境变量是否配置成功</a></li>
<li><a href="#%E6%A0%BC%E5%BC%8F%E5%8C%96namenode">格式化NameNode</a></li>
<li><a href="#hadoop%E8%B5%B7%E5%81%9C%E5%91%BD%E4%BB%A4">Hadoop起停命令</a></li>
<li><a href="#%E6%9F%A5%E7%9C%8Bwebui%E7%95%8C%E9%9D%A2">查看WebUI界面</a></li>
</ul>
</li>
</ul>
</li>
</ul>
(Hadoop伪分布式搭建)</p>
<h1 id="虚拟机准备阶段操作">虚拟机准备阶段操作</h1>
<p>本文是基于CentOS 7 系统搭建<br>
相关资源下载<br>
链接:<a href="https://pan.baidu.com/s/1FW228OfyURxEgnXW0qqpmA">https://pan.baidu.com/s/1FW228OfyURxEgnXW0qqpmA</a>  密码:18uc</p>
<h2 id="安全设置">安全设置</h2>
<h3 id="防火墙相关指令">防火墙相关指令</h3>
<pre><code class="language-shell"># 查看防火墙状态
firewall-cmd --state

# 停止防火墙
[root@localhost ~]# systemctl stop firewalld.service

# 禁止防火墙开机自启
[root@localhost ~]# systemctl disable firewalld.service 
</code></pre>
<h3 id="关闭关闭selinux">关闭关闭selinux</h3>
<pre><code class="language-shell">[root@localhost ~]# vi /etc/selinux/config
</code></pre>
<p>将 <code>SELINUX=enforcing</code>改为 <code>SELINUX=disabled</code></p>
<h2 id="ip设置">IP设置</h2>
<h3 id="查看机器ip">查看机器IP</h3>
<pre><code class="language-shell">[root@localhost ~]# ifconfig
</code></pre>
<figure data-type="image" tabindex="1"><img src="https://img-blog.csdnimg.cn/20200624164231711.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L00yODM1OTIzMzg=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述" loading="lazy"></figure>
<h3 id="修改主机名">修改主机名</h3>
<pre><code class="language-shell">[root@localhost ~]# vi /etc/hostname 
</code></pre>
<figure data-type="image" tabindex="2"><img src="https://img-blog.csdnimg.cn/20200624164355142.png" alt="在这里插入图片描述" loading="lazy"></figure>
<h3 id="修改ip及主机名映射">修改IP及主机名映射</h3>
<pre><code class="language-shell">[root@localhost ~]# vi /etc/hosts
</code></pre>
<figure data-type="image" tabindex="3"><img src="https://img-blog.csdnimg.cn/20200624164559390.png" alt="在这里插入图片描述" loading="lazy"></figure>
<h2 id="ssh免密登陆">SSH免密登陆</h2>
<pre><code class="language-shell">[root@localhost ~]# ssh-keygen -t rsa # 生产密钥
# 连续三次回车


# 将密钥发送给需要登陆本机的机器，这里只有一台机器 所以发给自己
[root@localhost ~]# ssh-copy-id root@CentOS

# 测试ssh
[root@localhost ~]# ssh root@CentOS
</code></pre>
<h1 id="hadoop伪分布式搭建">Hadoop伪分布式搭建</h1>
<ul>
<li>创建 install文件夹</li>
</ul>
<pre><code class="language-shell">[root@localhost ~]# mkdir /opt/install/
</code></pre>
<h2 id="jdk配置">JDK配置</h2>
<p>这里选用JDK8</p>
<h3 id="解压">解压</h3>
<pre><code class="language-shell">[root@localhost ~]# tar -zxvf jdk-8u144-linux-x64.tar.gz -C /opt/install/
</code></pre>
<h3 id="配置环境变量">配置环境变量</h3>
<pre><code class="language-shell">[root@localhost jdk1.8.0_144]# vi /etc/profile
# 加入配置 加入位置如下图所示
export JAVA_HOME=/opt/install/jdk1.8.0_144
export PATH=$PATH:$JAVA_HOME/bin

# 保存后刷新环境变量
[root@localhost jdk1.8.0_144]# source /etc/profile
</code></pre>
<figure data-type="image" tabindex="4"><img src="https://img-blog.csdnimg.cn/20200624170246893.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L00yODM1OTIzMzg=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述" loading="lazy"></figure>
<pre><code class="language-shell"># 刷新完 执行命令验证JDK是否安装成功
[root@localhost jdk1.8.0_144]# java -version
</code></pre>
<p>成功界面<br>
<img src="https://img-blog.csdnimg.cn/20200624171407596.png" alt="在这里插入图片描述" loading="lazy"></p>
<h2 id="hadoop配置">Hadoop配置</h2>
<h3 id="解压文件">解压文件</h3>
<pre><code class="language-shell">[root@localhost ~]# tar -zxvf hadoop-2.9.2.tar.gz -C /opt/install/
</code></pre>
<h3 id="修改配置文件">修改配置文件</h3>
<pre><code class="language-shell">[root@localhost ~]# cd /opt/install/hadoop-2.9.2/etc/hadoop
</code></pre>
<ul>
<li>hadoop-env.sh</li>
</ul>
<pre><code class="language-xml">export JAVA_HOME=/opt/install/jdk1.8.0_144
</code></pre>
<figure data-type="image" tabindex="5"><img src="https://img-blog.csdnimg.cn/20200624171838878.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L00yODM1OTIzMzg=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述" loading="lazy"></figure>
<ul>
<li>core-site.xml</li>
</ul>
<pre><code class="language-xml">	   &lt;!--  用于设置namenode并且作为Java程序的访问入口  --&gt;
       &lt;property&gt;
            &lt;name&gt;fs.defaultFS&lt;/name&gt;
            &lt;value&gt;hdfs://CentOS:8020&lt;/value&gt;
       &lt;/property&gt;
       &lt;!--  存储NameNode持久化的数据，DataNode块数据  --&gt;
       &lt;!--  手工创建$HADOOP_HOME/data/tmp  --&gt;
       &lt;property&gt;
             &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;
             &lt;value&gt;/opt/install/hadoop-2.9.2/data/tmp&lt;/value&gt;
       &lt;/property&gt;
</code></pre>
<figure data-type="image" tabindex="6"><img src="https://img-blog.csdnimg.cn/2020062417215562.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L00yODM1OTIzMzg=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述" loading="lazy"></figure>
<ul>
<li>hdfs-site.xml</li>
</ul>
<pre><code class="language-xml"> &lt;!--  设置副本数量 默认是3 可自行根据需求更改  --&gt;
        &lt;property&gt;		
            &lt;name&gt;dfs.replication&lt;/name&gt;
            &lt;value&gt;3&lt;/value&gt;
        &lt;/property&gt;
&lt;!--  权限，可省略  --&gt;
         &lt;property&gt;
             &lt;name&gt;dfs.permissions.enabled&lt;/name&gt;
             &lt;value&gt;false&lt;/value&gt;
          &lt;/property&gt;
          &lt;property&gt;
              &lt;name&gt;dfs.namenode.http.address&lt;/name&gt;
              &lt;value&gt;CentOS:50070&lt;/value&gt;
           &lt;/property&gt;
</code></pre>
<figure data-type="image" tabindex="7"><img src="https://img-blog.csdnimg.cn/20200624172329411.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L00yODM1OTIzMzg=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述" loading="lazy"></figure>
<ul>
<li>mapred-site.xml<br>
首先拷贝一个mapred-site.xml</li>
</ul>
<pre><code class="language-shell">[root@localhost hadoop]# cp mapred-site.xml.template mapred-site.xml
</code></pre>
<pre><code class="language-xml">&lt;!--  yarn 与 MapReduce相关  --&gt;
       &lt;property&gt;	 	        		
            &lt;name&gt;mapreduce.framework.name&lt;/name&gt;
            &lt;value&gt;yarn&lt;/value&gt;
       &lt;/property&gt;
</code></pre>
<figure data-type="image" tabindex="8"><img src="https://img-blog.csdnimg.cn/20200624172533905.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L00yODM1OTIzMzg=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述" loading="lazy"></figure>
<ul>
<li>yarn-site.xml</li>
</ul>
<pre><code class="language-xml"> &lt;property&gt;
          &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;
          &lt;value&gt;mapreduce_shuffle&lt;/value&gt;
  &lt;/property&gt;
</code></pre>
<figure data-type="image" tabindex="9"><img src="https://img-blog.csdnimg.cn/20200624172643893.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L00yODM1OTIzMzg=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述" loading="lazy"></figure>
<ul>
<li>slaves<br>
这里配置DataNode的<code>主机名</code> <code>伪分布式</code>情况下这里<code>NameNode</code>也充当<code>DataNode</code></li>
</ul>
<pre><code class="language-xml">CentOS
</code></pre>
<figure data-type="image" tabindex="10"><img src="https://img-blog.csdnimg.cn/20200624172811202.png" alt="在这里插入图片描述" loading="lazy"></figure>
<h3 id="配置hadoop环境变量">配置Hadoop环境变量</h3>
<pre><code class="language-shell">[root@localhost hadoop-2.9.2]# vim /etc/profile
# 加入
export HADOOP_HOME=/opt/install/hadoop-2.9.2
export PATH=$PATH:$JAVA_HOME/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin
</code></pre>
<figure data-type="image" tabindex="11"><img src="https://img-blog.csdnimg.cn/20200624173026102.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L00yODM1OTIzMzg=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述" loading="lazy"></figure>
<pre><code class="language-shell"># 刷新环境变量
[root@localhost hadoop-2.9.2]# source /etc/profile
</code></pre>
<h3 id="验证环境变量是否配置成功">验证环境变量是否配置成功</h3>
<pre><code class="language-shell">[root@localhost hadoop-2.9.2]# hadoop version
</code></pre>
<figure data-type="image" tabindex="12"><img src="https://img-blog.csdnimg.cn/20200624173219432.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L00yODM1OTIzMzg=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述" loading="lazy"></figure>
<h3 id="格式化namenode">格式化NameNode</h3>
<p>目的作用：格式化hdfs系统，并且生成存储数据块的目录</p>
<pre><code class="language-shell">[root@localhost hadoop-2.9.2]# hadoop namenode -format 
</code></pre>
<p>格式化成功后如图显示<br>
<img src="https://img-blog.csdnimg.cn/20200624173431762.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L00yODM1OTIzMzg=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述" loading="lazy"></p>
<h3 id="hadoop起停命令">Hadoop起停命令</h3>
<pre><code class="language-shell">   start-all.sh
   stop-all.sh
</code></pre>
<p>启动成后 <code>jps</code>查看进程<br>
<img src="https://img-blog.csdnimg.cn/20200624173640904.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L00yODM1OTIzMzg=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述" loading="lazy"></p>
<h3 id="查看webui界面">查看WebUI界面</h3>
<p>http://CentOS:50070 访问 hdfs<br>
http://CentOS:8088 访问 yarn<br>
<img src="https://img-blog.csdnimg.cn/20200624174037209.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L00yODM1OTIzMzg=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述" loading="lazy"><br>
<img src="https://img-blog.csdnimg.cn/20200624174138121.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L00yODM1OTIzMzg=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述" loading="lazy"></p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[GeoHash算法原理及实现]]></title>
        <id>https://mask0407.github.io/suanfa/</id>
        <link href="https://mask0407.github.io/suanfa/">
        </link>
        <updated>2020-06-29T03:19:42.000Z</updated>
        <content type="html"><![CDATA[<p><ul class="markdownIt-TOC">
<li>
<ul>
<li><a href="#geohash%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86">GeoHash算法原理</a>
<ul>
<li><a href="#%E6%A0%B7%E4%BE%8B%E6%95%B0%E6%8D%AE%E5%9F%BA%E4%BA%8E15%E6%AC%A1%E5%8C%BA%E5%9F%9F%E5%88%86%E5%89%B2">样例数据（基于15次区域分割）</a></li>
<li><a href="#geohash%E7%AE%97%E6%B3%95%E6%80%9D%E6%83%B3">GeoHash算法思想</a></li>
<li><a href="#geohash%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86-2">GeoHash算法原理</a></li>
<li><a href="#%E5%90%8E%E7%BB%AD%E9%97%AE%E9%A2%98">后续问题</a></li>
</ul>
</li>
<li><a href="#geohash%E7%AE%97%E6%B3%95%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0">GeoHash算法代码实现</a></li>
<li><a href="#%E5%86%99%E5%9C%A8%E6%9C%80%E5%90%8E">写在最后</a></li>
</ul>
</li>
</ul>
(GeoHash原理与Java实现)</p>
<h2 id="geohash算法原理">GeoHash算法原理</h2>
<blockquote>
<p>GeoHash是目前比较主流实现位置服务的技术，Geohash算法将经纬度二维数据编码为一个字符串，本质是一个降维的过程</p>
</blockquote>
<h3 id="样例数据基于15次区域分割">样例数据（基于15次区域分割）</h3>
<table>
<thead>
<tr>
<th>位置</th>
<th>经纬度</th>
<th>Geohash</th>
</tr>
</thead>
<tbody>
<tr>
<td>北京站</td>
<td>116.433589,39.910508</td>
<td>wx4g19</td>
</tr>
<tr>
<td>天安门</td>
<td>116.403874,39.913884</td>
<td>wx4g0f</td>
</tr>
<tr>
<td>首都机场</td>
<td>116.606819,40.086109</td>
<td>wx4uj3</td>
</tr>
</tbody>
</table>
<h3 id="geohash算法思想">GeoHash算法思想</h3>
<p>我们知道，经度范围是东经180到西经180，纬度范围是南纬90到北纬90，我们设定西经为负，南纬为负，所以地球上的经度范围就是[-180， 180]，纬度范围就是[-90，90]。如果以本初子午线、赤道为界，地球可以分成4个部分。</p>
<p>GeoHash的思想就是将地球划分的四部分映射到二维坐标上。</p>
<blockquote>
<p>[-90˚,0˚)代表0，(0˚,90˚]代表1，[-180˚,0)代表0，(0˚,180˚]代表1<br>
映射到二维空间划分为四部分则如下图</p>
</blockquote>
<p><img src="https://img-blog.csdnimg.cn/20200519092805300.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L00yODM1OTIzMzg=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述" loading="lazy"><br>
但是这么粗略的划分没有什么意义，想要更精确的使用GeoHash就需要再进一步<code>二分切割</code><br>
<img src="https://img-blog.csdnimg.cn/20200519095313362.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L00yODM1OTIzMzg=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述" loading="lazy"><br>
通过上图可看出，进一步<code>二分切割</code>将原本大略的划分变为细致的区域划分，这样就会更加精确。GeoHash算法就是基于这种思想，递归划分的次数越多，所计算出的数据越精确。</p>
<h3 id="geohash算法原理-2">GeoHash算法原理</h3>
<blockquote>
<p>GeoHash算法大体上分为三步：1. 计算经纬度的二进制、2. 合并经纬度的二进制、3. 通过Base32对合并后的二进制进行编码。</p>
</blockquote>
<ol>
<li>计算经纬度的二进制</li>
</ol>
<pre><code class="language-java">	//根据经纬度和范围，获取对应的二进制
	private BitSet getBits(double l, double floor, double ceiling) {
		BitSet buffer = new BitSet(numbits);
		for (int i = 0; i &lt; numbits; i++) {
			double mid = (floor + ceiling) / 2;
			if (l &gt;= mid) {
				buffer.set(i);
				floor = mid;
			} else {
				ceiling = mid;
			}
		}
		return buffer;
	}
</code></pre>
<p>上述代码numbits为：<code>private static int numbits = 3 * 5; //经纬度单独编码长度</code>也就是说将地球进行15次<code>二分切割</code></p>
<blockquote>
<p>注： 这里需要对BitSet类进行一下剖析，没了解过该类的话指定懵。</p>
</blockquote>
<p>了解BitSet只需了去了解它的set()、get()方法就足够了</p>
<ul>
<li>BitSet的set方法</li>
</ul>
<pre><code class="language-java"> /**
     * Sets the bit at the specified index to {@code true}.
     *
     * @param  bitIndex a bit index
     * @throws IndexOutOfBoundsException if the specified index is negative
     * @since  JDK1.0
     */
    public void set(int bitIndex) {
        if (bitIndex &lt; 0)
            throw new IndexOutOfBoundsException(&quot;bitIndex &lt; 0: &quot; + bitIndex);

        int wordIndex = wordIndex(bitIndex);
        expandTo(wordIndex);

        words[wordIndex] |= (1L &lt;&lt; bitIndex); // Restores invariants

        checkInvariants();
    }
</code></pre>
<p>set方法内<code>wordIndex(bitIndex)</code>底层将bitIndex右移6位然后返回，<code>ADDRESS_BITS_PER_WORD</code>为常量6</p>
<pre><code class="language-java">/**
    * Given a bit index, return word index containing it.
    */
   private static int wordIndex(int bitIndex) {
       return bitIndex &gt;&gt; ADDRESS_BITS_PER_WORD;
   }
</code></pre>
<p>set方法内的<code>expandTo(wordIndex)</code>只是一个判断数组是否需要扩容的方法</p>
<pre><code class="language-java">/**
     * Ensures that the BitSet can accommodate a given wordIndex,
     * temporarily violating the invariants.  The caller must
     * restore the invariants before returning to the user,
     * possibly using recalculateWordsInUse().
     * @param wordIndex the index to be accommodated.
     */
    private void expandTo(int wordIndex) {
        int wordsRequired = wordIndex+1;
        if (wordsInUse &lt; wordsRequired) {
            ensureCapacity(wordsRequired);
            wordsInUse = wordsRequired;
        }
    }
</code></pre>
<p>set内重要的一行代码<code>words[wordIndex] |= (1L &lt;&lt; bitIndex)</code>,这里只解释一下<code>|=</code></p>
<blockquote>
<p><code>a|=b</code>就是a=a|b,就是说将a、b转为二进制按位与，同0为0，否则为1</p>
</blockquote>
<ul>
<li>BitSet的get方法</li>
</ul>
<pre><code class="language-java"> /**
    * Returns the value of the bit with the specified index. The value
    * is {@code true} if the bit with the index {@code bitIndex}
    * is currently set in this {@code BitSet}; otherwise, the result
    * is {@code false}.
    *
    * @param  bitIndex   the bit index
    * @return the value of the bit with the specified index
    * @throws IndexOutOfBoundsException if the specified index is negative
    */
   public boolean get(int bitIndex) {
       if (bitIndex &lt; 0)
           throw new IndexOutOfBoundsException(&quot;bitIndex &lt; 0: &quot; + bitIndex);

       checkInvariants();

       int wordIndex = wordIndex(bitIndex);
       return (wordIndex &lt; wordsInUse)
           &amp;&amp; ((words[wordIndex] &amp; (1L &lt;&lt; bitIndex)) != 0);
   }
</code></pre>
<blockquote>
<p>get方法用一句话概括就是：如果传入的下标有值，返回true；反之为false</p>
</blockquote>
<p>以天安门坐标为例：<code>39.913884, 116.403874</code></p>
<pre><code class="language-java">	BitSet latbits = getBits(lat, -90, 90);
   	BitSet lonbits = getBits(lon, -180, 180);
   	// 纬度
   	for (int i = 0; i &lt; numbits; i++) {
   		System.out.print(latbits.get(i) + &quot; &quot;);
   	}
   	// 经度
    for (int i = 0; i &lt; numbits; i++) {
   		System.out.print(lonbits.get(i) + &quot; &quot;);
   	}
</code></pre>
<p>纬度经过转换为：</p>
<pre><code class="language-java">true false true true true false false false true true false false false true false 
</code></pre>
<p>转为二进制：</p>
<pre><code class="language-java">1 0 1 1 1 0 0 0 1 1 0 0 0 1 0 
</code></pre>
<p>经度经过转换为：</p>
<pre><code class="language-java">true true false true false false true false true true false false false true true 
</code></pre>
<p>转为二进制：</p>
<pre><code class="language-java">1 1 0 1 0 0 1 0 1 1 0 0 0 1 1 
</code></pre>
<ol start="2">
<li>合并经纬度二进制<br>
合并原则：经度占偶数位，纬度占奇数位。也就是说经纬度交替合并，首位0位置为经度的0位置</li>
</ol>
<p>合并后二进制编码为：</p>
<pre><code class="language-shell">11100 11101 00100 01111 00000 01110
</code></pre>
<ol start="3">
<li>使用Base32对合并后的经纬度二进制进行编码</li>
</ol>
<ul>
<li>代码实现</li>
</ul>
<pre><code class="language-java">// Base32进行编码
	public String encode(double lat, double lon) {
		BitSet latbits = getBits(lat, -90, 90);
		BitSet lonbits = getBits(lon, -180, 180);
		StringBuilder buffer = new StringBuilder();
		for (int i = 0; i &lt; numbits; i++) {
			buffer.append((lonbits.get(i)) ? '1' : '0');
			buffer.append((latbits.get(i)) ? '1' : '0');
		}
		String code = base32(Long.parseLong(buffer.toString(), 2));
		return code;
	}
</code></pre>
<p>本文案例经纬度编码后</p>
<pre><code class="language-java">wx4g0f
</code></pre>
<h3 id="后续问题">后续问题</h3>
<p>如果要使用此功能实现附近的人。假如红点为使用者，经过Geohash算法分割后只会推荐同区域<code>0011</code>中的绿点，但是如下图所示，蓝色点相对于绿色点更接近用户，所以区域划分的弊端就展现在这里。<br>
<img src="https://img-blog.csdnimg.cn/20200519160012889.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L00yODM1OTIzMzg=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述" loading="lazy"><br>
针对上述问题，我们可以人为获取红色用户所在的<code>0011</code>区域周边八个区域中的用户，即获取<code>0011</code>的同时还要获取<code>0100</code>,<code>0110</code>,<code>1100</code>,<code>0001</code>,<code>1001</code>,<code>0000</code>,<code>0010</code>,<code>1000</code></p>
<ul>
<li>代码实现</li>
</ul>
<pre><code class="language-java">	public ArrayList&lt;String&gt; getArroundGeoHash(double lat, double lon) {
		ArrayList&lt;String&gt; list = new ArrayList&lt;&gt;();
		double uplat = lat + minLat;
		double downLat = lat - minLat;

		double leftlng = lon - minLng;
		double rightLng = lon + minLng;

		String leftUp = encode(uplat, leftlng);
		list.add(leftUp);

		String leftMid = encode(lat, leftlng);
		list.add(leftMid);

		String leftDown = encode(downLat, leftlng);
		list.add(leftDown);

		String midUp = encode(uplat, lon);
		list.add(midUp);

		String midMid = encode(lat, lon);
		list.add(midMid);

		String midDown = encode(downLat, lon);
		list.add(midDown);

		String rightUp = encode(uplat, rightLng);
		list.add(rightUp);

		String rightMid = encode(lat, rightLng);
		list.add(rightMid);

		String rightDown = encode(downLat, rightLng);
		list.add(rightDown);

		return list;
	}
</code></pre>
<p>然后根据球体两点间的距离计算红色用户与周边区域用户距离，从而进行附近的人功能实现</p>
<ul>
<li>通过两经纬度计算距离java代码实现</li>
</ul>
<pre><code class="language-java">	static double getDistance(double lat1, double lon1, double lat2, double lon2) {
		// 经纬度（角度）转弧度。弧度用作参数，以调用Math.cos和Math.sin
		double radiansAX = Math.toRadians(lon1); // A经弧度
		double radiansAY = Math.toRadians(lat1); // A纬弧度
		double radiansBX = Math.toRadians(lon2); // B经弧度
		double radiansBY = Math.toRadians(lat2); // B纬弧度

		// 公式中“cosβ1cosβ2cos（α1-α2）+sinβ1sinβ2”的部分，得到∠AOB的cos值
		double cos = Math.cos(radiansAY) * Math.cos(radiansBY) * Math.cos(radiansAX - radiansBX)
				+ Math.sin(radiansAY) * Math.sin(radiansBY);
		double acos = Math.acos(cos); // 反余弦值
		return EARTH_RADIUS * acos; // 最终结果
	}
</code></pre>
<h2 id="geohash算法代码实现">GeoHash算法代码实现</h2>
<pre><code class="language-java">public class GeoHash {
	public static final double MINLAT = -90;
	public static final double MAXLAT = 90;
	public static final double MINLNG = -180;
	public static final double MAXLNG = 180;

	private static int numbits = 3 * 5; //经纬度单独编码长度

	private static double minLat;
	private static double minLng;

	private final static char[] digits = {'0', '1', '2', '3', '4', '5', '6', '7', '8',
			'9', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'j', 'k', 'm', 'n', 'p',
			'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z'};

	//定义编码映射关系
	final static HashMap&lt;Character, Integer&gt; lookup = new HashMap&lt;Character, Integer&gt;();

	//初始化编码映射内容
	static {
		int i = 0;
		for (char c : digits)
			lookup.put(c, i++);
	}

	public GeoHash() {
		setMinLatLng();
	}

	// Base32进行编码
	public String encode(double lat, double lon) {
		BitSet latbits = getBits(lat, -90, 90);
		BitSet lonbits = getBits(lon, -180, 180);
		StringBuilder buffer = new StringBuilder();
		for (int i = 0; i &lt; numbits; i++) {
			buffer.append((lonbits.get(i)) ? '1' : '0');
			buffer.append((latbits.get(i)) ? '1' : '0');
		}
		String code = base32(Long.parseLong(buffer.toString(), 2));
		return code;
	}

	public ArrayList&lt;String&gt; getArroundGeoHash(double lat, double lon) {
		ArrayList&lt;String&gt; list = new ArrayList&lt;&gt;();
		double uplat = lat + minLat;
		double downLat = lat - minLat;

		double leftlng = lon - minLng;
		double rightLng = lon + minLng;

		String leftUp = encode(uplat, leftlng);
		list.add(leftUp);

		String leftMid = encode(lat, leftlng);
		list.add(leftMid);

		String leftDown = encode(downLat, leftlng);
		list.add(leftDown);

		String midUp = encode(uplat, lon);
		list.add(midUp);

		String midMid = encode(lat, lon);
		list.add(midMid);

		String midDown = encode(downLat, lon);
		list.add(midDown);

		String rightUp = encode(uplat, rightLng);
		list.add(rightUp);

		String rightMid = encode(lat, rightLng);
		list.add(rightMid);

		String rightDown = encode(downLat, rightLng);
		list.add(rightDown);

		return list;
	}

	//根据经纬度和范围，获取对应的二进制
	private BitSet getBits(double l, double floor, double ceiling) {
		BitSet buffer = new BitSet(numbits);
		for (int i = 0; i &lt; numbits; i++) {
			double mid = (floor + ceiling) / 2;
			if (l &gt;= mid) {
				buffer.set(i);
				floor = mid;
			} else {
				ceiling = mid;
			}
		}
		return buffer;
	}

	//将经纬度合并后的二进制进行指定的32位编码
	private String base32(long i) {
		char[] buf = new char[65];
		int charPos = 64;
		boolean negative = (i &lt; 0);
		if (!negative) {
			i = -i;
		}
		while (i &lt;= -32) {
			buf[charPos--] = digits[(int) (-(i % 32))];
			i /= 32;
		}
		buf[charPos] = digits[(int) (-i)];
		if (negative) {
			buf[--charPos] = '-';
		}
		return new String(buf, charPos, (65 - charPos));
	}

	private void setMinLatLng() {
		minLat = MAXLAT - MINLAT;
		for (int i = 0; i &lt; numbits; i++) {
			minLat /= 2.0;
		}
		minLng = MAXLNG - MINLNG;
		for (int i = 0; i &lt; numbits; i++) {
			minLng /= 2.0;
		}
	}

	//根据二进制和范围解码
	private double decode(BitSet bs, double floor, double ceiling) {
		double mid = 0;
		for (int i = 0; i &lt; bs.length(); i++) {
			mid = (floor + ceiling) / 2;
			if (bs.get(i))
				floor = mid;
			else
				ceiling = mid;
		}
		return mid;
	}

	//对编码后的字符串解码
	public double[] decode(String geohash) {
		StringBuilder buffer = new StringBuilder();
		for (char c : geohash.toCharArray()) {
			int i = lookup.get(c) + 32;
			buffer.append(Integer.toString(i, 2).substring(1));
		}

		BitSet lonset = new BitSet();
		BitSet latset = new BitSet();

		//偶数位，经度
		int j = 0;
		for (int i = 0; i &lt; numbits * 2; i += 2) {
			boolean isSet = false;
			if (i &lt; buffer.length())
				isSet = buffer.charAt(i) == '1';
			lonset.set(j++, isSet);
		}

		//奇数位，纬度
		j = 0;
		for (int i = 1; i &lt; numbits * 2; i += 2) {
			boolean isSet = false;
			if (i &lt; buffer.length())
				isSet = buffer.charAt(i) == '1';
			latset.set(j++, isSet);
		}

		double lon = decode(lonset, -180, 180);
		double lat = decode(latset, -90, 90);

		return new double[]{lat, lon};
	}

	public static void main(String[] args) {
		GeoHash geoHash = new GeoHash();
		// 北京站
		String encode = geoHash.encode(39.910508, 116.433589);
		System.out.println(encode);

		// 天安门
		System.out.println(geoHash.encode(39.913884, 116.403874));

		// 首都机场
		System.out.println(geoHash.encode(40.086109, 116.606819));

		BitSet latbits = geoHash.getBits(39.913884, -90, 90);
		BitSet lonbits = geoHash.getBits(116.403874, -180, 180);

//		for (int i=0; i&lt; latbits.length(); i++) {
//			System.out.println(latbits.get(i));
//		}

		for (int i = 0; i &lt; numbits; i++) {
//			System.out.print(latbits.get(i));
			System.out.print(latbits.get(i) ? '1' : '0');
			System.out.print(&quot; &quot;);
		}

		System.out.println();
		StringBuilder buffer = new StringBuilder();
		for (int i = 0; i &lt; numbits; i++) {
			buffer.append((lonbits.get(i)) ? '1' : '0');
			buffer.append((latbits.get(i)) ? '1' : '0');
		}

		System.out.println(buffer.toString());

		System.out.println(geoHash.encode(39.913884, 116.403874));
	}

}
</code></pre>
<h2 id="写在最后">写在最后</h2>
<p>如果嫌GeoHash算法麻烦，但是还想用它，没关系。<br>
Redis知道你懒<a href="https://redis.io/commands/GEOHASH">Redis官网GeoHash用法</a></p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Spark入门( 九)——机器学习 Spark MLlib]]></title>
        <id>https://mask0407.github.io/spark08/</id>
        <link href="https://mask0407.github.io/spark08/">
        </link>
        <updated>2020-06-29T03:18:58.000Z</updated>
        <content type="html"><![CDATA[<p><ul class="markdownIt-TOC">
<li><a href="#%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%98%AF%E4%BB%80%E4%B9%88">机器学习是什么？</a>
<ul>
<li><a href="#%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0">机器学习</a></li>
<li><a href="#spark-mllib">Spark MLlib</a></li>
</ul>
</li>
<li><a href="#spark-mllib%E6%A1%88%E4%BE%8B">Spark MLlib案例</a>
<ul>
<li><a href="#%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8">快速入门</a></li>
<li><a href="#%E5%9F%BA%E6%9C%AC%E7%BB%9F%E8%AE%A1">基本统计</a>
<ul>
<li><a href="#correlation%E7%9B%B8%E5%85%B3%E6%80%A7">Correlation(相关性)</a></li>
<li><a href="#hypothesis-testing%E5%81%87%E8%AE%BE%E6%A3%80%E9%AA%8C">Hypothesis testing(假设检验)</a></li>
<li><a href="#summarizer%E6%80%BB%E7%BB%93%E5%99%A8">Summarizer(总结器)</a></li>
</ul>
</li>
<li><a href="#%E6%9C%AA%E5%AE%8C%E5%BE%85%E7%BB%AD">未完待续。。。</a></li>
</ul>
</li>
</ul>
(Spark MLlib)</p>
<h1 id="机器学习是什么">机器学习是什么？</h1>
<h2 id="机器学习">机器学习</h2>
<p>数据挖掘有着50多年的发展历史。机器学习就是其子领域之一，特点是利用大型计算机集群来从海量数据中分析和提取知识</p>
<p>机器学习与计算统计学密切相关。它与数学优化紧密关联，为其提供方法、理论和应用领域。机器学习在各种传统设计和编程不能胜任的计算机任务中有广泛应用。典型的应用如<code>垃圾邮件过滤</code>、<code>光学字符识别(OCR)</code>、<code>搜索引擎和计算机视觉</code>。机器学习有时和数据挖掘联用，但更偏向<code>探索性数据分析</code>，亦称为<code>无监督学习</code>。</p>
<p>与学习系统的可用输入自然属性不同，机器学习系统可分为<code>3种</code>。学习算法发现输入数据的内在结构。它可以<code>有目标(隐含模式)</code>,也可以是发现特征的一种途径。</p>
<table>
<thead>
<tr>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>无监督学习</td>
<td>学习系统的输入数据中并不包含对应的<code>标签(或期望的输出)</code>，它需要自行从输入中找到输入数据的内在结构</td>
</tr>
<tr>
<td>监督学习</td>
<td>系统已知各输入对应的<code>期望输出</code>系统的目标是学习如何将输入映射到输出</td>
</tr>
<tr>
<td>强化学习</td>
<td>系统与环境进行交互，它有已定义的目标，但没有人类显式地告知其是否正在接近目标</td>
</tr>
</tbody>
</table>
<h2 id="spark-mllib">Spark MLlib</h2>
<p>MLlib是Spark的机器学习（ML）库。其目标是使实用的机器学习可扩展且容易。在较高级别，它提供了以下工具：</p>
<blockquote>
<ul>
<li>ML算法：常见的学习算法，例如分类，回归，聚类和协同过滤</li>
<li>特征化：特征提取，变换，降维和选择</li>
<li>管道：用于构建，评估和调整ML管道的工具</li>
<li>持久性：保存和加载算法，模型和管道</li>
<li>实用程序：线性代数，统计信息，数据处理等。</li>
</ul>
</blockquote>
<h1 id="spark-mllib案例">Spark MLlib案例</h1>
<blockquote>
<p>基于<code>DataFrame</code>的API是主要API</p>
</blockquote>
<h2 id="快速入门">快速入门</h2>
<ul>
<li>pom.xml</li>
</ul>
<pre><code class="language-xml">&lt;!-- https://mvnrepository.com/artifact/org.apache.spark/spark-mllib --&gt;
		&lt;dependency&gt;
			&lt;groupId&gt;org.apache.spark&lt;/groupId&gt;
			&lt;artifactId&gt;spark-mllib_2.11&lt;/artifactId&gt;
			&lt;version&gt;2.2.1&lt;/version&gt;
		&lt;/dependency&gt;
</code></pre>
<pre><code class="language-java">def main(args: Array[String]): Unit = {
		// 屏蔽日志
		Logger.getLogger(&quot;org.apache.spark&quot;).setLevel(Level.WARN)
		Logger.getLogger(&quot;org.apache.jetty.server&quot;).setLevel(Level.OFF)

		val spark = SparkSession
			.builder()
			.master(&quot;local[*]&quot;)
			.appName(Demo01.getClass.getName)
    		.getOrCreate()

		import spark.implicits._

		val data = Seq(
			/**
			 * 稀疏向量表示方式
			 * 4, Seq((0, 1.0), (3, -2.0))
			 * 表示向量长度为4 有两个非0位置：0和3位置，0和3的值分别为1.0、-2.0
			 * 该向量可表示为(1.0, 0, 0, -2.0)
 			 */
			Vectors.sparse(4, Seq((0, 1.0), (3, -2.0))),
			// 密集向量表示方式
			Vectors.dense(4.0, 5.0, 0.0, 3.0),
			Vectors.dense(6.0, 7.0, 0.0, 8.0),
			Vectors.sparse(4, Seq((0, 9.0), (3, 1.0)))
		)

		val df = data.map(Tuple1.apply).toDF(&quot;features&quot;)
		df.show()
		
		// 计算df features列的相关性
		val Row(coeff1: Matrix) = Correlation.corr(df, &quot;features&quot;).head
		println(s&quot;Pearson correlation matrix:\n $coeff1&quot;)

		val Row(coeff2: Matrix) = Correlation.corr(df, &quot;features&quot;, &quot;spearman&quot;).head
		println(s&quot;Spearman correlation matrix:\n $coeff2&quot;)
		
		spark.stop()
	}
</code></pre>
<h2 id="基本统计">基本统计</h2>
<h3 id="correlation相关性">Correlation(相关性)</h3>
<blockquote>
<p>Correlation 使用指定的方法为向量的输入数据集计算相关矩阵。输出将是一个DataFrame，其中包含向量列的相关矩阵。</p>
</blockquote>
<p>皮尔森系数公式：</p>
<blockquote>
<p>当两个变量的线性关系增强时，相关系数趋于1或-1。正相关时趋于1，负相关时趋于-1。当两个变量独立时相关系统为0，但反之不成立。当Y和X服从联合正态分布时，其相互独立和不相关是等价的<br>
<img src="https://img-blog.csdnimg.cn/20200624110228116.png" alt="在这里插入图片描述" loading="lazy"></p>
</blockquote>
<pre><code class="language-java">		val data = Seq(
			/**
			 * 稀疏向量表示方式
			 * 4, Seq((0, 1.0), (3, -2.0))
			 * 表示向量长度为4 有两个非0位置：0和3位置，0和3的值分别为1.0、-2.0
			 * 该向量可表示为(1.0, 0, 0, -2.0)
 			 */
			Vectors.sparse(4, Seq((0, 1.0), (3, -2.0))),
			// 密集向量表示方式
			Vectors.dense(4.0, 5.0, 0.0, 3.0),
			Vectors.dense(6.0, 7.0, 0.0, 8.0),
			Vectors.sparse(4, Seq((0, 9.0), (3, 1.0)))
		)

		val df = data.map(Tuple1.apply).toDF(&quot;features&quot;)
		df.show()
		
		// 计算features的相关性, method系数默认为Pearson
		val Row(coeff1: Matrix) = Correlation.corr(df, &quot;features&quot;).head
		println(s&quot;Pearson correlation matrix:\n $coeff1&quot;)

		// 计算features的相关性, method系数为Spearson
		val Row(coeff2: Matrix) = Correlation.corr(df, &quot;features&quot;, &quot;spearman&quot;).head
		println(s&quot;Spearman correlation matrix:\n $coeff2&quot;)
</code></pre>
<h3 id="hypothesis-testing假设检验">Hypothesis testing(假设检验)</h3>
<blockquote>
<p>假设检验是一种强大的统计工具，可用来确定结果是否具有统计学意义，以及该结果是否偶然发生。spark.ml当前支持Pearson的卡方（<code>数学处理错误</code>）测试独立性。<br>
ChiSquareTest针对标签上的每个功能进行Pearson的独立性测试。对于每个要素，（要素，标签）对将转换为列联矩阵，针对该列矩阵计算卡方统计量。所有标签和特征值必须是分类的。</p>
</blockquote>
<pre><code class="language-java">		import org.apache.spark.ml.linalg.{Vector, Vectors}
		import org.apache.spark.ml.stat.ChiSquareTest

		val data = Seq(
			(0.0, Vectors.dense(0.5, 10.0)),
			(0.0, Vectors.dense(1.5, 20.0)),
			(1.0, Vectors.dense(1.5, 30.0)),
			(0.0, Vectors.dense(3.5, 30.0)),
			(0.0, Vectors.dense(3.5, 40.0)),
			(1.0, Vectors.dense(3.5, 40.0))
		)

		val df = data.toDF(&quot;label&quot;, &quot;features&quot;)
		df.show()
		val chi = ChiSquareTest.test(df, &quot;features&quot;, &quot;label&quot;).head
		println(s&quot;pValues = ${chi.getAs[Vector](0)}&quot;)
		println(s&quot;degreesOfFreedom ${chi.getSeq[Int](1).mkString(&quot;[&quot;, &quot;,&quot;, &quot;]&quot;)}&quot;)
		println(s&quot;statistics ${chi.getAs[Vector](2)}&quot;)
</code></pre>
<h3 id="summarizer总结器">Summarizer(总结器)</h3>
<pre><code class="language-java">		import spark.implicits._
		import org.apache.spark.ml.stat.Summarizer._

		val data = Seq(
			(Vectors.dense(2.0, 3.0, 5.0), 1.0),
			(Vectors.dense(4.0, 6.0, 7.0), 2.0)
		)

		val df = data.toDF(&quot;features&quot;, &quot;weight&quot;)

		val (meanVal, varianceVal) = df.select(metrics(&quot;mean&quot;, &quot;variance&quot;)
			.summary($&quot;features&quot;, $&quot;weight&quot;).as(&quot;summary&quot;))
			.select(&quot;summary.mean&quot;, &quot;summary.variance&quot;)
			.as[(Vector, Vector)].first()

		println(s&quot;with weight: mean = ${meanVal}, variance = ${varianceVal}&quot;)

		val (meanVal2, varianceVal2) = df.select(mean($&quot;features&quot;), variance($&quot;features&quot;))
			.as[(Vector, Vector)].first()

		println(s&quot;without weight: mean = ${meanVal2}, sum = ${varianceVal2}&quot;)
</code></pre>
<h2 id="未完待续">未完待续。。。</h2>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Spark入门( 八)——Spark流计算新玩法-Structured Streaming]]></title>
        <id>https://mask0407.github.io/spark07/</id>
        <link href="https://mask0407.github.io/spark07/">
        </link>
        <updated>2020-06-29T03:18:21.000Z</updated>
        <content type="html"><![CDATA[<p><ul class="markdownIt-TOC">
<li><a href="#%E7%AE%80%E4%BB%8B">简介</a></li>
<li><a href="#%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8%E6%A1%88%E4%BE%8B">快速入门案例</a>
<ul>
<li><a href="#%E7%A8%8B%E5%BA%8F%E6%B5%81%E7%A8%8B%E7%BB%93%E6%9E%84">程序流程结构</a></li>
</ul>
</li>
<li><a href="#%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5">基本概念</a></li>
<li><a href="#%E6%95%85%E9%9A%9C%E5%AE%B9%E9%94%99">故障容错</a></li>
<li><a href="#structured-streaming-api">Structured Streaming API</a>
<ul>
<li><a href="#input-sources">Input Sources</a>
<ul>
<li><a href="#file-source">File Source</a>
<ul>
<li><a href="#socket-sourcedebug">Socket source(debug)</a></li>
</ul>
</li>
<li><a href="#kafka-source">Kafka source</a></li>
</ul>
</li>
<li><a href="#output-sink">Output Sink</a>
<ul>
<li><a href="#file-sinkappend-mode-only">File sink(Append Mode Only)</a></li>
<li><a href="#kafkasinkappendupdatecomplete">KafkaSink((Append|Update|Complete))</a></li>
<li><a href="#foreach-sinkappendupdatecomplate">Foreach sink(Append|Update|Complate)</a>
<ul>
<li><a href="#userrowwriter">UserRowWriter</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#window-on-event-time">Window on Event Time</a></li>
<li><a href="#%E5%A4%84%E7%90%86%E5%BB%B6%E8%BF%9F-data-%E5%92%8C-watermarking">处理延迟 Data 和 Watermarking</a>
<ul>
<li><a href="#watermarking%E4%BF%9D%E9%9A%9C%E6%9C%BA%E5%88%B6">Watermarking保障机制：</a></li>
<li><a href="#spark%E6%B8%85%E9%99%A4window%E8%81%9A%E5%90%88%E7%8A%B6%E6%80%81%E6%9D%A1%E4%BB%B6">Spark清除window聚合状态条件</a>
<ul>
<li><a href="#update-mode">Update Mode</a></li>
<li><a href="#append-mode">Append Mode</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#join-%E6%93%8D%E4%BD%9C">Join 操作</a>
<ul>
<li><a href="#stream-static-joins">Stream-static Joins</a></li>
<li><a href="#stream-stream-joins">Stream-stream Joins</a>
<ul>
<li><a href="#inner-join">inner join</a></li>
<li><a href="#outer-join">outer join</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
(Structured Streaming)</p>
<h1 id="简介">简介</h1>
<p><code>Structured Streaming</code> 构建在<code>SparkSQL</code>之上的流处理引擎。可以使用户继续使用<code>DataSet/dataFrame</code>操<br>
作流数据。并且提供了多种计算模型可供选择，默认情况下，使用的依然是Spark的marco batch这种计<br>
算模型能够到100ms左右的end-to-end的精准一次的容错计算。除此之外也提供了基于<code>EventTime</code>语义<br>
的窗口计算（DStream 基于Processor Time不同）。同时在spark-2.3版本又提出新的计算模型<br>
<code>Continuous Processing</code>可以达到1ms左右的精准一次的容错计算。</p>
<h1 id="快速入门案例">快速入门案例</h1>
<ul>
<li>pom</li>
</ul>
<pre><code class="language-xml">		&lt;dependency&gt;
			&lt;groupId&gt;org.apache.spark&lt;/groupId&gt;
			&lt;artifactId&gt;spark-sql_2.11&lt;/artifactId&gt;
			&lt;version&gt;2.4.3&lt;/version&gt;
		&lt;/dependency&gt;
		&lt;dependency&gt;
			&lt;groupId&gt;org.apache.spark&lt;/groupId&gt;
			&lt;artifactId&gt;spark-core_2.11&lt;/artifactId&gt;
			&lt;version&gt;2.4.3&lt;/version&gt;
		&lt;/dependency&gt;
</code></pre>
<ul>
<li>WordCount</li>
</ul>
<pre><code class="language-java">def main(args: Array[String]): Unit = {
		Logger.getLogger(&quot;org.apache.spark&quot;).setLevel(Level.WARN)
		Logger.getLogger(&quot;org.apache.jetty.server&quot;).setLevel(Level.OFF)

		//1.构建SparkSession
		val spark = SparkSession.builder()
			.appName(&quot;wordcount&quot;)
			.master(&quot;local[*]&quot;)
			.getOrCreate()
		import spark.implicits._

		//2.创建输入流-readStream
		var lines = spark.readStream
			.format(&quot;socket&quot;)
			.option(&quot;host&quot;, &quot;localhost&quot;)
			.option(&quot;port&quot;, 9999)
			.load()

		//3.对dataframe实现转换
		var wordCounts = lines.as[String]
    		.flatMap(_.split(&quot;\\s+&quot;))
    		.groupBy(&quot;value&quot;)
    		.count()


		//4.构建query 输出
		val query = wordCounts.writeStream
			.format(&quot;console&quot;)
			.outputMode(OutputMode.Update()) //有状态持续计算 Complete| Update| Append
			.start()

		//5.等待流结束
		query.awaitTermination()
	}
</code></pre>
<ul>
<li>有状态持续计算 Complete| Update| Append 之间的区别</li>
</ul>
<blockquote>
<ol>
<li>Complete: 每一个trigger到来时，就输出整个完整的dataframe</li>
<li>Update: 只输出那些被修改的Row。<br>
每一次window sliding，就去跟原来的结果比较，有变化就输出</li>
<li>Append: 只输出新添加的（原来没有的）Row（）（如果是groupby，要有watermark才可以）<br>
每当一个watermark时间结束了，这个临时的结果再回转换成正式的结果并导出。</li>
</ol>
</blockquote>
<ul>
<li>nc -l 999 输入</li>
</ul>
<pre><code>aa bb cc aa
cc aa aa aa
</code></pre>
<ul>
<li>输出结果(由于使用了Update 第二次输入没有<code>bb</code>，所有Batch: 2没有bb输出)</li>
</ul>
<pre><code>-------------------------------------------
Batch: 1
-------------------------------------------
+-----+-----+
|value|count|
+-----+-----+
|   cc|    1|
|   bb|    1|
|   aa|    2|
+-----+-----+

-------------------------------------------
Batch: 2
-------------------------------------------
+-----+-----+
|value|count|
+-----+-----+
|   cc|    2|
|   aa|    5|
+-----+-----+
</code></pre>
<h2 id="程序流程结构">程序流程结构</h2>
<blockquote>
<p>1.构建<code>SparkSession</code> 对象<br>
2.借助于<code>SparkSession#readStream</code>加载动态的<code>Dataframe</code><br>
3.使用<code>DataFrame API</code>或者是SQL语句 实现对动态数据计算<br>
4.通过<code>DataFrame#writeStream</code>方法构建<code>StreamQuery</code>对象<br>
5.调用<code>StreamQuery#awaitTermination</code>等待关闭指令</p>
</blockquote>
<h1 id="基本概念">基本概念</h1>
<p>Structure Stream的核心思想是通过将实时数据流看成是一个持续插入table.因此用户就可以使用SQL查 询DynamicTable|UnboundedTable。底层Spark通过StreamQuery实现对数据持续计算。<br>
<img src="https://img-blog.csdnimg.cn/20200612174115861.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L00yODM1OTIzMzg=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述" loading="lazy"><br>
当对Input执行转换的时候系统产生一张结果表 <code>ResultTable</code> ,当有新的数据产生的时候，系统会往<br>
<code>Input Table</code> 插入一行数据，这会最终导致系统更新 <code>ResultTable</code> ,每一次的更新系统将更新的数<br>
据写到外围系统-Sink.<br>
<img src="https://img-blog.csdnimg.cn/20200612174328732.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L00yODM1OTIzMzg=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述" loading="lazy"></p>
<ul>
<li><code>Output</code> 定义如何将Result写出到外围系统，目前Spark支持三种输出模式：(上面已经简单介绍过了)</li>
</ul>
<ol>
<li><code>Complete Mode</code> - 整个ResultTable的数据会被写到外围系统。</li>
<li><code>Update Mode</code> - 只会将ResultTable中被更新的行，写到外围系统（ spark-2.1.1 +支持）</li>
<li><code>Append Mode</code> - 只有新数据插入ResultTable的时候，才会将结果输出。注意：这种模式只适用<br>
于被插入结果表的数据都是只读的情况下，才可以将输出模式定义为Append（查询当中不应该出<br>
现聚合算子，当然也有特例，例如流中声明watermarker）</li>
</ol>
<p>由于Structure Streaming计算的特点，Spark会在内存当中存储程序计算中间状态用于生产结果表的数<br>
据，Spark并不会存储 Input Table 的数据，一旦处理完成之后，读取的数据会被丢弃。整个聚合的<br>
过程无需用户干预（对比Storm，Storm状态管理需要将数据写到外围系统）。</p>
<h1 id="故障容错">故障容错</h1>
<p>Structured Streaming通过<code>checkpoint</code>和<code>write ahead log</code>去记录每一次批处理的数据源的偏移量（区<br>
间），可以保证在失败的时候可以重复的读取数据源。其次Structure Streaming也提供了Sink的<code>幂等写</code><br>
的特性（在编程中一个幂等 操作的特点是其任意多次执行所产生的影响均与一次执行的影响相同），<br>
因此Structure Streaming实现<code>end-to-end</code> <code>exactly-once</code>语义的故障恢复。</p>
<h1 id="structured-streaming-api">Structured Streaming API</h1>
<p>自Spark-2.0版本以后<code>Dataframe/Dataset</code>才可以处理有界数据和无界数据。<code>Structured Streaming</code>也是用<br>
<code>SparkSession</code>方式去创建<code>Dataset/DataFrame</code> ,同时所有<code>Dataset/DataFrame</code> 的操作保持和<code>Spark SQL</code><br>
中<code>Dataset/DataFrame</code> 一致。</p>
<h2 id="input-sources">Input Sources</h2>
<h3 id="file-source">File Source</h3>
<p>目前支持支持<code>text</code>, <code>csv</code>, <code>json</code>, <code>orc</code>, <code>parquet</code>等格式的文件，当这些数据被放入到采样目录，系统会以流的<br>
形式读取采样目录下的文件.</p>
<pre><code class="language-java">		//1.创建SparkSession
		val spark = SparkSession
			.builder()
			.master(&quot;local[*]&quot;)
			.appName(&quot;printline&quot;)
			.getOrCreate()

		import spark.implicits._
		var df = spark.readStream
			.format(&quot;text&quot;)
			//json/csv/parquet/orc 等
			.load(&quot;file:///Users/mashikang/IdeaProjects/structured_stream/src/main/resources&quot;)

		var userDF = df.as[String]
			.map(line =&gt; line.split(&quot;\\s+&quot;))
			.map(tokens =&gt; (tokens(0).toInt, tokens(1), tokens(2).toBoolean, tokens(3).toInt))
			.toDF(&quot;id&quot;, &quot;name&quot;, &quot;sex&quot;, &quot;age&quot;)

		val query = userDF.writeStream.format(&quot;console&quot;)
			.outputMode(OutputMode.Append())
			.start()


		query.awaitTermination()
</code></pre>
<ul>
<li>文件</li>
</ul>
<pre><code class="language-text">1 zhangsan true 20
2 lisi true 28
3 wangwu false 24
4 zhaoliu true 28
</code></pre>
<h4 id="socket-sourcedebug">Socket source(debug)</h4>
<pre><code class="language-java">		//1.构建SparkSession
		val spark = SparkSession.builder()
			.appName(&quot;wordcount&quot;)
			.master(&quot;local[*]&quot;)
			.getOrCreate()
		import spark.implicits._

		//2.创建输入流-readStream
		var lines = spark.readStream
			.format(&quot;socket&quot;)
			.option(&quot;host&quot;, &quot;localhost&quot;)
			.option(&quot;port&quot;, 9999)
			.load()

		//3.对dataframe实现转换
		var wordCounts = lines.as[String]
    		.flatMap(_.split(&quot;\\s+&quot;))
    		.groupBy(&quot;value&quot;)
    		.count()


		//4.构建query 输出
		val query = wordCounts.writeStream
			.format(&quot;console&quot;)
			.outputMode(OutputMode.Update()) //有状态持续计算 Complete| Update
			.start()

		//5.等待流结束
		query.awaitTermination()
</code></pre>
<h3 id="kafka-source">Kafka source</h3>
<ul>
<li>pom.xml</li>
</ul>
<pre><code class="language-xml">&lt;dependency&gt;
 	&lt;groupId&gt;org.apache.spark&lt;/groupId&gt;
	&lt;artifactId&gt;spark-sql-kafka-0-10_2.11&lt;/artifactId&gt; 
	&lt;version&gt;2.4.3&lt;/version&gt; 
&lt;/dependency&gt;
</code></pre>
<pre><code class="language-java">		//1.创建SparkSession
		val spark=SparkSession .builder()
			.master(&quot;local[*]&quot;)
			.appName(&quot;printline&quot;)
			.getOrCreate()

		import spark.implicits._
		var df=spark.readStream
			.format(&quot;kafka&quot;)
			.option(&quot;kafka.bootstrap.servers&quot;, &quot;localhost:9092&quot;)
			.option(&quot;subscribe&quot;, &quot;topic01&quot;)
			.load()
			.selectExpr(&quot;CAST(key AS STRING)&quot;,&quot;CAST(value AS STRING)&quot;)

		val wordCounts=df.select(&quot;value&quot;).as[String]
			.flatMap(_.split(&quot;\\s+&quot;))
			.coalesce(1)
			.map((_,1))
			.toDF(&quot;word&quot;,&quot;count&quot;)
			.groupBy(&quot;word&quot;)
			.sum(&quot;count&quot;)

		val query = wordCounts.writeStream.
			format(&quot;console&quot;)
			.outputMode(OutputMode.Update())
			.start()

		query.awaitTermination()
</code></pre>
<h2 id="output-sink">Output Sink</h2>
<h3 id="file-sinkappend-mode-only">File sink(Append Mode Only)</h3>
<pre><code class="language-java">	//1.创建SparkSession
		val spark = SparkSession.builder()
			.master(&quot;local[*]&quot;)
			.appName(&quot;printline&quot;)
			.getOrCreate()

		import spark.implicits._
		var df = spark.readStream
			.format(&quot;kafka&quot;)
			.option(&quot;kafka.bootstrap.servers&quot;, &quot;localhost:9092&quot;)
			.option(&quot;subscribe&quot;, &quot;topic01&quot;)
			.load()
			.selectExpr(&quot;CAST(key AS STRING)&quot;, &quot;CAST(value AS STRING)&quot;)

		val wordCounts = df.select(&quot;value&quot;).as[String]
			.flatMap(_.split(&quot;\\s+&quot;))
			.coalesce(1)
			.map((_, 1))
			.toDF(&quot;word&quot;, &quot;count&quot;)

		val query = wordCounts.writeStream
			.format(&quot;json&quot;)
			.option(&quot;checkpointLocation&quot;, &quot;file:///Users/mashikang/IdeaProjects/structured_stream/src/main/resources/checkpoints&quot;)
			.outputMode(OutputMode.Append())
			.start(&quot;file:////Users/mashikang/IdeaProjects/structured_stream/src/main/resource/json&quot;)


		query.awaitTermination()
</code></pre>
<h3 id="kafkasinkappendupdatecomplete">KafkaSink((Append|Update|Complete))</h3>
<pre><code class="language-java">//1.创建SparkSession
		val spark = SparkSession.builder()
			.master(&quot;local[*]&quot;)
			.appName(&quot;printline&quot;)
			.getOrCreate()

		import spark.implicits._
		import org.apache.spark.sql.functions._

		var df = spark.readStream
			.format(&quot;kafka&quot;)
			.option(&quot;kafka.bootstrap.servers&quot;, &quot;localhost:9092&quot;)
			.option(&quot;subscribe&quot;, &quot;topic01&quot;)
			.load()
			.selectExpr(&quot;CAST(key AS STRING)&quot;, &quot;CAST(value AS STRING)&quot;)

		val wordCounts = df.select(&quot;value&quot;).as[String]
			.flatMap(_.split(&quot;\\s+&quot;))
			.coalesce(1)
			.map((_, 1))
			.toDF(&quot;word&quot;, &quot;count&quot;)
			.groupBy(&quot;word&quot;)
			.agg(sum(&quot;count&quot;) as &quot;count&quot;)
			.selectExpr(&quot;word&quot;, &quot;CAST(count AS STRING)&quot;)
			.withColumnRenamed(&quot;word&quot;, &quot;key&quot;)
			.withColumnRenamed(&quot;count&quot;, &quot;value&quot;)

		val query = wordCounts.writeStream
			.format(&quot;kafka&quot;)
			.option(&quot;kafka.bootstrap.servers&quot;, &quot;localhost:9092&quot;)
			.option(&quot;topic&quot;, &quot;topic02&quot;)
			.option(&quot;checkpointLocation&quot;, &quot;file:///Users/mashikang/IdeaProjects/structured_stream/src/main/resources/checkpoints&quot;)
			.outputMode(OutputMode.Update())
			.start()

		query.awaitTermination()
</code></pre>
<h3 id="foreach-sinkappendupdatecomplate">Foreach sink(Append|Update|Complate)</h3>
<h4 id="userrowwriter">UserRowWriter</h4>
<p>这里的 open方法在，每一次微批的时候触发，其中 <code>epochId</code>表示计算的批次。一般如果要保证<br>
<code>exactly-once</code> 语义的处理时候，需要在外围系统存储 <code>epochId</code>，如果存在重复计算 <code>epochId</code> 不<br>
变。</p>
<pre><code class="language-java">class UserRowWriter extends ForeachWriter[Row] {
	// 存储 上一次epochid
	var lastEpochId: Long = -1L

	/**
	 * 计算 当前是否处理当前批次，如果epochId=lastEpochId说明是重复记录，丢弃更新 false
	 * epochId!=lastEpochId 返回true 调用 open
	 *
	 * @param partitionId
	 * @param epochId
	 * @return
	 */
	override def open(partitionId: Long, epochId: Long): Boolean = {
		var flag: Boolean = false
		if (epochId != -1L) {
			if (lastEpochId == epochId) {
				// 是重复记录
				flag = false
			} else {
				flag = true
				lastEpochId = epochId
			}
		} else {
			// 第一次进来
			lastEpochId = epochId
			flag = true
		}
		flag
	}

	override def process(value: Row): Unit = {
		println(&quot; ,epochId:&quot; + lastEpochId)
	}

	override def close(errorOrNull: Throwable): Unit = {
		if (errorOrNull != null)
			errorOrNull.printStackTrace()
	}
}
</code></pre>
<pre><code class="language-java">		//1.创建SparkSession
		val spark = SparkSession.builder()
			.master(&quot;local[*]&quot;)
			.appName(&quot;printline&quot;)
			.getOrCreate()

		import spark.implicits._
		import org.apache.spark.sql.functions._

		var df = spark.readStream
			.format(&quot;kafka&quot;)
			.option(&quot;kafka.bootstrap.servers&quot;, &quot;localhost:9092&quot;)
			.option(&quot;subscribe&quot;, &quot;topic01&quot;)
			.load()
			.selectExpr(&quot;CAST(key AS STRING)&quot;, &quot;CAST(value AS STRING)&quot;)

		val wordCounts = df.select(&quot;value&quot;).as[String]
			.flatMap(_.split(&quot;\\s+&quot;))
			.coalesce(1)
			.map((_, 1))
			.toDF(&quot;word&quot;, &quot;count&quot;)
			.groupBy(&quot;word&quot;)
			.agg(sum(&quot;count&quot;) as &quot;count&quot;)
			.selectExpr(&quot;word&quot;, &quot;CAST(count AS STRING)&quot;)
			.withColumnRenamed(&quot;word&quot;, &quot;key&quot;)
			.withColumnRenamed(&quot;count&quot;, &quot;value&quot;)

		val query = wordCounts.writeStream
			.outputMode(OutputMode.Update())
			.foreach(new UserRowWriter)
			.start()

		query.awaitTermination()
</code></pre>
<h2 id="window-on-event-time">Window on Event Time</h2>
<p>Structured Streaming使用聚合函数基于EventTime计算window是非常简单的类似于分组聚合。分组聚<br>
合是按照指定的column字段对表中的数据进行分组，然后使用聚合函数对用户指定的column字段进行<br>
聚合。<br>
下面一张图描绘的是计算10分钟内的单词统计，每间隔5分钟滑动一个时间窗口。<br>
<img src="https://img-blog.csdnimg.cn/20200615160910610.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L00yODM1OTIzMzg=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述" loading="lazy"><br>
按照窗口原始含义是将落入到同一个窗口的数据进行分组，因此在Structured Streaming可以使用<br>
groupby和window表达窗口计算</p>
<pre><code class="language-java">		Logger.getLogger(&quot;org.apache.spark&quot;).setLevel(Level.WARN)
		Logger.getLogger(&quot;org.apache.jetty.server&quot;).setLevel(Level.OFF)

		//1.创建SparkSession
		val spark=SparkSession
			.builder()
			.master(&quot;local[*]&quot;)
			.appName(&quot;printline&quot;)
			.getOrCreate()

		import spark.implicits._
		//字符,时间戳
		var df=spark.readStream .format(&quot;socket&quot;)
			.option(&quot;host&quot;, &quot;localhost&quot;)
			.option(&quot;port&quot;, &quot;9999&quot;)
			.load()

		import org.apache.spark.sql.functions._
		var sdf=new SimpleDateFormat(&quot;mm:ss&quot;)

		val wordCounts=df.select(&quot;value&quot;)
			.as[String]
			.map(_.split(&quot;,&quot;))
			// 这里的Timestamp导如java.sql的依赖
			.map(tokens=&gt;(tokens(0),new Timestamp(tokens(1).toLong)))
			.toDF(&quot;word&quot;,&quot;timestamp&quot;)
			.groupBy(
				window($&quot;timestamp&quot;,&quot;10 seconds&quot;,&quot;5 seconds&quot;),
				$&quot;word&quot;
			)
			.count()
			.map(r=&gt;
				(sdf.format(r.getStruct(0).getTimestamp(0)),
					sdf.format(r.getStruct(0).getTimestamp(1)),
					r.getString(1),r.getLong(2)))
			.toDF(&quot;start&quot;,&quot;end&quot;,&quot;word&quot;,&quot;count&quot;)

		val query = wordCounts.writeStream
			.outputMode(OutputMode.Update())
			.format(&quot;console&quot;)
			.start()


		query.awaitTermination()
</code></pre>
<h2 id="处理延迟-data-和-watermarking">处理延迟 Data 和 Watermarking</h2>
<p>默认情况下，Spark会把落入到时间窗口的数据进行聚合操作。但是需要思考的是Event-Time是基于事<br>
件的时间戳进行窗口聚合的。那就有可能事件窗口已经触发很久了，但是有一些元素因为某种原因，导<br>
致迟到了，这个时候Spark需要将迟到的的数据加入到已经触发的窗口进行重复计算。但是需要注意如<br>
果在长时间的流计算过程中，如果不去限定窗口计算的时间，那么意味着Spark要在内存中一直存储窗<br>
口的状态，这样是不切实际的，因此Spark提供一种称为<code>watermarker</code>的机制用于限定存储在Spark内存<br>
中中间结果存储的时间，这样系统就可以将已经确定触发过的窗口的中间结果给删除。如果后续还有数<br>
据在窗口endtime以后抵达该窗口，Spark把这种数据定义为<code>late数据</code>。其中<code>watermarker</code>计算方式 <code>max event time seen by engine - late threshold</code>如果<code>watermarker</code>的取值大于了时间窗口的<br>
endtime即可认定该窗口的计算结果就可以被丢弃了。如果此时再有数据落入到已经被丢弃的时间窗<br>
口，则该迟到的数据会被Spark放弃更新，也就是丢弃。</p>
<blockquote>
<p>Watermarking=<code>max event time seen by engine - late threshold</code></p>
</blockquote>
<h3 id="watermarking保障机制">Watermarking保障机制：</h3>
<ul>
<li>能够保证在window的EndTime &gt; 水位线的窗口的状态Spark会存储起来，这个时候如果有迟到的<br>
数据再水位线没有淹没window之前Spark可以保障迟到的数据能正常的处理。</li>
<li>如果水位线已经没过了窗口的end时间，那么后续迟到数据不一定能够被处理，换句话说，迟到越<br>
久的数据 被处理的几率越小。</li>
</ul>
<blockquote>
<p>如果是使用<code>水位线计算</code> ，输出模式必须是Update或者Append,否则系统不会删除。</p>
</blockquote>
<pre><code class="language-java">		Logger.getLogger(&quot;org.apache.spark&quot;).setLevel(Level.WARN)
		Logger.getLogger(&quot;org.apache.jetty.server&quot;).setLevel(Level.OFF)

		//1.创建SparkSession
		val spark=SparkSession
			.builder()
			.master(&quot;local[*]&quot;)
			.appName(&quot;printline&quot;)
			.getOrCreate()

		import spark.implicits._
		//字符,时间戳
		var df=spark.readStream .format(&quot;socket&quot;)
			.option(&quot;host&quot;, &quot;localhost&quot;)
			.option(&quot;port&quot;, &quot;9999&quot;)
			.load()

		import org.apache.spark.sql.functions._
		var sdf=new SimpleDateFormat(&quot;mm:ss&quot;)

		val wordCounts=df.select(&quot;value&quot;)
			.as[String]
			.map(_.split(&quot;,&quot;))
			// 这里的Timestamp导如java.sql的依赖
			.map(tokens=&gt;(tokens(0),new Timestamp(tokens(1).toLong)))
			.toDF(&quot;word&quot;,&quot;timestamp&quot;)
			// 与上面窗口的API相比，多了水位线的设置
			.withWatermark(&quot;timestamp&quot;, &quot;5 seconds&quot;)
			.groupBy(
				window($&quot;timestamp&quot;,&quot;10 seconds&quot;,&quot;5 seconds&quot;),
				$&quot;word&quot;
			)
			.count()
			.map(r=&gt;
				(sdf.format(r.getStruct(0).getTimestamp(0)),
					sdf.format(r.getStruct(0).getTimestamp(1)),
					r.getString(1),r.getLong(2)))
			.toDF(&quot;start&quot;,&quot;end&quot;,&quot;word&quot;,&quot;count&quot;)

		val query = wordCounts.writeStream
			.outputMode(OutputMode.Update())
			.format(&quot;console&quot;)
			.start()


		query.awaitTermination()
</code></pre>
<h3 id="spark清除window聚合状态条件">Spark清除window聚合状态条件</h3>
<ul>
<li>Output mode 必须是 Append 或者 Update.，如果是Update 只要窗口有数据更新即可有输出。<br>
如果是Append，必须当水位线没过window的时候才会将Result写出。</li>
</ul>
<h4 id="update-mode">Update Mode</h4>
<figure data-type="image" tabindex="1"><img src="https://img-blog.csdnimg.cn/20200615175338128.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L00yODM1OTIzMzg=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述" loading="lazy"></figure>
<h4 id="append-mode">Append Mode</h4>
<figure data-type="image" tabindex="2"><img src="https://img-blog.csdnimg.cn/20200615175542447.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L00yODM1OTIzMzg=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述" loading="lazy"></figure>
<ul>
<li>必须在分组出现聚合使用时间column/window列</li>
<li>withWaterMaker的时间column必须和groupBy后面时间column保持一致，例如： 错误实例<code>df.withWatermark(&quot;time&quot;, &quot;1 min&quot;).groupBy(&quot;time2&quot;).count()</code>。</li>
<li>一定要在分组聚合之前调用withWaterMaking，例如<code>df.groupBy(&quot;time&quot;).count().withWatermark(&quot;time&quot;, &quot;1 min&quot;)</code> 错误实例<br>
<code>df.withWatermark(&quot;time&quot;, &quot;1 min&quot;).groupBy(&quot;time&quot;).count()</code>正确写法。</li>
</ul>
<h2 id="join-操作">Join 操作</h2>
<p>Structured Streaming 不仅仅支持对静态的 Dataset/DataFrame 做join操作，也支持对streaming<br>
Dataset/DataFrame实现join操作。</p>
<ul>
<li>Stream-static Joins <code>spark-2.0</code> 支持</li>
<li>Stream-stream Joins <code>Spark 2.3</code> 支持</li>
</ul>
<h3 id="stream-static-joins">Stream-static Joins</h3>
<pre><code class="language-java">		//1.创建SparkSession
		val spark=SparkSession
			.builder()
			.master(&quot;local[6]&quot;)
			.appName(&quot;printline&quot;)
			.getOrCreate()

		import spark.implicits._

		import org.apache.spark.sql.functions._

		/**
		 * +---+------+---+
		 * | id| name|age|
		 * +---+------+---+
		 * | 1| 张三 | 18|
		 * | 2| lisi| 28|
		 * | 3|wangwu| 38|
		 * +---+------+---+
		 */
		val userDF=spark.read
			.format(&quot;json&quot;)
			.load(&quot;/Users/mashikang/IdeaProjects/structured_stream/src/main/resources/json&quot;)
			.selectExpr(&quot;CAST(id AS INTEGER)&quot;,&quot;name&quot;,&quot;CAST(age AS INTEGER)&quot;)

		//1 apple 1 4.5
		var orderItemDF= spark.readStream
			.format(&quot;socket&quot;)
			.option(&quot;host&quot;,&quot;localhost&quot;)
			.option(&quot;port&quot;,9999)
			.load() .as[String]
			.map(line=&gt;line.split(&quot;\\s+&quot;))
			.map(tokens=&gt;(tokens(0).toInt,tokens(1),tokens(2).toInt,tokens(3).toDouble))
			.toDF(&quot;uid&quot;,&quot;item&quot;,&quot;count&quot;,&quot;price&quot;)

		val jointResults = orderItemDF.join(userDF,$&quot;id&quot;===$&quot;uid&quot;,&quot;left_outer&quot;)

		val query = jointResults
			.writeStream
			.format(&quot;console&quot;)
			.outputMode(OutputMode.Append())
			.start()

		query.awaitTermination()
</code></pre>
<h3 id="stream-stream-joins">Stream-stream Joins</h3>
<blockquote>
<ul>
<li>两边流都必须声明watermarker，告知引擎什么是可以清楚状态（默认取最低）。</li>
<li>需要在连接条件中添加eventTime column的时间约束，这样引擎就知道什么时候可以清除后续<br>
的流的状态。
<ul>
<li>Time range join conditions</li>
<li>Join on event-time windows</li>
</ul>
</li>
</ul>
</blockquote>
<h4 id="inner-join">inner join</h4>
<ul>
<li><strong>方案1 Time range join conditions</strong></li>
</ul>
<pre><code class="language-java">		//1.创建SparkSession
		val spark = SparkSession
			.builder()
			.master(&quot;local[*]&quot;)
			.appName(&quot;printline&quot;)
			.getOrCreate()

		import spark.implicits._

		//001 apple 1 4.5 1566529410000
		val orderDF = spark
			.readStream
			.format(&quot;socket&quot;)
			.option(&quot;host&quot;, &quot;localhost&quot;)
			.option(&quot;port&quot;, 9999)
			.load()
			.map(row =&gt; row.getAs[String](&quot;value&quot;).split(&quot;\\s+&quot;))
			.map(t =&gt; (t(0), t(1), (t(2).toInt * t(3).toDouble), new Timestamp(t(4).toLong)))
			.toDF(&quot;uid&quot;, &quot;item&quot;, &quot;cost&quot;, &quot;order_time&quot;)

		//001 zhangsan 1566529410000
		val userDF = spark
			.readStream
			.format(&quot;socket&quot;)
			.option(&quot;host&quot;, &quot;localhost&quot;)
			.option(&quot;port&quot;, 8888)
			.load()
			.map(row =&gt; row.getAs[String](&quot;value&quot;).split(&quot;\\s+&quot;))
			.map(t =&gt; (t(0), t(1), new Timestamp(t(2).toLong)))
			.toDF(&quot;id&quot;, &quot;name&quot;, &quot;login_time&quot;)

		import org.apache.spark.sql.functions._

		//用户的登陆数据缓存 2 seconds 订单数据缓存4秒
		val userWatermarker = userDF.withWatermark(&quot;login_time&quot;, &quot;2 seconds&quot;)
		val orderWaterMarker = orderDF.withWatermark(&quot;order_time&quot;, &quot;4 seconds&quot;)

		//连接 用户登陆以后将2秒以内的购买行为和用进行join 
		val joinDF = userWatermarker.join(orderWaterMarker,
			expr(
				&quot;&quot;&quot;
				  |id=uid and order_time &gt;= login_time and order_time &lt;= login_time + interval 2 seconds 
				&quot;&quot;&quot;.stripMargin)
		)
		val query = joinDF.writeStream
			.format(&quot;console&quot;)
			.outputMode(OutputMode.Append()).start() 
		
		query.awaitTermination()
</code></pre>
<ul>
<li><strong>方案2Join on event-time windows</strong></li>
</ul>
<pre><code class="language-java">	//1.创建SparkSession
		val spark = SparkSession
			.builder()
			.master(&quot;local[*]&quot;)
			.appName(&quot;printline&quot;)
			.getOrCreate()

		import spark.implicits._

		//001 apple 1 4.5 1566529410000
		val orderDF = spark
			.readStream
			.format(&quot;socket&quot;)
			.option(&quot;host&quot;, &quot;localhost&quot;)
			.option(&quot;port&quot;, 9999)
			.load()
			.map(row =&gt; row.getAs[String](&quot;value&quot;).split(&quot;\\s+&quot;))
			.map(t =&gt; (t(0), t(1), (t(2).toInt * t(3).toDouble), new Timestamp(t(4).toLong)))
			.toDF(&quot;uid&quot;, &quot;item&quot;, &quot;cost&quot;, &quot;order_time&quot;)

		//001 zhangsan 1566529410000
		val userDF = spark
			.readStream
			.format(&quot;socket&quot;)
			.option(&quot;host&quot;, &quot;localhost&quot;)
			.option(&quot;port&quot;, 8888)
			.load()
			.map(row =&gt; row.getAs[String](&quot;value&quot;).split(&quot;\\s+&quot;))
			.map(t =&gt; (t(0), t(1), new Timestamp(t(2).toLong)))
			.toDF(&quot;id&quot;, &quot;name&quot;, &quot;login_time&quot;)

		import org.apache.spark.sql.functions._

		//用户的登陆数据缓存 2 seconds 订单数据缓存4秒
		val userWatermarker = userDF.withWatermark(&quot;login_time&quot;, &quot;2 seconds&quot;)
			.select(
				window($&quot;login_time&quot;, &quot;5 seconds&quot;),
				$&quot;id&quot;, $&quot;name&quot;, $&quot;login_time&quot;)
			.withColumnRenamed(&quot;window&quot;, &quot;leftWindow&quot;)

		val orderWaterMarker = orderDF.withWatermark(&quot;order_time&quot;, &quot;4 seconds&quot;)
			.select(
				window($&quot;order_time&quot;, &quot;5 seconds&quot;),
				$&quot;uid&quot;, $&quot;item&quot;, $&quot;cost&quot;, $&quot;order_time&quot;)
			.withColumnRenamed(&quot;window&quot;, &quot;rightWindow&quot;)

		//连接用户登陆以后将2秒以内的购买行为和用进行join
		val joinDF = userWatermarker
			.join(
				orderWaterMarker,
				expr(
					&quot;&quot;&quot;
					  |id=uid and leftWindow = rightWindow
					&quot;&quot;&quot;.stripMargin)
			)
			
		val query = joinDF.writeStream
			.format(&quot;console&quot;)
			.outputMode(OutputMode.Append()).start() 
		
		query.awaitTermination()
</code></pre>
<h4 id="outer-join">outer join</h4>
<pre><code class="language-java">	//1.创建SparkSession
		val spark = SparkSession
			.builder()
			.master(&quot;local[*]&quot;)
			.appName(&quot;printline&quot;)
			.getOrCreate()

		import spark.implicits._

		//001 apple 1 4.5 1566529410000
		val orderDF = spark
			.readStream
			.format(&quot;socket&quot;)
			.option(&quot;host&quot;, &quot;localhost&quot;)
			.option(&quot;port&quot;, 9999)
			.load()
			.map(row =&gt; row.getAs[String](&quot;value&quot;).split(&quot;\\s+&quot;))
			.map(t =&gt; (t(0), t(1), (t(2).toInt * t(3).toDouble), new Timestamp(t(4).toLong)))
			.toDF(&quot;uid&quot;, &quot;item&quot;, &quot;cost&quot;, &quot;order_time&quot;)

		//001 zhangsan 1566529410000
		val userDF = spark
			.readStream
			.format(&quot;socket&quot;)
			.option(&quot;host&quot;, &quot;localhost&quot;)
			.option(&quot;port&quot;, 8888)
			.load()
			.map(row =&gt; row.getAs[String](&quot;value&quot;).split(&quot;\\s+&quot;))
			.map(t =&gt; (t(0), t(1), new Timestamp(t(2).toLong)))
			.toDF(&quot;id&quot;, &quot;name&quot;, &quot;login_time&quot;)

		import org.apache.spark.sql.functions._

		//系统分别会对 user 和 order 缓存 最近 1 seconds 和 2 seconds 数据,
		// 一旦时间过去，系统就无 法保证数据状态继续保留
		val loginWatermarker=userDF.withWatermark(&quot;login_time&quot;,&quot;1 second&quot;)
		val orderWatermarker=orderDF.withWatermark(&quot;order_time&quot;,&quot;2 seconds&quot;)

		//计算订单的时间 &amp; 用户 登陆之后的0~1 seconds 关联 数据 并且进行join
		val joinDF = loginWatermarker
		.join(
				orderWatermarker,
				expr(&quot;uid=id and order_time &gt;= login_time and order_time &lt;= login_time + interval 1 seconds&quot;),
				&quot;leftOuter&quot;
			)
			
		val query = joinDF.writeStream
			.format(&quot;console&quot;)
			.outputMode(OutputMode.Append()).start() 
		
		query.awaitTermination()
</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Spark入门(七)——最全的Saprk SQL算子介绍与使用(下)]]></title>
        <id>https://mask0407.github.io/spark06/</id>
        <link href="https://mask0407.github.io/spark06/">
        </link>
        <updated>2020-06-29T03:16:05.000Z</updated>
        <content type="html"><![CDATA[<p><ul class="markdownIt-TOC">
<li>
<ul>
<li><a href="#sql%E8%AF%AD%E6%B3%95%E6%9F%A5%E8%AF%A2">SQL语法查询</a>
<ul>
<li><a href="#%E5%8D%95%E8%A1%8C%E6%9F%A5%E8%AF%A2">单行查询</a></li>
<li><a href="#%E6%A8%A1%E7%B3%8A%E6%9F%A5%E8%AF%A2">模糊查询</a></li>
<li><a href="#%E6%8E%92%E5%BA%8F%E6%9F%A5%E8%AF%A2">排序查询</a></li>
<li><a href="#limit%E6%9F%A5%E8%AF%A2">limit查询</a></li>
<li><a href="#%E5%88%86%E7%BB%84%E6%9F%A5%E8%AF%A2">分组查询</a></li>
<li><a href="#having%E8%BF%87%E6%BB%A4">having过滤</a></li>
<li><a href="#case-when">case-when</a></li>
<li><a href="#%E8%A1%8C%E8%BD%AC%E5%88%97">行转列</a></li>
<li><a href="#pivot">pivot</a></li>
<li><a href="#cube%E8%AE%A1%E7%AE%97">Cube计算</a></li>
<li><a href="#join%E8%A1%A8%E8%BF%9E%E6%8E%A5">Join表连接</a></li>
<li><a href="#%E5%AD%90%E6%9F%A5%E8%AF%A2">子查询</a></li>
<li><a href="#%E5%BC%80%E7%AA%97%E5%87%BD%E6%95%B0">开窗函数</a>
<ul>
<li><a href="#%E5%BC%80%E7%AA%97%E5%87%BD%E6%95%B0sql%E8%A7%A3%E8%AF%BB">开窗函数SQL解读</a></li>
<li><a href="#row_num">ROW_NUM</a></li>
<li><a href="#rank">RANK()</a></li>
<li><a href="#dense_rank-%E5%AF%86%E9%9B%86%E6%8E%92%E5%90%8D">DENSE_RANK() /密集排名</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#%E8%87%AA%E5%AE%9A%E4%B9%89%E5%87%BD%E6%95%B0">自定义函数</a>
<ul>
<li><a href="#%E5%8D%95%E8%A1%8C%E5%87%BD%E6%95%B0">单行函数</a></li>
<li><a href="#%E8%81%9A%E5%90%88%E5%87%BD%E6%95%B0untyped">聚合函数（untyped）</a></li>
</ul>
</li>
<li><a href="#loadsave">Load/Save</a>
<ul>
<li><a href="#paquet">Paquet</a></li>
<li><a href="#json">JSON</a></li>
<li><a href="#orc%E5%AD%98%E5%82%A8%E5%8E%8B%E7%BC%A9%E6%A0%BC%E5%BC%8F%E6%AF%94%E8%BE%83%E8%8A%82%E7%9C%81%E7%A9%BA%E9%97%B4">ORC(存储压缩格式，比较节省空间)</a></li>
<li><a href="#csv">CSV</a></li>
<li><a href="#jdbc">JDBC</a></li>
<li><a href="#dataframe%E8%BD%AC%E4%B8%BArdd">DataFrame转为RDD</a></li>
</ul>
</li>
</ul>
</li>
</ul>
(Spark SQL 查询)</p>
<h2 id="sql语法查询">SQL语法查询</h2>
<h3 id="单行查询">单行查询</h3>
<pre><code class="language-java">// 单行查询
var userDF = List((1, &quot;张三&quot;, true, 18, 15000, 1))
	.toDF(&quot;id&quot;, &quot;name&quot;, &quot;sex&quot;, &quot;age&quot;, &quot;salary&quot;, &quot;dept&quot;)
	
userDF.createTempView(&quot;t_employee&quot;)

val sql = &quot;select * from t_employee where name = '张三'&quot;
spark.sql(sql)
	.show()
</code></pre>
<pre><code>+---+----+----+---+------+----+
| id|name| sex|age|salary|dept|
+---+----+----+---+------+----+
|  1|张三|true| 18| 15000|   1|
+---+----+----+---+------+----+
</code></pre>
<h3 id="模糊查询">模糊查询</h3>
<pre><code class="language-java">var userDF= List((1,&quot;张三&quot;,true,18,15000,1))
	.toDF(&quot;id&quot;,&quot;name&quot;,&quot;sex&quot;,&quot;age&quot;,&quot;salary&quot;,&quot;dept&quot;)
	
userDF.createTempView(&quot;t_employee&quot;)

val sql=&quot;select * from t_employee where name like '%三%'&quot;
spark.sql(sql)
	.show()
</code></pre>
<pre><code>+---+----+----+---+------+----+
| id|name| sex|age|salary|dept|
+---+----+----+---+------+----+
|  1|张三|true| 18| 15000|   1|
+---+----+----+---+------+----+
</code></pre>
<h3 id="排序查询">排序查询</h3>
<pre><code class="language-java">var userDF = List((1, &quot;张三&quot;, true, 18, 15000, 1), (2, &quot;ls&quot;, false, 18, 12000, 1))
	.toDF(&quot;id&quot;, &quot;name&quot;, &quot;sex&quot;, &quot;age&quot;, &quot;salary&quot;, &quot;dept&quot;)
//构建视图
userDF.createTempView(&quot;t_employee&quot;)
val sql =
	&quot;&quot;&quot;
	  |select * from t_employee where salary &gt; 10000 order by salary desc
	&quot;&quot;&quot;
	.stripMargin
spark.sql(sql)
	.show()
</code></pre>
<pre><code>+---+----+-----+---+------+----+
| id|name|  sex|age|salary|dept|
+---+----+-----+---+------+----+
|  1|张三| true| 18| 15000|   1|
|  2|李四|false| 18| 12000|   1|
+---+----+-----+---+------+----+
</code></pre>
<h3 id="limit查询">limit查询</h3>
<pre><code class="language-java">var userDF= List( (1,&quot;张三&quot;,true,18,15000,1), 
	(2,&quot;李四&quot;,false,18,12000,1), 
	(3,&quot;王五&quot;,false,18,16000,2)
) .toDF(&quot;id&quot;,&quot;name&quot;,&quot;sex&quot;,&quot;age&quot;,&quot;salary&quot;,&quot;dept&quot;)
 
//构建视图 
userDF.createTempView(&quot;t_employee&quot;) 
val sql=
	&quot;&quot;&quot;
	  |select * from t_employee where salary &gt; 10000 order by salary desc limit 2 
	&quot;&quot;&quot;.stripMargin
spark.sql(sql)
	.show()
</code></pre>
<pre><code>+---+----+-----+---+------+----+
| id|name|  sex|age|salary|dept|
+---+----+-----+---+------+----+
|  3|王五|false| 18| 16000|   2|
|  1|张三| true| 18| 15000|   1|
+---+----+-----+---+------+----+
</code></pre>
<h3 id="分组查询">分组查询</h3>
<pre><code class="language-java">var userDF= List( (1,&quot;张三&quot;,true,18,15000,1),
	(2,&quot;李四&quot;,false,18,12000,1),
	(3,&quot;王五&quot;,false,18,16000,2)
) .toDF(&quot;id&quot;,&quot;name&quot;,&quot;sex&quot;,&quot;age&quot;,&quot;salary&quot;,&quot;dept&quot;)

//构建视图
userDF.createTempView(&quot;t_employee&quot;)
val sql=
	&quot;&quot;&quot;
	  |select dept ,avg(salary) as avg_slalary from t_employee
	  |group by dept order by avg_slalary desc
	&quot;&quot;&quot;.stripMargin
spark.sql(sql)
	.show()
</code></pre>
<pre><code>+----+-----------+
|dept|avg_slalary|
+----+-----------+
|   2|    16000.0|
|   1|    13500.0|
+----+-----------+
</code></pre>
<h3 id="having过滤">having过滤</h3>
<pre><code class="language-java">var userDF= List( (1,&quot;张三&quot;,true,18,15000,1),
	(2,&quot;李四&quot;,false,18,12000,1),
	(3,&quot;王五&quot;,false,18,16000,2)
) .toDF(&quot;id&quot;,&quot;name&quot;,&quot;sex&quot;,&quot;age&quot;,&quot;salary&quot;,&quot;dept&quot;)

//构建视图
userDF.createTempView(&quot;t_employee&quot;)
val sql=
	&quot;&quot;&quot;
	  | select dept ,avg(salary) as avg_slalary
	  | from t_employee group by dept 
	  | having avg_slalary &gt; 13500 
	  | order by avg_slalary desc
	&quot;&quot;&quot;.stripMargin
spark.sql(sql)
	.show()
</code></pre>
<pre><code>+----+-----------+
|dept|avg_slalary|
+----+-----------+
|   2|    16000.0|
+----+-----------+
</code></pre>
<h3 id="case-when">case-when</h3>
<pre><code class="language-java">var userDF= List( (1,&quot;张三&quot;,true,18,15000,1),
	(2,&quot;李四&quot;,false,18,12000,1),
	(3,&quot;王五&quot;,false,18,16000,2)
) .toDF(&quot;id&quot;,&quot;name&quot;,&quot;sex&quot;,&quot;age&quot;,&quot;salary&quot;,&quot;dept&quot;)

//构建视图
userDF.createTempView(&quot;t_employee&quot;)
val sql=
	&quot;&quot;&quot;
	  |select id,name,case sex when true then '男' else '女' end as sex_alias
	  |from t_employee
	&quot;&quot;&quot;.stripMargin
spark.sql(sql)
	.show()
</code></pre>
<pre><code>+---+----+---------+
| id|name|sex_alias|
+---+----+---------+
|  1|张三|       男|
|  2|李四|       女|
|  3|王五|       女|
+---+----+---------+
</code></pre>
<h3 id="行转列">行转列</h3>
<pre><code class="language-java">// 行转列
var scoreDF = List(
	(1, &quot;语文&quot;, 100),
	(1, &quot;数学&quot;, 100),
	(1, &quot;英语&quot;, 100),
	(2, &quot;数学&quot;, 79),
	(2, &quot;语文&quot;, 80),
	(2, &quot;英语&quot;, 100)
).toDF(&quot;id&quot;, &quot;course&quot;, &quot;score&quot;)
scoreDF.createOrReplaceTempView(&quot;t_course&quot;)

val sql =
	&quot;&quot;&quot;
	  |  select id,
	  |  max(case course when '数学' then score else 0 end) as math,
	  |  max(case course when '英语' then score else 0 end) as english,
	  |  max(case course when '语文' then score else 0 end) as chinese
	  |  from t_course group by id
	&quot;&quot;&quot;.stripMargin
	
spark.sql(sql)
	.show()
</code></pre>
<pre><code>+---+----+-------+-------+
| id|math|english|chinese|
+---+----+-------+-------+
|  1| 100|    100|    100|
|  2|  79|    100|     80|
+---+----+-------+-------+
</code></pre>
<h3 id="pivot">pivot</h3>
<pre><code class="language-java">var scoreDF = List(
	(1, &quot;语文&quot;, 100),
	(1, &quot;数学&quot;, 100),
	(1, &quot;英语&quot;, 100),
	(2, &quot;数学&quot;, 79),
	(2, &quot;语文&quot;, 80),
	(2, &quot;英语&quot;, 100)
).toDF(&quot;id&quot;, &quot;course&quot;, &quot;score&quot;)
scoreDF.createOrReplaceTempView(&quot;t_course&quot;)

val sql =
	&quot;&quot;&quot;
	  |select * 
	  |from t_course 
	  |pivot(max(score) for course in ('数学' ,'语文','英语'))
	  |
	&quot;&quot;&quot;.stripMargin
	  
spark.sql(sql)
	.show()
</code></pre>
<pre><code>+---+----+----+----+
| id|数学|语文|英语|
+---+----+----+----+
|  1| 100| 100| 100|
|  2|  79|  80| 100|
+---+----+----+----+
</code></pre>
<blockquote>
<p>在书写SQL的时候除去聚合字段和输出列明字段，其他字段作为groupby后的隐藏字段。</p>
</blockquote>
<h3 id="cube计算">Cube计算</h3>
<pre><code class="language-java">// Cube计算
val frame = List(
	(110, 50, 80, 80),
	(120, 60, 95, 75),
	(120, 50, 96, 70)
) .toDF(&quot;height&quot;, &quot;weight&quot;, &quot;uiq&quot;, &quot;ueq&quot;)
frame.createTempView(&quot;t_user&quot;)

val sql=
	&quot;&quot;&quot;
	  |select height,weight,avg(uiq),avg(ueq)
	  |from t_user
	  |group by cube(height,weight)
	&quot;&quot;&quot;.stripMargin
	
spark.sql(sql)
	.show()
</code></pre>
<pre><code>+------+------+-----------------+--------+
|height|weight|         avg(uiq)|avg(ueq)|
+------+------+-----------------+--------+
|   110|    50|             80.0|    80.0|
|   120|  null|             95.5|    72.5|
|   120|    60|             95.0|    75.0|
|  null|    60|             95.0|    75.0|	 // weight 是60的所有数据 的uiq、ueq平均值
|  null|  null|90.33333333333333|    75.0|   // 所有数据的uiq、ueq平均值
|   120|    50|             96.0|    70.0|
|   110|  null|             80.0|    80.0|
|  null|    50|             88.0|    75.0|
+------+------+-----------------+--------+
</code></pre>
<h3 id="join表连接">Join表连接</h3>
<pre><code class="language-java">// join
val userCatagoryCostDF=List(
	(1,&quot;电脑配件&quot;,100),
	(1,&quot;母婴用品&quot;,100), 
	(1,&quot;生活用品&quot;,100),
	(2,&quot;居家美食&quot;,79),
	(2,&quot;消费电子&quot;,80),
	(2,&quot;生活用品&quot;,100)
).toDF(&quot;uid&quot;,&quot;category&quot;,&quot;cost&quot;)

val usersDF= List(
	(1,&quot;张晓三&quot;,true,18,15000),
	(2,&quot;李晓四&quot;,true,18,18000),
	(3,&quot;王晓五&quot;,false,18,10000)
).toDF(&quot;id&quot;,&quot;name&quot;,&quot;sex&quot;,&quot;age&quot;,&quot;salary&quot;)

usersDF.createTempView(&quot;t_user&quot;)
userCatagoryCostDF.createTempView(&quot;t_user_cost&quot;)

val sql =
	&quot;&quot;&quot;
	  |select u.*,o.*
	  |from t_user u
	  |left join t_user_cost o
	  |on u.id=o.uid
	  |where uid is not null
	&quot;&quot;&quot;.stripMargin
	
spark.sql(sql)
	.show()
</code></pre>
<pre><code>+---+------+----+---+------+---+--------+----+
| id|  name| sex|age|salary|uid|category|cost|
+---+------+----+---+------+---+--------+----+
|  1|张晓三|true| 18| 15000|  1|电脑配件| 100|
|  1|张晓三|true| 18| 15000|  1|母婴用品| 100|
|  1|张晓三|true| 18| 15000|  1|生活用品| 100|
|  2|李晓四|true| 18| 18000|  2|居家美食|  79|
|  2|李晓四|true| 18| 18000|  2|消费电子|  80|
|  2|李晓四|true| 18| 18000|  2|生活用品| 100|
+---+------+----+---+------+---+--------+----+
</code></pre>
<h3 id="子查询">子查询</h3>
<pre><code class="language-java">// 子查询
var df=List(
	(1,&quot;zs&quot;,true,1,15000),
	(2,&quot;ls&quot;,false,2,18000),
	(3,&quot;ww&quot;,false,2,14000),
	(4,&quot;zl&quot;,false,1,18000),
	(5,&quot;win7&quot;,false,1,16000)
).toDF(&quot;id&quot;,&quot;name&quot;,&quot;sex&quot;,&quot;dept&quot;,&quot;salary&quot;)
df.createTempView(&quot;t_employee&quot;)

val sql=
	&quot;&quot;&quot;
	  |select id,name,salary,dept,
	  |(select avg(salary) from t_employee t2 where t1.dept=t2.dept) as avg_salary
	  |from t_employee t1
	  |order by dept desc
	&quot;&quot;&quot;.stripMargin
	
spark.sql(sql)
	.show()
</code></pre>
<pre><code>+---+----+------+----+------------------+
| id|name|salary|dept|        avg_salary|
+---+----+------+----+------------------+
|  2|  ls| 18000|   2|           16000.0|
|  3|  ww| 14000|   2|           16000.0|
|  5|win7| 16000|   1|16333.333333333334|
|  1|  zs| 15000|   1|16333.333333333334|
|  4|  zl| 18000|   1|16333.333333333334|
+---+----+------+----+------------------+
</code></pre>
<blockquote>
<p>在spark SQL不允许在子查询中使用非等值连接。（MySQL|Oracle支持）</p>
</blockquote>
<h3 id="开窗函数">开窗函数</h3>
<p>在正常的统计分析中 ，通常使用聚合函数作为分析，聚合分析函数的特点是将n行记录合并成一行，在数据库的统计当中 还有一种统计称为开窗统计，开窗函数可以实现将一行变成多行。可以将数据库查询的每一条记录比作是一幢高楼的一 层, 开窗函数就是在每一层开一扇窗, 让每一层能看到整装楼的全貌或一部分。</p>
<pre><code class="language-java">// 开窗函数
var df=List(
	(1,&quot;zs&quot;,true,1,15000),
	(2,&quot;ls&quot;,false,2,18000),
	(3,&quot;ww&quot;,false,2,14000),
	(4,&quot;zl&quot;,false,1,18000),
	(5,&quot;win7&quot;,false,1,16000)
).toDF(&quot;id&quot;,&quot;name&quot;,&quot;sex&quot;,&quot;dept&quot;,&quot;salary&quot;)
df.createTempView(&quot;t_employee&quot;)

val sql=
	&quot;&quot;&quot;
	  |select id,name,salary,dept,
	  |count(id) over(partition by dept order by salary desc) as rank,
	  |(count(id) over(partition by dept order by salary desc rows between current row and unbounded following) - 1) as low_than_me,
	  |avg(salary) over(partition by dept rows between unbounded preceding and unbounded following) as avg_salary,
	  |avg(salary) over() as all_avg_salary 
	  |from t_employee t1 order by dept desc
	&quot;&quot;&quot;.stripMargin
	
spark.sql(sql)
	.show()
spark.stop()
</code></pre>
<pre><code>+---+----+------+----+----+-----------+------------------+--------------+
| id|name|salary|dept|rank|low_than_me|        avg_salary|all_avg_salary|
+---+----+------+----+----+-----------+------------------+--------------+
|  2|  ls| 18000|   2|   1|          1|           16000.0|       16200.0|
|  3|  ww| 14000|   2|   2|          0|           16000.0|       16200.0|
|  4|  zl| 18000|   1|   1|          2|16333.333333333334|       16200.0|
|  5|win7| 16000|   1|   2|          1|16333.333333333334|       16200.0|
|  1|  zs| 15000|   1|   3|          0|16333.333333333334|       16200.0|
+---+----+------+----+----+-----------+------------------+--------------+
</code></pre>
<h4 id="开窗函数sql解读">开窗函数SQL解读</h4>
<pre><code class="language-sql">	  select id,name,salary,dept,
	  # 按部门分组、工资倒叙排序展示 当前部门的id总数
	  count(id) over(partition by dept order by salary desc) as rank,
	  # 按部门分组、工资倒叙排序展示当前行至最后一行id总数-1
	  (count(id) over(partition by dept order by salary desc rows between current row and unbounded following) - 1) as low_than_me,
	  # 按部门分组展示首行至尾行的平均工资 如：2部门平均工资16000 1部门平均工资16333.333333333334
	  avg(salary) over(partition by dept rows between unbounded preceding and unbounded following) as avg_salary,
	  # 展示所有员工的平均工资
	  avg(salary) over() as all_avg_salary 
	  from t_employee t1 order by dept desc
</code></pre>
<ul>
<li>总结</li>
</ul>
<blockquote>
<p>聚合函数(字段) over ([[partition by 字段] order by 字段 asc [rows between 起始行偏移量 and 终止偏移量]] )</p>
</blockquote>
<ul>
<li>其中：偏移量的取值</li>
</ul>
<blockquote>
<p>preceding：用于累加前N行（分区之内）。若是从分区第一行头开始，则为 unbounded。 N为：相对当前行向前 的偏移量<code>负数</code>。<br>
following:与preceding相反，累加后N行（分区之内）。若是累加到该分区结束则为unbounded。N为：相对当 前行向后的偏移量<code>正数</code> current row：顾名思义，当前行，偏移量为0</p>
</blockquote>
<h4 id="row_num">ROW_NUM</h4>
<p>统计当前记录所在的行号</p>
<pre><code class="language-java">// ROW_NUM
var df=List(
	(1,&quot;zs&quot;,true,1,15000),
	(2,&quot;ls&quot;,false,2,18000),
	(3,&quot;ww&quot;,false,2,14000),
	(4,&quot;zl&quot;,false,1,18000),
	(5,&quot;win7&quot;,false,1,16000)
).toDF(&quot;id&quot;,&quot;name&quot;,&quot;sex&quot;,&quot;dept&quot;,&quot;salary&quot;)
df.createTempView(&quot;t_employee&quot;)

val sql=
	&quot;&quot;&quot;
	  |select id,name,salary,dept,
	  |ROW_NUMBER() over(partition by dept order by salary desc) as rank
	  |from t_employee t1
	  |order by dept desc
	&quot;&quot;&quot;.stripMargin
	
spark.sql(sql)
	.show()
</code></pre>
<pre><code>+---+----+------+----+----+
| id|name|salary|dept|rank|
+---+----+------+----+----+
|  2|  ls| 18000|   2|   1|
|  3|  ww| 14000|   2|   2|
|  4|  zl| 18000|   1|   1|
|  5|win7| 16000|   1|   2|
|  1|  zs| 15000|   1|   3|
+---+----+------+----+----+
</code></pre>
<blockquote>
<p>如果部门存在相同薪资此时ROW_NUMBER只能表示当前记录在窗口行标</p>
</blockquote>
<h4 id="rank">RANK()</h4>
<pre><code class="language-java">// RANK
var df=List(
	(1,&quot;zs&quot;,true,1,15000),
	(2,&quot;ls&quot;,false,2,18000),
	(3,&quot;ww&quot;,false,2,14000),
	(4,&quot;zl&quot;,false,1,18000),
	(6,&quot;zl1&quot;,true,1,18000),
	(5,&quot;win7&quot;,false,1,16000)
).toDF(&quot;id&quot;,&quot;name&quot;,&quot;sex&quot;,&quot;dept&quot;,&quot;salary&quot;)
df.createTempView(&quot;t_employee&quot;)

val sql=
	&quot;&quot;&quot;
	  |select id,name,salary,dept,
	  |RANK() over(partition by dept order by salary desc) as rank
	  |from t_employee t1
	  |order by dept desc
	&quot;&quot;&quot;.stripMargin
	
spark.sql(sql)
	.show()
</code></pre>
<pre><code>+---+----+------+----+----+
| id|name|salary|dept|rank|
+---+----+------+----+----+
|  2|  ls| 18000|   2|   1|
|  3|  ww| 14000|   2|   2|
|  4|  zl| 18000|   1|   1|
|  6| zl1| 18000|   1|   1|
|  5|win7| 16000|   1|   3| //因为出现两个排名为1的，所有这里是3，故而排名序号不连续
|  1|  zs| 15000|   1|   4|
+---+----+------+----+----+
</code></pre>
<blockquote>
<p>与ROW_NUM相比，排名特点是不连续。</p>
</blockquote>
<h4 id="dense_rank-密集排名">DENSE_RANK() /密集排名</h4>
<pre><code class="language-java">// DENSE_RANK/密集排名
var df = List(
	(1, &quot;zs&quot;, true, 1, 15000),
	(2, &quot;ls&quot;, false, 2, 18000),
	(3, &quot;ww&quot;, false, 2, 14000),
	(4, &quot;zl&quot;, false, 1, 18000),
	(6, &quot;zl1&quot;, true, 1, 18000),
	(5, &quot;win7&quot;, false, 1, 16000)
).toDF(&quot;id&quot;, &quot;name&quot;, &quot;sex&quot;, &quot;dept&quot;, &quot;salary&quot;)
df.createTempView(&quot;t_employee&quot;)

val sql =
	&quot;&quot;&quot;
	  |select id,name,salary,dept,
	  |DENSE_RANK() over(partition by dept order by salary desc) as rank
	  |from t_employee t1
	  |order by dept desc
	&quot;&quot;&quot;.stripMargin
	
spark.sql(sql)
	.show()
</code></pre>
<pre><code>+---+----+------+----+----+
| id|name|salary|dept|rank|
+---+----+------+----+----+
|  3|  ww| 14000|   2|   2|
|  2|  ls| 18000|   2|   1|
|  4|  zl| 18000|   1|   1|
|  6| zl1| 18000|   1|   1|
|  1|  zs| 15000|   1|   3|
|  5|win7| 16000|   1|   2|
+---+----+------+----+----+
</code></pre>
<h2 id="自定义函数">自定义函数</h2>
<h3 id="单行函数">单行函数</h3>
<pre><code class="language-java">// 自定义单行函数
var df = List(
	(1, &quot;zs&quot;, true, 1, 15000),
	(2, &quot;ls&quot;, false, 2, 18000),
	(3, &quot;ww&quot;, false, 2, 14000),
	(4, &quot;zl&quot;, false, 1, 18000),
	(6, &quot;zl1&quot;, true, 1, 18000),
	(5, &quot;win7&quot;, false, 1, 16000)
).toDF(&quot;id&quot;, &quot;name&quot;, &quot;sex&quot;, &quot;dept&quot;, &quot;salary&quot;)
df.createTempView(&quot;t_employee&quot;)

spark.udf
	.register(&quot;convertSex&quot;, (sex: Boolean) =&gt; {
		sex match {
			case true =&gt; &quot;男&quot;
			case false =&gt; &quot;女&quot;
		}
	})
	
val sql =
	&quot;&quot;&quot;
	  |select id,name,convertSex(sex) as usex
	  |from t_employee
	&quot;&quot;&quot;.stripMargin
	
spark.sql(sql)
	.show()
</code></pre>
<pre><code>+---+----+----+
| id|name|usex|
+---+----+----+
|  1|  zs|  男|
|  2|  ls|  女|
|  3|  ww|  女|
|  4|  zl|  女|
|  6| zl1|  男|
|  5|win7|  女|
+---+----+----+
</code></pre>
<h3 id="聚合函数untyped">聚合函数（untyped）</h3>
<p>只需要写一个类继承 <code>UserDefinedAggregateFunction</code> 即可。</p>
<pre><code class="language-java">import org.apache.spark.sql.Row
import org.apache.spark.sql.expressions.{MutableAggregationBuffer, UserDefinedAggregateFunction}
import org.apache.spark.sql.types.{DataType, DoubleType, StructType}

class CustomSum extends UserDefinedAggregateFunction {
	//1.输入的字段类型信息 name属性 叫什么无所谓
	override def inputSchema: StructType = {
		new StructType().add(&quot;salary&quot;, DoubleType)
	}

	//2.中间结果变量类型
	override def bufferSchema: StructType = {
		new StructType().add(&quot;taotalsalary&quot;, DoubleType)
	}

	//3.最终返回结果的类型
	override def dataType: DataType = DoubleType

	//4.设置返回结果类型是否固定
	override def deterministic: Boolean = true

	//5.初始化中间结果
	override def initialize(buffer: MutableAggregationBuffer): Unit = {
		//第0个位置元素是0.0
		buffer.update(0, 0.0)
	}

	//6.将传如的数值添加到中间结果变量中
	override def update(buffer: MutableAggregationBuffer, input: Row): Unit = {
		val history = buffer.getAs[Double](0)
		val current = input.getAs[Double](0)
		buffer.update(0, history + current)
	}

	//7.将局部结果聚合到buffer1中
	override def merge(buffer1: MutableAggregationBuffer, buffer2: Row): Unit = {
		val result = buffer1.getAs[Double](0) + buffer2.getAs[Double](0)
		buffer1.update(0, result)
	}

	//8.返回最终结果 
	override def evaluate(buffer: Row): Any = {
		buffer.getAs[Double](0)
	}
}
</code></pre>
<ul>
<li>spark 代码</li>
</ul>
<pre><code class="language-java">// 自定义聚合函数（untyped）
var df = List(
	(1, &quot;zs&quot;, true, 1, 15000),
	(2, &quot;ls&quot;, false, 2, 18000),
	(3, &quot;ww&quot;, false, 2, 14000),
	(4, &quot;zl&quot;, false, 1, 18000),
	(6, &quot;zl1&quot;, true, 1, 18000),
	(5, &quot;win7&quot;, false, 1, 16000)
).toDF(&quot;id&quot;, &quot;name&quot;, &quot;sex&quot;, &quot;dept&quot;, &quot;salary&quot;)
df.createTempView(&quot;t_employee&quot;)

spark.udf
	.register(&quot;customSum&quot;, new CustomSum)
val sql =
	&quot;&quot;&quot;
	  |select dept,customSum(salary)
	  |from t_employee
	  |group by dept
	&quot;&quot;&quot;.stripMargin
spark.sql(sql)
	.show()
</code></pre>
<pre><code>+----+---------------------------------+
|dept|customsum(CAST(salary AS DOUBLE))|
+----+---------------------------------+
|   1|                          67000.0|
|   2|                          32000.0|
+----+---------------------------------+
</code></pre>
<h2 id="loadsave">Load/Save</h2>
<h3 id="paquet">Paquet</h3>
<ul>
<li>Parquet简介</li>
</ul>
<blockquote>
<p>Parquet是面向分析型业务的列式存储格式，由Twitter和Cloudera合作开发，2015年5月从Apache的孵化器里毕业成为Apache顶级项目<br>
<a href="http://parquet.apache.org/">http://parquet.apache.org/ </a></p>
</blockquote>
<pre><code class="language-java">// paquet
var df=List(
	(1,&quot;zs&quot;,true,1,15000),
	(2,&quot;ls&quot;,false,2,18000),
	(3,&quot;ww&quot;,false,2,14000),
	(4,&quot;zl&quot;,false,1,18000),
	(6,&quot;zl1&quot;,true,1,18000),
	(5,&quot;win7&quot;,false,1,16000)
).toDF(&quot;id&quot;,&quot;name&quot;,&quot;sex&quot;,&quot;dept&quot;,&quot;salary&quot;)

df.write
	.format(&quot;parquet&quot;)
	.save(&quot;file:////Users/mashikang/IdeaProjects/spark_sql/src/main/resources/parquet&quot;)
	
spark.read
	.parquet(&quot;file:////Users/mashikang/IdeaProjects/spark_sql/src/main/resources/parquet&quot;)
	.show()
</code></pre>
<pre><code>+---+----+-----+----+------+
| id|name|  sex|dept|salary|
+---+----+-----+----+------+
|  5|win7|false|   1| 16000|
|  6| zl1| true|   1| 18000|
|  4|  zl|false|   1| 18000|
|  3|  ww|false|   2| 14000|
|  1|  zs| true|   1| 15000|
|  2|  ls|false|   2| 18000|
+---+----+-----+----+------+
</code></pre>
<ul>
<li>存储文件样式<br>
<img src="https://img-blog.csdnimg.cn/20200612151145861.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L00yODM1OTIzMzg=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述" loading="lazy"></li>
</ul>
<h3 id="json">JSON</h3>
<pre><code class="language-java">// json
var df=List(
	(1,&quot;zs&quot;,true,1,15000),
	(2,&quot;ls&quot;,false,2,18000),
	(3,&quot;ww&quot;,false,2,14000),
	(4,&quot;zl&quot;,false,1,18000),
	(6,&quot;zl1&quot;,true,1,18000),
	(5,&quot;win7&quot;,false,1,16000)
).toDF(&quot;id&quot;,&quot;name&quot;,&quot;sex&quot;,&quot;dept&quot;,&quot;salary&quot;)

df.write
	.format(&quot;json&quot;)
	.save(&quot;file:///Users/mashikang/IdeaProjects/spark_sql/src/main/resources/json&quot;)
spark.read
	.json(&quot;file:///Users/mashikang/IdeaProjects/spark_sql/src/main/resources/json&quot;)
	.show()
</code></pre>
<pre><code>+----+---+----+------+-----+
|dept| id|name|salary|  sex|
+----+---+----+------+-----+
|   1|  5|win7| 16000|false|
|   2|  3|  ww| 14000|false|
|   1|  4|  zl| 18000|false|
|   2|  2|  ls| 18000|false|
|   1|  6| zl1| 18000| true|
|   1|  1|  zs| 15000| true|
+----+---+----+------+-----+
</code></pre>
<h3 id="orc存储压缩格式比较节省空间">ORC(存储压缩格式，比较节省空间)</h3>
<pre><code>// ORC
var df=List(
	(1,&quot;zs&quot;,true,1,15000),
	(2,&quot;ls&quot;,false,2,18000),
	(3,&quot;ww&quot;,false,2,14000),
	(4,&quot;zl&quot;,false,1,18000),
	(6,&quot;zl1&quot;,true,1,18000),
	(5,&quot;win7&quot;,false,1,16000)
).toDF(&quot;id&quot;,&quot;name&quot;,&quot;sex&quot;,&quot;dept&quot;,&quot;salary&quot;)

df.write
	.format(&quot;orc&quot;)
	.save(&quot;file:///Users/mashikang/IdeaProjects/spark_sql/src/main/resources/orc&quot;)
	
spark.read
	.orc(&quot;file:///Users/mashikang/IdeaProjects/spark_sql/src/main/resources/orc&quot;)
	.show()
</code></pre>
<pre><code>+---+----+-----+----+------+
| id|name|  sex|dept|salary|
+---+----+-----+----+------+
|  5|win7|false|   1| 16000|
|  4|  zl|false|   1| 18000|
|  3|  ww|false|   2| 14000|
|  6| zl1| true|   1| 18000|
|  1|  zs| true|   1| 15000|
|  2|  ls|false|   2| 18000|
+---+----+-----+----+------+
</code></pre>
<h3 id="csv">CSV</h3>
<pre><code>// CSV
var df=List(
	(1,&quot;zs&quot;,true,1,15000),
	(2,&quot;ls&quot;,false,2,18000),
	(3,&quot;ww&quot;,false,2,14000),
	(4,&quot;zl&quot;,false,1,18000),
	(6,&quot;zl1&quot;,true,1,18000),
	(5,&quot;win7&quot;,false,1,16000)
).toDF(&quot;id&quot;,&quot;name&quot;,&quot;sex&quot;,&quot;dept&quot;,&quot;salary&quot;)

df.write
	.format(&quot;csv&quot;) 
	.option(&quot;sep&quot;, &quot;,&quot;) 
	.option(&quot;inferSchema&quot;, &quot;true&quot;)
	.option(&quot;header&quot;, &quot;true&quot;) 
	.save(&quot;file:///Users/mashikang/IdeaProjects/spark_sql/src/main/resources/csv&quot;) 
spark.read 
	.option(&quot;sep&quot;, &quot;,&quot;) 
	.option(&quot;inferSchema&quot;, &quot;true&quot;) 
	.option(&quot;header&quot;, &quot;true&quot;) 
	.csv(&quot;file:///Users/mashikang/IdeaProjects/spark_sql/src/main/resources/csv&quot;) 
	.show()
</code></pre>
<pre><code>+---+----+-----+----+------+
| id|name|  sex|dept|salary|
+---+----+-----+----+------+
|  5|win7|false|   1| 16000|
|  4|  zl|false|   1| 18000|
|  3|  ww|false|   2| 14000|
|  2|  ls|false|   2| 18000|
|  6| zl1| true|   1| 18000|
|  1|  zs| true|   1| 15000|
+---+----+-----+----+------+
</code></pre>
<h3 id="jdbc">JDBC</h3>
<pre><code class="language-java">// JDBC
val usersDF = List(
	(1, &quot;张晓三&quot;, 1, 15000),
	(2, &quot;李晓四&quot;, 1, 18000),
	(3, &quot;王晓五&quot;, 1, 10000)
).toDF(&quot;id&quot;, &quot;name&quot;, &quot;dept&quot;, &quot;salary&quot;)

usersDF.write
	.format(&quot;jdbc&quot;)
	.mode(SaveMode.Overwrite)
	.option(&quot;user&quot;, &quot;root&quot;)
	.option(&quot;password&quot;, &quot;root&quot;)
	.option(&quot;url&quot;, &quot;jdbc:mysql://localhost:3306/test&quot;)
	.option(&quot;dbtable&quot;, &quot;t_user&quot;)
	.save()
	
val props = new Properties()
props.put(&quot;user&quot;, &quot;root&quot;)
props.put(&quot;password&quot;, &quot;root&quot;)

spark.read
	.jdbc(&quot;jdbc:mysql://localhost:3306/test&quot;, &quot;t_user&quot;, props)
	.show()
</code></pre>
<p>或者</p>
<pre><code class="language-java">val usersDF = List(
	(1, &quot;张晓三&quot;, 1, 15000),
	(2, &quot;李晓四&quot;, 1, 18000),
	(3, &quot;王晓五&quot;, 1, 10000)
).toDF(&quot;id&quot;, &quot;name&quot;, &quot;dept&quot;, &quot;salary&quot;)
usersDF.write
	.format(&quot;jdbc&quot;)
	.mode(SaveMode.Overwrite)
	.option(&quot;user&quot;, &quot;root&quot;)
	.option(&quot;password&quot;, &quot;root&quot;)
	.option(&quot;url&quot;, &quot;jdbc:mysql://localhost:3306/test&quot;)
	.option(&quot;dbtable&quot;, &quot;t_user&quot;)
	.save()

spark.read.format(&quot;jdbc&quot;)
			.option(&quot;user&quot;, &quot;root&quot;)
			.option(&quot;password&quot;, &quot;root&quot;)
			.option(&quot;url&quot;, &quot;jdbc:mysql://localhost:3306/test&quot;)
			.option(&quot;dbtable&quot;, &quot;t_user&quot;)
			.load()
			.show()
</code></pre>
<h3 id="dataframe转为rdd">DataFrame转为RDD</h3>
<pre><code class="language-java">val usersDF = List(
	(1, &quot;张晓三&quot;, 1, 15000.0),
	(2, &quot;李晓四&quot;, 1, 18000.0),
	(3, &quot;王晓五&quot;, 1, 10000.0)
).toDF(&quot;id&quot;, &quot;name&quot;, &quot;dept&quot;, &quot;salary&quot;)
usersDF.rdd.foreachPartition(its =&gt; {
	its.foreach(row =&gt; {
		val id = row.getAs[Int](&quot;id&quot;)
		val name = row.getAs[String](&quot;name&quot;)
		val salary = row.getAs[Double](&quot;salary&quot;)
		println(s&quot;$id,$name,$salary&quot;)
	})
})
</code></pre>
<pre><code>2,李晓四,18000.0
3,王晓五,10000.0
1,张晓三,15000.0
</code></pre>
]]></content>
    </entry>
</feed>