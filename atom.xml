<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://mask0407.github.io</id>
    <title>个人博客</title>
    <updated>2020-06-29T04:58:59.964Z</updated>
    <generator>https://github.com/jpmonette/feed</generator>
    <link rel="alternate" href="https://mask0407.github.io"/>
    <link rel="self" href="https://mask0407.github.io/atom.xml"/>
    <subtitle>温故而知新</subtitle>
    <logo>https://mask0407.github.io/images/avatar.png</logo>
    <icon>https://mask0407.github.io/favicon.ico</icon>
    <rights>All rights reserved 2020, 个人博客</rights>
    <entry>
        <title type="html"><![CDATA[GeoHash算法原理及实现]]></title>
        <id>https://mask0407.github.io/suanfa/</id>
        <link href="https://mask0407.github.io/suanfa/">
        </link>
        <updated>2020-06-29T03:19:42.000Z</updated>
        <content type="html"><![CDATA[<p><ul class="markdownIt-TOC">
<li>
<ul>
<li><a href="#geohash%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86">GeoHash算法原理</a>
<ul>
<li><a href="#%E6%A0%B7%E4%BE%8B%E6%95%B0%E6%8D%AE%E5%9F%BA%E4%BA%8E15%E6%AC%A1%E5%8C%BA%E5%9F%9F%E5%88%86%E5%89%B2">样例数据（基于15次区域分割）</a></li>
<li><a href="#geohash%E7%AE%97%E6%B3%95%E6%80%9D%E6%83%B3">GeoHash算法思想</a></li>
<li><a href="#geohash%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86-2">GeoHash算法原理</a></li>
<li><a href="#%E5%90%8E%E7%BB%AD%E9%97%AE%E9%A2%98">后续问题</a></li>
</ul>
</li>
<li><a href="#geohash%E7%AE%97%E6%B3%95%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0">GeoHash算法代码实现</a></li>
<li><a href="#%E5%86%99%E5%9C%A8%E6%9C%80%E5%90%8E">写在最后</a></li>
</ul>
</li>
</ul>
(GeoHash原理与Java实现)</p>
<h2 id="geohash算法原理">GeoHash算法原理</h2>
<blockquote>
<p>GeoHash是目前比较主流实现位置服务的技术，Geohash算法将经纬度二维数据编码为一个字符串，本质是一个降维的过程</p>
</blockquote>
<h3 id="样例数据基于15次区域分割">样例数据（基于15次区域分割）</h3>
<table>
<thead>
<tr>
<th>位置</th>
<th>经纬度</th>
<th>Geohash</th>
</tr>
</thead>
<tbody>
<tr>
<td>北京站</td>
<td>116.433589,39.910508</td>
<td>wx4g19</td>
</tr>
<tr>
<td>天安门</td>
<td>116.403874,39.913884</td>
<td>wx4g0f</td>
</tr>
<tr>
<td>首都机场</td>
<td>116.606819,40.086109</td>
<td>wx4uj3</td>
</tr>
</tbody>
</table>
<h3 id="geohash算法思想">GeoHash算法思想</h3>
<p>我们知道，经度范围是东经180到西经180，纬度范围是南纬90到北纬90，我们设定西经为负，南纬为负，所以地球上的经度范围就是[-180， 180]，纬度范围就是[-90，90]。如果以本初子午线、赤道为界，地球可以分成4个部分。</p>
<p>GeoHash的思想就是将地球划分的四部分映射到二维坐标上。</p>
<blockquote>
<p>[-90˚,0˚)代表0，(0˚,90˚]代表1，[-180˚,0)代表0，(0˚,180˚]代表1<br>
映射到二维空间划分为四部分则如下图</p>
</blockquote>
<p><img src="https://img-blog.csdnimg.cn/20200519092805300.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L00yODM1OTIzMzg=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述" loading="lazy"><br>
但是这么粗略的划分没有什么意义，想要更精确的使用GeoHash就需要再进一步<code>二分切割</code><br>
<img src="https://img-blog.csdnimg.cn/20200519095313362.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L00yODM1OTIzMzg=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述" loading="lazy"><br>
通过上图可看出，进一步<code>二分切割</code>将原本大略的划分变为细致的区域划分，这样就会更加精确。GeoHash算法就是基于这种思想，递归划分的次数越多，所计算出的数据越精确。</p>
<h3 id="geohash算法原理-2">GeoHash算法原理</h3>
<blockquote>
<p>GeoHash算法大体上分为三步：1. 计算经纬度的二进制、2. 合并经纬度的二进制、3. 通过Base32对合并后的二进制进行编码。</p>
</blockquote>
<ol>
<li>计算经纬度的二进制</li>
</ol>
<pre><code class="language-java">	//根据经纬度和范围，获取对应的二进制
	private BitSet getBits(double l, double floor, double ceiling) {
		BitSet buffer = new BitSet(numbits);
		for (int i = 0; i &lt; numbits; i++) {
			double mid = (floor + ceiling) / 2;
			if (l &gt;= mid) {
				buffer.set(i);
				floor = mid;
			} else {
				ceiling = mid;
			}
		}
		return buffer;
	}
</code></pre>
<p>上述代码numbits为：<code>private static int numbits = 3 * 5; //经纬度单独编码长度</code>也就是说将地球进行15次<code>二分切割</code></p>
<blockquote>
<p>注： 这里需要对BitSet类进行一下剖析，没了解过该类的话指定懵。</p>
</blockquote>
<p>了解BitSet只需了去了解它的set()、get()方法就足够了</p>
<ul>
<li>BitSet的set方法</li>
</ul>
<pre><code class="language-java"> /**
     * Sets the bit at the specified index to {@code true}.
     *
     * @param  bitIndex a bit index
     * @throws IndexOutOfBoundsException if the specified index is negative
     * @since  JDK1.0
     */
    public void set(int bitIndex) {
        if (bitIndex &lt; 0)
            throw new IndexOutOfBoundsException(&quot;bitIndex &lt; 0: &quot; + bitIndex);

        int wordIndex = wordIndex(bitIndex);
        expandTo(wordIndex);

        words[wordIndex] |= (1L &lt;&lt; bitIndex); // Restores invariants

        checkInvariants();
    }
</code></pre>
<p>set方法内<code>wordIndex(bitIndex)</code>底层将bitIndex右移6位然后返回，<code>ADDRESS_BITS_PER_WORD</code>为常量6</p>
<pre><code class="language-java">/**
    * Given a bit index, return word index containing it.
    */
   private static int wordIndex(int bitIndex) {
       return bitIndex &gt;&gt; ADDRESS_BITS_PER_WORD;
   }
</code></pre>
<p>set方法内的<code>expandTo(wordIndex)</code>只是一个判断数组是否需要扩容的方法</p>
<pre><code class="language-java">/**
     * Ensures that the BitSet can accommodate a given wordIndex,
     * temporarily violating the invariants.  The caller must
     * restore the invariants before returning to the user,
     * possibly using recalculateWordsInUse().
     * @param wordIndex the index to be accommodated.
     */
    private void expandTo(int wordIndex) {
        int wordsRequired = wordIndex+1;
        if (wordsInUse &lt; wordsRequired) {
            ensureCapacity(wordsRequired);
            wordsInUse = wordsRequired;
        }
    }
</code></pre>
<p>set内重要的一行代码<code>words[wordIndex] |= (1L &lt;&lt; bitIndex)</code>,这里只解释一下<code>|=</code></p>
<blockquote>
<p><code>a|=b</code>就是a=a|b,就是说将a、b转为二进制按位与，同0为0，否则为1</p>
</blockquote>
<ul>
<li>BitSet的get方法</li>
</ul>
<pre><code class="language-java"> /**
    * Returns the value of the bit with the specified index. The value
    * is {@code true} if the bit with the index {@code bitIndex}
    * is currently set in this {@code BitSet}; otherwise, the result
    * is {@code false}.
    *
    * @param  bitIndex   the bit index
    * @return the value of the bit with the specified index
    * @throws IndexOutOfBoundsException if the specified index is negative
    */
   public boolean get(int bitIndex) {
       if (bitIndex &lt; 0)
           throw new IndexOutOfBoundsException(&quot;bitIndex &lt; 0: &quot; + bitIndex);

       checkInvariants();

       int wordIndex = wordIndex(bitIndex);
       return (wordIndex &lt; wordsInUse)
           &amp;&amp; ((words[wordIndex] &amp; (1L &lt;&lt; bitIndex)) != 0);
   }
</code></pre>
<blockquote>
<p>get方法用一句话概括就是：如果传入的下标有值，返回true；反之为false</p>
</blockquote>
<p>以天安门坐标为例：<code>39.913884, 116.403874</code></p>
<pre><code class="language-java">	BitSet latbits = getBits(lat, -90, 90);
   	BitSet lonbits = getBits(lon, -180, 180);
   	// 纬度
   	for (int i = 0; i &lt; numbits; i++) {
   		System.out.print(latbits.get(i) + &quot; &quot;);
   	}
   	// 经度
    for (int i = 0; i &lt; numbits; i++) {
   		System.out.print(lonbits.get(i) + &quot; &quot;);
   	}
</code></pre>
<p>纬度经过转换为：</p>
<pre><code class="language-java">true false true true true false false false true true false false false true false 
</code></pre>
<p>转为二进制：</p>
<pre><code class="language-java">1 0 1 1 1 0 0 0 1 1 0 0 0 1 0 
</code></pre>
<p>经度经过转换为：</p>
<pre><code class="language-java">true true false true false false true false true true false false false true true 
</code></pre>
<p>转为二进制：</p>
<pre><code class="language-java">1 1 0 1 0 0 1 0 1 1 0 0 0 1 1 
</code></pre>
<ol start="2">
<li>合并经纬度二进制<br>
合并原则：经度占偶数位，纬度占奇数位。也就是说经纬度交替合并，首位0位置为经度的0位置</li>
</ol>
<p>合并后二进制编码为：</p>
<pre><code class="language-shell">11100 11101 00100 01111 00000 01110
</code></pre>
<ol start="3">
<li>使用Base32对合并后的经纬度二进制进行编码</li>
</ol>
<ul>
<li>代码实现</li>
</ul>
<pre><code class="language-java">// Base32进行编码
	public String encode(double lat, double lon) {
		BitSet latbits = getBits(lat, -90, 90);
		BitSet lonbits = getBits(lon, -180, 180);
		StringBuilder buffer = new StringBuilder();
		for (int i = 0; i &lt; numbits; i++) {
			buffer.append((lonbits.get(i)) ? '1' : '0');
			buffer.append((latbits.get(i)) ? '1' : '0');
		}
		String code = base32(Long.parseLong(buffer.toString(), 2));
		return code;
	}
</code></pre>
<p>本文案例经纬度编码后</p>
<pre><code class="language-java">wx4g0f
</code></pre>
<h3 id="后续问题">后续问题</h3>
<p>如果要使用此功能实现附近的人。假如红点为使用者，经过Geohash算法分割后只会推荐同区域<code>0011</code>中的绿点，但是如下图所示，蓝色点相对于绿色点更接近用户，所以区域划分的弊端就展现在这里。<br>
<img src="https://img-blog.csdnimg.cn/20200519160012889.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L00yODM1OTIzMzg=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述" loading="lazy"><br>
针对上述问题，我们可以人为获取红色用户所在的<code>0011</code>区域周边八个区域中的用户，即获取<code>0011</code>的同时还要获取<code>0100</code>,<code>0110</code>,<code>1100</code>,<code>0001</code>,<code>1001</code>,<code>0000</code>,<code>0010</code>,<code>1000</code></p>
<ul>
<li>代码实现</li>
</ul>
<pre><code class="language-java">	public ArrayList&lt;String&gt; getArroundGeoHash(double lat, double lon) {
		ArrayList&lt;String&gt; list = new ArrayList&lt;&gt;();
		double uplat = lat + minLat;
		double downLat = lat - minLat;

		double leftlng = lon - minLng;
		double rightLng = lon + minLng;

		String leftUp = encode(uplat, leftlng);
		list.add(leftUp);

		String leftMid = encode(lat, leftlng);
		list.add(leftMid);

		String leftDown = encode(downLat, leftlng);
		list.add(leftDown);

		String midUp = encode(uplat, lon);
		list.add(midUp);

		String midMid = encode(lat, lon);
		list.add(midMid);

		String midDown = encode(downLat, lon);
		list.add(midDown);

		String rightUp = encode(uplat, rightLng);
		list.add(rightUp);

		String rightMid = encode(lat, rightLng);
		list.add(rightMid);

		String rightDown = encode(downLat, rightLng);
		list.add(rightDown);

		return list;
	}
</code></pre>
<p>然后根据球体两点间的距离计算红色用户与周边区域用户距离，从而进行附近的人功能实现</p>
<ul>
<li>通过两经纬度计算距离java代码实现</li>
</ul>
<pre><code class="language-java">	static double getDistance(double lat1, double lon1, double lat2, double lon2) {
		// 经纬度（角度）转弧度。弧度用作参数，以调用Math.cos和Math.sin
		double radiansAX = Math.toRadians(lon1); // A经弧度
		double radiansAY = Math.toRadians(lat1); // A纬弧度
		double radiansBX = Math.toRadians(lon2); // B经弧度
		double radiansBY = Math.toRadians(lat2); // B纬弧度

		// 公式中“cosβ1cosβ2cos（α1-α2）+sinβ1sinβ2”的部分，得到∠AOB的cos值
		double cos = Math.cos(radiansAY) * Math.cos(radiansBY) * Math.cos(radiansAX - radiansBX)
				+ Math.sin(radiansAY) * Math.sin(radiansBY);
		double acos = Math.acos(cos); // 反余弦值
		return EARTH_RADIUS * acos; // 最终结果
	}
</code></pre>
<h2 id="geohash算法代码实现">GeoHash算法代码实现</h2>
<pre><code class="language-java">public class GeoHash {
	public static final double MINLAT = -90;
	public static final double MAXLAT = 90;
	public static final double MINLNG = -180;
	public static final double MAXLNG = 180;

	private static int numbits = 3 * 5; //经纬度单独编码长度

	private static double minLat;
	private static double minLng;

	private final static char[] digits = {'0', '1', '2', '3', '4', '5', '6', '7', '8',
			'9', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'j', 'k', 'm', 'n', 'p',
			'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z'};

	//定义编码映射关系
	final static HashMap&lt;Character, Integer&gt; lookup = new HashMap&lt;Character, Integer&gt;();

	//初始化编码映射内容
	static {
		int i = 0;
		for (char c : digits)
			lookup.put(c, i++);
	}

	public GeoHash() {
		setMinLatLng();
	}

	// Base32进行编码
	public String encode(double lat, double lon) {
		BitSet latbits = getBits(lat, -90, 90);
		BitSet lonbits = getBits(lon, -180, 180);
		StringBuilder buffer = new StringBuilder();
		for (int i = 0; i &lt; numbits; i++) {
			buffer.append((lonbits.get(i)) ? '1' : '0');
			buffer.append((latbits.get(i)) ? '1' : '0');
		}
		String code = base32(Long.parseLong(buffer.toString(), 2));
		return code;
	}

	public ArrayList&lt;String&gt; getArroundGeoHash(double lat, double lon) {
		ArrayList&lt;String&gt; list = new ArrayList&lt;&gt;();
		double uplat = lat + minLat;
		double downLat = lat - minLat;

		double leftlng = lon - minLng;
		double rightLng = lon + minLng;

		String leftUp = encode(uplat, leftlng);
		list.add(leftUp);

		String leftMid = encode(lat, leftlng);
		list.add(leftMid);

		String leftDown = encode(downLat, leftlng);
		list.add(leftDown);

		String midUp = encode(uplat, lon);
		list.add(midUp);

		String midMid = encode(lat, lon);
		list.add(midMid);

		String midDown = encode(downLat, lon);
		list.add(midDown);

		String rightUp = encode(uplat, rightLng);
		list.add(rightUp);

		String rightMid = encode(lat, rightLng);
		list.add(rightMid);

		String rightDown = encode(downLat, rightLng);
		list.add(rightDown);

		return list;
	}

	//根据经纬度和范围，获取对应的二进制
	private BitSet getBits(double l, double floor, double ceiling) {
		BitSet buffer = new BitSet(numbits);
		for (int i = 0; i &lt; numbits; i++) {
			double mid = (floor + ceiling) / 2;
			if (l &gt;= mid) {
				buffer.set(i);
				floor = mid;
			} else {
				ceiling = mid;
			}
		}
		return buffer;
	}

	//将经纬度合并后的二进制进行指定的32位编码
	private String base32(long i) {
		char[] buf = new char[65];
		int charPos = 64;
		boolean negative = (i &lt; 0);
		if (!negative) {
			i = -i;
		}
		while (i &lt;= -32) {
			buf[charPos--] = digits[(int) (-(i % 32))];
			i /= 32;
		}
		buf[charPos] = digits[(int) (-i)];
		if (negative) {
			buf[--charPos] = '-';
		}
		return new String(buf, charPos, (65 - charPos));
	}

	private void setMinLatLng() {
		minLat = MAXLAT - MINLAT;
		for (int i = 0; i &lt; numbits; i++) {
			minLat /= 2.0;
		}
		minLng = MAXLNG - MINLNG;
		for (int i = 0; i &lt; numbits; i++) {
			minLng /= 2.0;
		}
	}

	//根据二进制和范围解码
	private double decode(BitSet bs, double floor, double ceiling) {
		double mid = 0;
		for (int i = 0; i &lt; bs.length(); i++) {
			mid = (floor + ceiling) / 2;
			if (bs.get(i))
				floor = mid;
			else
				ceiling = mid;
		}
		return mid;
	}

	//对编码后的字符串解码
	public double[] decode(String geohash) {
		StringBuilder buffer = new StringBuilder();
		for (char c : geohash.toCharArray()) {
			int i = lookup.get(c) + 32;
			buffer.append(Integer.toString(i, 2).substring(1));
		}

		BitSet lonset = new BitSet();
		BitSet latset = new BitSet();

		//偶数位，经度
		int j = 0;
		for (int i = 0; i &lt; numbits * 2; i += 2) {
			boolean isSet = false;
			if (i &lt; buffer.length())
				isSet = buffer.charAt(i) == '1';
			lonset.set(j++, isSet);
		}

		//奇数位，纬度
		j = 0;
		for (int i = 1; i &lt; numbits * 2; i += 2) {
			boolean isSet = false;
			if (i &lt; buffer.length())
				isSet = buffer.charAt(i) == '1';
			latset.set(j++, isSet);
		}

		double lon = decode(lonset, -180, 180);
		double lat = decode(latset, -90, 90);

		return new double[]{lat, lon};
	}

	public static void main(String[] args) {
		GeoHash geoHash = new GeoHash();
		// 北京站
		String encode = geoHash.encode(39.910508, 116.433589);
		System.out.println(encode);

		// 天安门
		System.out.println(geoHash.encode(39.913884, 116.403874));

		// 首都机场
		System.out.println(geoHash.encode(40.086109, 116.606819));

		BitSet latbits = geoHash.getBits(39.913884, -90, 90);
		BitSet lonbits = geoHash.getBits(116.403874, -180, 180);

//		for (int i=0; i&lt; latbits.length(); i++) {
//			System.out.println(latbits.get(i));
//		}

		for (int i = 0; i &lt; numbits; i++) {
//			System.out.print(latbits.get(i));
			System.out.print(latbits.get(i) ? '1' : '0');
			System.out.print(&quot; &quot;);
		}

		System.out.println();
		StringBuilder buffer = new StringBuilder();
		for (int i = 0; i &lt; numbits; i++) {
			buffer.append((lonbits.get(i)) ? '1' : '0');
			buffer.append((latbits.get(i)) ? '1' : '0');
		}

		System.out.println(buffer.toString());

		System.out.println(geoHash.encode(39.913884, 116.403874));
	}

}
</code></pre>
<h2 id="写在最后">写在最后</h2>
<p>如果嫌GeoHash算法麻烦，但是还想用它，没关系。<br>
Redis知道你懒<a href="https://redis.io/commands/GEOHASH">Redis官网GeoHash用法</a></p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Spark入门( 九)——机器学习 Spark MLlib]]></title>
        <id>https://mask0407.github.io/spark08/</id>
        <link href="https://mask0407.github.io/spark08/">
        </link>
        <updated>2020-06-29T03:18:58.000Z</updated>
        <content type="html"><![CDATA[<p><ul class="markdownIt-TOC">
<li><a href="#%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%98%AF%E4%BB%80%E4%B9%88">机器学习是什么？</a>
<ul>
<li><a href="#%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0">机器学习</a></li>
<li><a href="#spark-mllib">Spark MLlib</a></li>
</ul>
</li>
<li><a href="#spark-mllib%E6%A1%88%E4%BE%8B">Spark MLlib案例</a>
<ul>
<li><a href="#%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8">快速入门</a></li>
<li><a href="#%E5%9F%BA%E6%9C%AC%E7%BB%9F%E8%AE%A1">基本统计</a>
<ul>
<li><a href="#correlation%E7%9B%B8%E5%85%B3%E6%80%A7">Correlation(相关性)</a></li>
<li><a href="#hypothesis-testing%E5%81%87%E8%AE%BE%E6%A3%80%E9%AA%8C">Hypothesis testing(假设检验)</a></li>
<li><a href="#summarizer%E6%80%BB%E7%BB%93%E5%99%A8">Summarizer(总结器)</a></li>
</ul>
</li>
<li><a href="#%E6%9C%AA%E5%AE%8C%E5%BE%85%E7%BB%AD">未完待续。。。</a></li>
</ul>
</li>
</ul>
(Spark MLlib)</p>
<h1 id="机器学习是什么">机器学习是什么？</h1>
<h2 id="机器学习">机器学习</h2>
<p>数据挖掘有着50多年的发展历史。机器学习就是其子领域之一，特点是利用大型计算机集群来从海量数据中分析和提取知识</p>
<p>机器学习与计算统计学密切相关。它与数学优化紧密关联，为其提供方法、理论和应用领域。机器学习在各种传统设计和编程不能胜任的计算机任务中有广泛应用。典型的应用如<code>垃圾邮件过滤</code>、<code>光学字符识别(OCR)</code>、<code>搜索引擎和计算机视觉</code>。机器学习有时和数据挖掘联用，但更偏向<code>探索性数据分析</code>，亦称为<code>无监督学习</code>。</p>
<p>与学习系统的可用输入自然属性不同，机器学习系统可分为<code>3种</code>。学习算法发现输入数据的内在结构。它可以<code>有目标(隐含模式)</code>,也可以是发现特征的一种途径。</p>
<table>
<thead>
<tr>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>无监督学习</td>
<td>学习系统的输入数据中并不包含对应的<code>标签(或期望的输出)</code>，它需要自行从输入中找到输入数据的内在结构</td>
</tr>
<tr>
<td>监督学习</td>
<td>系统已知各输入对应的<code>期望输出</code>系统的目标是学习如何将输入映射到输出</td>
</tr>
<tr>
<td>强化学习</td>
<td>系统与环境进行交互，它有已定义的目标，但没有人类显式地告知其是否正在接近目标</td>
</tr>
</tbody>
</table>
<h2 id="spark-mllib">Spark MLlib</h2>
<p>MLlib是Spark的机器学习（ML）库。其目标是使实用的机器学习可扩展且容易。在较高级别，它提供了以下工具：</p>
<blockquote>
<ul>
<li>ML算法：常见的学习算法，例如分类，回归，聚类和协同过滤</li>
<li>特征化：特征提取，变换，降维和选择</li>
<li>管道：用于构建，评估和调整ML管道的工具</li>
<li>持久性：保存和加载算法，模型和管道</li>
<li>实用程序：线性代数，统计信息，数据处理等。</li>
</ul>
</blockquote>
<h1 id="spark-mllib案例">Spark MLlib案例</h1>
<blockquote>
<p>基于<code>DataFrame</code>的API是主要API</p>
</blockquote>
<h2 id="快速入门">快速入门</h2>
<ul>
<li>pom.xml</li>
</ul>
<pre><code class="language-xml">&lt;!-- https://mvnrepository.com/artifact/org.apache.spark/spark-mllib --&gt;
		&lt;dependency&gt;
			&lt;groupId&gt;org.apache.spark&lt;/groupId&gt;
			&lt;artifactId&gt;spark-mllib_2.11&lt;/artifactId&gt;
			&lt;version&gt;2.2.1&lt;/version&gt;
		&lt;/dependency&gt;
</code></pre>
<pre><code class="language-java">def main(args: Array[String]): Unit = {
		// 屏蔽日志
		Logger.getLogger(&quot;org.apache.spark&quot;).setLevel(Level.WARN)
		Logger.getLogger(&quot;org.apache.jetty.server&quot;).setLevel(Level.OFF)

		val spark = SparkSession
			.builder()
			.master(&quot;local[*]&quot;)
			.appName(Demo01.getClass.getName)
    		.getOrCreate()

		import spark.implicits._

		val data = Seq(
			/**
			 * 稀疏向量表示方式
			 * 4, Seq((0, 1.0), (3, -2.0))
			 * 表示向量长度为4 有两个非0位置：0和3位置，0和3的值分别为1.0、-2.0
			 * 该向量可表示为(1.0, 0, 0, -2.0)
 			 */
			Vectors.sparse(4, Seq((0, 1.0), (3, -2.0))),
			// 密集向量表示方式
			Vectors.dense(4.0, 5.0, 0.0, 3.0),
			Vectors.dense(6.0, 7.0, 0.0, 8.0),
			Vectors.sparse(4, Seq((0, 9.0), (3, 1.0)))
		)

		val df = data.map(Tuple1.apply).toDF(&quot;features&quot;)
		df.show()
		
		// 计算df features列的相关性
		val Row(coeff1: Matrix) = Correlation.corr(df, &quot;features&quot;).head
		println(s&quot;Pearson correlation matrix:\n $coeff1&quot;)

		val Row(coeff2: Matrix) = Correlation.corr(df, &quot;features&quot;, &quot;spearman&quot;).head
		println(s&quot;Spearman correlation matrix:\n $coeff2&quot;)
		
		spark.stop()
	}
</code></pre>
<h2 id="基本统计">基本统计</h2>
<h3 id="correlation相关性">Correlation(相关性)</h3>
<blockquote>
<p>Correlation 使用指定的方法为向量的输入数据集计算相关矩阵。输出将是一个DataFrame，其中包含向量列的相关矩阵。</p>
</blockquote>
<p>皮尔森系数公式：</p>
<blockquote>
<p>当两个变量的线性关系增强时，相关系数趋于1或-1。正相关时趋于1，负相关时趋于-1。当两个变量独立时相关系统为0，但反之不成立。当Y和X服从联合正态分布时，其相互独立和不相关是等价的<br>
<img src="https://img-blog.csdnimg.cn/20200624110228116.png" alt="在这里插入图片描述" loading="lazy"></p>
</blockquote>
<pre><code class="language-java">		val data = Seq(
			/**
			 * 稀疏向量表示方式
			 * 4, Seq((0, 1.0), (3, -2.0))
			 * 表示向量长度为4 有两个非0位置：0和3位置，0和3的值分别为1.0、-2.0
			 * 该向量可表示为(1.0, 0, 0, -2.0)
 			 */
			Vectors.sparse(4, Seq((0, 1.0), (3, -2.0))),
			// 密集向量表示方式
			Vectors.dense(4.0, 5.0, 0.0, 3.0),
			Vectors.dense(6.0, 7.0, 0.0, 8.0),
			Vectors.sparse(4, Seq((0, 9.0), (3, 1.0)))
		)

		val df = data.map(Tuple1.apply).toDF(&quot;features&quot;)
		df.show()
		
		// 计算features的相关性, method系数默认为Pearson
		val Row(coeff1: Matrix) = Correlation.corr(df, &quot;features&quot;).head
		println(s&quot;Pearson correlation matrix:\n $coeff1&quot;)

		// 计算features的相关性, method系数为Spearson
		val Row(coeff2: Matrix) = Correlation.corr(df, &quot;features&quot;, &quot;spearman&quot;).head
		println(s&quot;Spearman correlation matrix:\n $coeff2&quot;)
</code></pre>
<h3 id="hypothesis-testing假设检验">Hypothesis testing(假设检验)</h3>
<blockquote>
<p>假设检验是一种强大的统计工具，可用来确定结果是否具有统计学意义，以及该结果是否偶然发生。spark.ml当前支持Pearson的卡方（<code>数学处理错误</code>）测试独立性。<br>
ChiSquareTest针对标签上的每个功能进行Pearson的独立性测试。对于每个要素，（要素，标签）对将转换为列联矩阵，针对该列矩阵计算卡方统计量。所有标签和特征值必须是分类的。</p>
</blockquote>
<pre><code class="language-java">		import org.apache.spark.ml.linalg.{Vector, Vectors}
		import org.apache.spark.ml.stat.ChiSquareTest

		val data = Seq(
			(0.0, Vectors.dense(0.5, 10.0)),
			(0.0, Vectors.dense(1.5, 20.0)),
			(1.0, Vectors.dense(1.5, 30.0)),
			(0.0, Vectors.dense(3.5, 30.0)),
			(0.0, Vectors.dense(3.5, 40.0)),
			(1.0, Vectors.dense(3.5, 40.0))
		)

		val df = data.toDF(&quot;label&quot;, &quot;features&quot;)
		df.show()
		val chi = ChiSquareTest.test(df, &quot;features&quot;, &quot;label&quot;).head
		println(s&quot;pValues = ${chi.getAs[Vector](0)}&quot;)
		println(s&quot;degreesOfFreedom ${chi.getSeq[Int](1).mkString(&quot;[&quot;, &quot;,&quot;, &quot;]&quot;)}&quot;)
		println(s&quot;statistics ${chi.getAs[Vector](2)}&quot;)
</code></pre>
<h3 id="summarizer总结器">Summarizer(总结器)</h3>
<pre><code class="language-java">		import spark.implicits._
		import org.apache.spark.ml.stat.Summarizer._

		val data = Seq(
			(Vectors.dense(2.0, 3.0, 5.0), 1.0),
			(Vectors.dense(4.0, 6.0, 7.0), 2.0)
		)

		val df = data.toDF(&quot;features&quot;, &quot;weight&quot;)

		val (meanVal, varianceVal) = df.select(metrics(&quot;mean&quot;, &quot;variance&quot;)
			.summary($&quot;features&quot;, $&quot;weight&quot;).as(&quot;summary&quot;))
			.select(&quot;summary.mean&quot;, &quot;summary.variance&quot;)
			.as[(Vector, Vector)].first()

		println(s&quot;with weight: mean = ${meanVal}, variance = ${varianceVal}&quot;)

		val (meanVal2, varianceVal2) = df.select(mean($&quot;features&quot;), variance($&quot;features&quot;))
			.as[(Vector, Vector)].first()

		println(s&quot;without weight: mean = ${meanVal2}, sum = ${varianceVal2}&quot;)
</code></pre>
<h2 id="未完待续">未完待续。。。</h2>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Spark入门( 八)——Spark流计算新玩法-Structured Streaming]]></title>
        <id>https://mask0407.github.io/spark07/</id>
        <link href="https://mask0407.github.io/spark07/">
        </link>
        <updated>2020-06-29T03:18:21.000Z</updated>
        <content type="html"><![CDATA[<p><ul class="markdownIt-TOC">
<li><a href="#%E7%AE%80%E4%BB%8B">简介</a></li>
<li><a href="#%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8%E6%A1%88%E4%BE%8B">快速入门案例</a>
<ul>
<li><a href="#%E7%A8%8B%E5%BA%8F%E6%B5%81%E7%A8%8B%E7%BB%93%E6%9E%84">程序流程结构</a></li>
</ul>
</li>
<li><a href="#%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5">基本概念</a></li>
<li><a href="#%E6%95%85%E9%9A%9C%E5%AE%B9%E9%94%99">故障容错</a></li>
<li><a href="#structured-streaming-api">Structured Streaming API</a>
<ul>
<li><a href="#input-sources">Input Sources</a>
<ul>
<li><a href="#file-source">File Source</a>
<ul>
<li><a href="#socket-sourcedebug">Socket source(debug)</a></li>
</ul>
</li>
<li><a href="#kafka-source">Kafka source</a></li>
</ul>
</li>
<li><a href="#output-sink">Output Sink</a>
<ul>
<li><a href="#file-sinkappend-mode-only">File sink(Append Mode Only)</a></li>
<li><a href="#kafkasinkappendupdatecomplete">KafkaSink((Append|Update|Complete))</a></li>
<li><a href="#foreach-sinkappendupdatecomplate">Foreach sink(Append|Update|Complate)</a>
<ul>
<li><a href="#userrowwriter">UserRowWriter</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#window-on-event-time">Window on Event Time</a></li>
<li><a href="#%E5%A4%84%E7%90%86%E5%BB%B6%E8%BF%9F-data-%E5%92%8C-watermarking">处理延迟 Data 和 Watermarking</a>
<ul>
<li><a href="#watermarking%E4%BF%9D%E9%9A%9C%E6%9C%BA%E5%88%B6">Watermarking保障机制：</a></li>
<li><a href="#spark%E6%B8%85%E9%99%A4window%E8%81%9A%E5%90%88%E7%8A%B6%E6%80%81%E6%9D%A1%E4%BB%B6">Spark清除window聚合状态条件</a>
<ul>
<li><a href="#update-mode">Update Mode</a></li>
<li><a href="#append-mode">Append Mode</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#join-%E6%93%8D%E4%BD%9C">Join 操作</a>
<ul>
<li><a href="#stream-static-joins">Stream-static Joins</a></li>
<li><a href="#stream-stream-joins">Stream-stream Joins</a>
<ul>
<li><a href="#inner-join">inner join</a></li>
<li><a href="#outer-join">outer join</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
(Structured Streaming)</p>
<h1 id="简介">简介</h1>
<p><code>Structured Streaming</code> 构建在<code>SparkSQL</code>之上的流处理引擎。可以使用户继续使用<code>DataSet/dataFrame</code>操<br>
作流数据。并且提供了多种计算模型可供选择，默认情况下，使用的依然是Spark的marco batch这种计<br>
算模型能够到100ms左右的end-to-end的精准一次的容错计算。除此之外也提供了基于<code>EventTime</code>语义<br>
的窗口计算（DStream 基于Processor Time不同）。同时在spark-2.3版本又提出新的计算模型<br>
<code>Continuous Processing</code>可以达到1ms左右的精准一次的容错计算。</p>
<h1 id="快速入门案例">快速入门案例</h1>
<ul>
<li>pom</li>
</ul>
<pre><code class="language-xml">		&lt;dependency&gt;
			&lt;groupId&gt;org.apache.spark&lt;/groupId&gt;
			&lt;artifactId&gt;spark-sql_2.11&lt;/artifactId&gt;
			&lt;version&gt;2.4.3&lt;/version&gt;
		&lt;/dependency&gt;
		&lt;dependency&gt;
			&lt;groupId&gt;org.apache.spark&lt;/groupId&gt;
			&lt;artifactId&gt;spark-core_2.11&lt;/artifactId&gt;
			&lt;version&gt;2.4.3&lt;/version&gt;
		&lt;/dependency&gt;
</code></pre>
<ul>
<li>WordCount</li>
</ul>
<pre><code class="language-java">def main(args: Array[String]): Unit = {
		Logger.getLogger(&quot;org.apache.spark&quot;).setLevel(Level.WARN)
		Logger.getLogger(&quot;org.apache.jetty.server&quot;).setLevel(Level.OFF)

		//1.构建SparkSession
		val spark = SparkSession.builder()
			.appName(&quot;wordcount&quot;)
			.master(&quot;local[*]&quot;)
			.getOrCreate()
		import spark.implicits._

		//2.创建输入流-readStream
		var lines = spark.readStream
			.format(&quot;socket&quot;)
			.option(&quot;host&quot;, &quot;localhost&quot;)
			.option(&quot;port&quot;, 9999)
			.load()

		//3.对dataframe实现转换
		var wordCounts = lines.as[String]
    		.flatMap(_.split(&quot;\\s+&quot;))
    		.groupBy(&quot;value&quot;)
    		.count()


		//4.构建query 输出
		val query = wordCounts.writeStream
			.format(&quot;console&quot;)
			.outputMode(OutputMode.Update()) //有状态持续计算 Complete| Update| Append
			.start()

		//5.等待流结束
		query.awaitTermination()
	}
</code></pre>
<ul>
<li>有状态持续计算 Complete| Update| Append 之间的区别</li>
</ul>
<blockquote>
<ol>
<li>Complete: 每一个trigger到来时，就输出整个完整的dataframe</li>
<li>Update: 只输出那些被修改的Row。<br>
每一次window sliding，就去跟原来的结果比较，有变化就输出</li>
<li>Append: 只输出新添加的（原来没有的）Row（）（如果是groupby，要有watermark才可以）<br>
每当一个watermark时间结束了，这个临时的结果再回转换成正式的结果并导出。</li>
</ol>
</blockquote>
<ul>
<li>nc -l 999 输入</li>
</ul>
<pre><code>aa bb cc aa
cc aa aa aa
</code></pre>
<ul>
<li>输出结果(由于使用了Update 第二次输入没有<code>bb</code>，所有Batch: 2没有bb输出)</li>
</ul>
<pre><code>-------------------------------------------
Batch: 1
-------------------------------------------
+-----+-----+
|value|count|
+-----+-----+
|   cc|    1|
|   bb|    1|
|   aa|    2|
+-----+-----+

-------------------------------------------
Batch: 2
-------------------------------------------
+-----+-----+
|value|count|
+-----+-----+
|   cc|    2|
|   aa|    5|
+-----+-----+
</code></pre>
<h2 id="程序流程结构">程序流程结构</h2>
<blockquote>
<p>1.构建<code>SparkSession</code> 对象<br>
2.借助于<code>SparkSession#readStream</code>加载动态的<code>Dataframe</code><br>
3.使用<code>DataFrame API</code>或者是SQL语句 实现对动态数据计算<br>
4.通过<code>DataFrame#writeStream</code>方法构建<code>StreamQuery</code>对象<br>
5.调用<code>StreamQuery#awaitTermination</code>等待关闭指令</p>
</blockquote>
<h1 id="基本概念">基本概念</h1>
<p>Structure Stream的核心思想是通过将实时数据流看成是一个持续插入table.因此用户就可以使用SQL查 询DynamicTable|UnboundedTable。底层Spark通过StreamQuery实现对数据持续计算。<br>
<img src="https://img-blog.csdnimg.cn/20200612174115861.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L00yODM1OTIzMzg=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述" loading="lazy"><br>
当对Input执行转换的时候系统产生一张结果表 <code>ResultTable</code> ,当有新的数据产生的时候，系统会往<br>
<code>Input Table</code> 插入一行数据，这会最终导致系统更新 <code>ResultTable</code> ,每一次的更新系统将更新的数<br>
据写到外围系统-Sink.<br>
<img src="https://img-blog.csdnimg.cn/20200612174328732.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L00yODM1OTIzMzg=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述" loading="lazy"></p>
<ul>
<li><code>Output</code> 定义如何将Result写出到外围系统，目前Spark支持三种输出模式：(上面已经简单介绍过了)</li>
</ul>
<ol>
<li><code>Complete Mode</code> - 整个ResultTable的数据会被写到外围系统。</li>
<li><code>Update Mode</code> - 只会将ResultTable中被更新的行，写到外围系统（ spark-2.1.1 +支持）</li>
<li><code>Append Mode</code> - 只有新数据插入ResultTable的时候，才会将结果输出。注意：这种模式只适用<br>
于被插入结果表的数据都是只读的情况下，才可以将输出模式定义为Append（查询当中不应该出<br>
现聚合算子，当然也有特例，例如流中声明watermarker）</li>
</ol>
<p>由于Structure Streaming计算的特点，Spark会在内存当中存储程序计算中间状态用于生产结果表的数<br>
据，Spark并不会存储 Input Table 的数据，一旦处理完成之后，读取的数据会被丢弃。整个聚合的<br>
过程无需用户干预（对比Storm，Storm状态管理需要将数据写到外围系统）。</p>
<h1 id="故障容错">故障容错</h1>
<p>Structured Streaming通过<code>checkpoint</code>和<code>write ahead log</code>去记录每一次批处理的数据源的偏移量（区<br>
间），可以保证在失败的时候可以重复的读取数据源。其次Structure Streaming也提供了Sink的<code>幂等写</code><br>
的特性（在编程中一个幂等 操作的特点是其任意多次执行所产生的影响均与一次执行的影响相同），<br>
因此Structure Streaming实现<code>end-to-end</code> <code>exactly-once</code>语义的故障恢复。</p>
<h1 id="structured-streaming-api">Structured Streaming API</h1>
<p>自Spark-2.0版本以后<code>Dataframe/Dataset</code>才可以处理有界数据和无界数据。<code>Structured Streaming</code>也是用<br>
<code>SparkSession</code>方式去创建<code>Dataset/DataFrame</code> ,同时所有<code>Dataset/DataFrame</code> 的操作保持和<code>Spark SQL</code><br>
中<code>Dataset/DataFrame</code> 一致。</p>
<h2 id="input-sources">Input Sources</h2>
<h3 id="file-source">File Source</h3>
<p>目前支持支持<code>text</code>, <code>csv</code>, <code>json</code>, <code>orc</code>, <code>parquet</code>等格式的文件，当这些数据被放入到采样目录，系统会以流的<br>
形式读取采样目录下的文件.</p>
<pre><code class="language-java">		//1.创建SparkSession
		val spark = SparkSession
			.builder()
			.master(&quot;local[*]&quot;)
			.appName(&quot;printline&quot;)
			.getOrCreate()

		import spark.implicits._
		var df = spark.readStream
			.format(&quot;text&quot;)
			//json/csv/parquet/orc 等
			.load(&quot;file:///Users/mashikang/IdeaProjects/structured_stream/src/main/resources&quot;)

		var userDF = df.as[String]
			.map(line =&gt; line.split(&quot;\\s+&quot;))
			.map(tokens =&gt; (tokens(0).toInt, tokens(1), tokens(2).toBoolean, tokens(3).toInt))
			.toDF(&quot;id&quot;, &quot;name&quot;, &quot;sex&quot;, &quot;age&quot;)

		val query = userDF.writeStream.format(&quot;console&quot;)
			.outputMode(OutputMode.Append())
			.start()


		query.awaitTermination()
</code></pre>
<ul>
<li>文件</li>
</ul>
<pre><code class="language-text">1 zhangsan true 20
2 lisi true 28
3 wangwu false 24
4 zhaoliu true 28
</code></pre>
<h4 id="socket-sourcedebug">Socket source(debug)</h4>
<pre><code class="language-java">		//1.构建SparkSession
		val spark = SparkSession.builder()
			.appName(&quot;wordcount&quot;)
			.master(&quot;local[*]&quot;)
			.getOrCreate()
		import spark.implicits._

		//2.创建输入流-readStream
		var lines = spark.readStream
			.format(&quot;socket&quot;)
			.option(&quot;host&quot;, &quot;localhost&quot;)
			.option(&quot;port&quot;, 9999)
			.load()

		//3.对dataframe实现转换
		var wordCounts = lines.as[String]
    		.flatMap(_.split(&quot;\\s+&quot;))
    		.groupBy(&quot;value&quot;)
    		.count()


		//4.构建query 输出
		val query = wordCounts.writeStream
			.format(&quot;console&quot;)
			.outputMode(OutputMode.Update()) //有状态持续计算 Complete| Update
			.start()

		//5.等待流结束
		query.awaitTermination()
</code></pre>
<h3 id="kafka-source">Kafka source</h3>
<ul>
<li>pom.xml</li>
</ul>
<pre><code class="language-xml">&lt;dependency&gt;
 	&lt;groupId&gt;org.apache.spark&lt;/groupId&gt;
	&lt;artifactId&gt;spark-sql-kafka-0-10_2.11&lt;/artifactId&gt; 
	&lt;version&gt;2.4.3&lt;/version&gt; 
&lt;/dependency&gt;
</code></pre>
<pre><code class="language-java">		//1.创建SparkSession
		val spark=SparkSession .builder()
			.master(&quot;local[*]&quot;)
			.appName(&quot;printline&quot;)
			.getOrCreate()

		import spark.implicits._
		var df=spark.readStream
			.format(&quot;kafka&quot;)
			.option(&quot;kafka.bootstrap.servers&quot;, &quot;localhost:9092&quot;)
			.option(&quot;subscribe&quot;, &quot;topic01&quot;)
			.load()
			.selectExpr(&quot;CAST(key AS STRING)&quot;,&quot;CAST(value AS STRING)&quot;)

		val wordCounts=df.select(&quot;value&quot;).as[String]
			.flatMap(_.split(&quot;\\s+&quot;))
			.coalesce(1)
			.map((_,1))
			.toDF(&quot;word&quot;,&quot;count&quot;)
			.groupBy(&quot;word&quot;)
			.sum(&quot;count&quot;)

		val query = wordCounts.writeStream.
			format(&quot;console&quot;)
			.outputMode(OutputMode.Update())
			.start()

		query.awaitTermination()
</code></pre>
<h2 id="output-sink">Output Sink</h2>
<h3 id="file-sinkappend-mode-only">File sink(Append Mode Only)</h3>
<pre><code class="language-java">	//1.创建SparkSession
		val spark = SparkSession.builder()
			.master(&quot;local[*]&quot;)
			.appName(&quot;printline&quot;)
			.getOrCreate()

		import spark.implicits._
		var df = spark.readStream
			.format(&quot;kafka&quot;)
			.option(&quot;kafka.bootstrap.servers&quot;, &quot;localhost:9092&quot;)
			.option(&quot;subscribe&quot;, &quot;topic01&quot;)
			.load()
			.selectExpr(&quot;CAST(key AS STRING)&quot;, &quot;CAST(value AS STRING)&quot;)

		val wordCounts = df.select(&quot;value&quot;).as[String]
			.flatMap(_.split(&quot;\\s+&quot;))
			.coalesce(1)
			.map((_, 1))
			.toDF(&quot;word&quot;, &quot;count&quot;)

		val query = wordCounts.writeStream
			.format(&quot;json&quot;)
			.option(&quot;checkpointLocation&quot;, &quot;file:///Users/mashikang/IdeaProjects/structured_stream/src/main/resources/checkpoints&quot;)
			.outputMode(OutputMode.Append())
			.start(&quot;file:////Users/mashikang/IdeaProjects/structured_stream/src/main/resource/json&quot;)


		query.awaitTermination()
</code></pre>
<h3 id="kafkasinkappendupdatecomplete">KafkaSink((Append|Update|Complete))</h3>
<pre><code class="language-java">//1.创建SparkSession
		val spark = SparkSession.builder()
			.master(&quot;local[*]&quot;)
			.appName(&quot;printline&quot;)
			.getOrCreate()

		import spark.implicits._
		import org.apache.spark.sql.functions._

		var df = spark.readStream
			.format(&quot;kafka&quot;)
			.option(&quot;kafka.bootstrap.servers&quot;, &quot;localhost:9092&quot;)
			.option(&quot;subscribe&quot;, &quot;topic01&quot;)
			.load()
			.selectExpr(&quot;CAST(key AS STRING)&quot;, &quot;CAST(value AS STRING)&quot;)

		val wordCounts = df.select(&quot;value&quot;).as[String]
			.flatMap(_.split(&quot;\\s+&quot;))
			.coalesce(1)
			.map((_, 1))
			.toDF(&quot;word&quot;, &quot;count&quot;)
			.groupBy(&quot;word&quot;)
			.agg(sum(&quot;count&quot;) as &quot;count&quot;)
			.selectExpr(&quot;word&quot;, &quot;CAST(count AS STRING)&quot;)
			.withColumnRenamed(&quot;word&quot;, &quot;key&quot;)
			.withColumnRenamed(&quot;count&quot;, &quot;value&quot;)

		val query = wordCounts.writeStream
			.format(&quot;kafka&quot;)
			.option(&quot;kafka.bootstrap.servers&quot;, &quot;localhost:9092&quot;)
			.option(&quot;topic&quot;, &quot;topic02&quot;)
			.option(&quot;checkpointLocation&quot;, &quot;file:///Users/mashikang/IdeaProjects/structured_stream/src/main/resources/checkpoints&quot;)
			.outputMode(OutputMode.Update())
			.start()

		query.awaitTermination()
</code></pre>
<h3 id="foreach-sinkappendupdatecomplate">Foreach sink(Append|Update|Complate)</h3>
<h4 id="userrowwriter">UserRowWriter</h4>
<p>这里的 open方法在，每一次微批的时候触发，其中 <code>epochId</code>表示计算的批次。一般如果要保证<br>
<code>exactly-once</code> 语义的处理时候，需要在外围系统存储 <code>epochId</code>，如果存在重复计算 <code>epochId</code> 不<br>
变。</p>
<pre><code class="language-java">class UserRowWriter extends ForeachWriter[Row] {
	// 存储 上一次epochid
	var lastEpochId: Long = -1L

	/**
	 * 计算 当前是否处理当前批次，如果epochId=lastEpochId说明是重复记录，丢弃更新 false
	 * epochId!=lastEpochId 返回true 调用 open
	 *
	 * @param partitionId
	 * @param epochId
	 * @return
	 */
	override def open(partitionId: Long, epochId: Long): Boolean = {
		var flag: Boolean = false
		if (epochId != -1L) {
			if (lastEpochId == epochId) {
				// 是重复记录
				flag = false
			} else {
				flag = true
				lastEpochId = epochId
			}
		} else {
			// 第一次进来
			lastEpochId = epochId
			flag = true
		}
		flag
	}

	override def process(value: Row): Unit = {
		println(&quot; ,epochId:&quot; + lastEpochId)
	}

	override def close(errorOrNull: Throwable): Unit = {
		if (errorOrNull != null)
			errorOrNull.printStackTrace()
	}
}
</code></pre>
<pre><code class="language-java">		//1.创建SparkSession
		val spark = SparkSession.builder()
			.master(&quot;local[*]&quot;)
			.appName(&quot;printline&quot;)
			.getOrCreate()

		import spark.implicits._
		import org.apache.spark.sql.functions._

		var df = spark.readStream
			.format(&quot;kafka&quot;)
			.option(&quot;kafka.bootstrap.servers&quot;, &quot;localhost:9092&quot;)
			.option(&quot;subscribe&quot;, &quot;topic01&quot;)
			.load()
			.selectExpr(&quot;CAST(key AS STRING)&quot;, &quot;CAST(value AS STRING)&quot;)

		val wordCounts = df.select(&quot;value&quot;).as[String]
			.flatMap(_.split(&quot;\\s+&quot;))
			.coalesce(1)
			.map((_, 1))
			.toDF(&quot;word&quot;, &quot;count&quot;)
			.groupBy(&quot;word&quot;)
			.agg(sum(&quot;count&quot;) as &quot;count&quot;)
			.selectExpr(&quot;word&quot;, &quot;CAST(count AS STRING)&quot;)
			.withColumnRenamed(&quot;word&quot;, &quot;key&quot;)
			.withColumnRenamed(&quot;count&quot;, &quot;value&quot;)

		val query = wordCounts.writeStream
			.outputMode(OutputMode.Update())
			.foreach(new UserRowWriter)
			.start()

		query.awaitTermination()
</code></pre>
<h2 id="window-on-event-time">Window on Event Time</h2>
<p>Structured Streaming使用聚合函数基于EventTime计算window是非常简单的类似于分组聚合。分组聚<br>
合是按照指定的column字段对表中的数据进行分组，然后使用聚合函数对用户指定的column字段进行<br>
聚合。<br>
下面一张图描绘的是计算10分钟内的单词统计，每间隔5分钟滑动一个时间窗口。<br>
<img src="https://img-blog.csdnimg.cn/20200615160910610.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L00yODM1OTIzMzg=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述" loading="lazy"><br>
按照窗口原始含义是将落入到同一个窗口的数据进行分组，因此在Structured Streaming可以使用<br>
groupby和window表达窗口计算</p>
<pre><code class="language-java">		Logger.getLogger(&quot;org.apache.spark&quot;).setLevel(Level.WARN)
		Logger.getLogger(&quot;org.apache.jetty.server&quot;).setLevel(Level.OFF)

		//1.创建SparkSession
		val spark=SparkSession
			.builder()
			.master(&quot;local[*]&quot;)
			.appName(&quot;printline&quot;)
			.getOrCreate()

		import spark.implicits._
		//字符,时间戳
		var df=spark.readStream .format(&quot;socket&quot;)
			.option(&quot;host&quot;, &quot;localhost&quot;)
			.option(&quot;port&quot;, &quot;9999&quot;)
			.load()

		import org.apache.spark.sql.functions._
		var sdf=new SimpleDateFormat(&quot;mm:ss&quot;)

		val wordCounts=df.select(&quot;value&quot;)
			.as[String]
			.map(_.split(&quot;,&quot;))
			// 这里的Timestamp导如java.sql的依赖
			.map(tokens=&gt;(tokens(0),new Timestamp(tokens(1).toLong)))
			.toDF(&quot;word&quot;,&quot;timestamp&quot;)
			.groupBy(
				window($&quot;timestamp&quot;,&quot;10 seconds&quot;,&quot;5 seconds&quot;),
				$&quot;word&quot;
			)
			.count()
			.map(r=&gt;
				(sdf.format(r.getStruct(0).getTimestamp(0)),
					sdf.format(r.getStruct(0).getTimestamp(1)),
					r.getString(1),r.getLong(2)))
			.toDF(&quot;start&quot;,&quot;end&quot;,&quot;word&quot;,&quot;count&quot;)

		val query = wordCounts.writeStream
			.outputMode(OutputMode.Update())
			.format(&quot;console&quot;)
			.start()


		query.awaitTermination()
</code></pre>
<h2 id="处理延迟-data-和-watermarking">处理延迟 Data 和 Watermarking</h2>
<p>默认情况下，Spark会把落入到时间窗口的数据进行聚合操作。但是需要思考的是Event-Time是基于事<br>
件的时间戳进行窗口聚合的。那就有可能事件窗口已经触发很久了，但是有一些元素因为某种原因，导<br>
致迟到了，这个时候Spark需要将迟到的的数据加入到已经触发的窗口进行重复计算。但是需要注意如<br>
果在长时间的流计算过程中，如果不去限定窗口计算的时间，那么意味着Spark要在内存中一直存储窗<br>
口的状态，这样是不切实际的，因此Spark提供一种称为<code>watermarker</code>的机制用于限定存储在Spark内存<br>
中中间结果存储的时间，这样系统就可以将已经确定触发过的窗口的中间结果给删除。如果后续还有数<br>
据在窗口endtime以后抵达该窗口，Spark把这种数据定义为<code>late数据</code>。其中<code>watermarker</code>计算方式 <code>max event time seen by engine - late threshold</code>如果<code>watermarker</code>的取值大于了时间窗口的<br>
endtime即可认定该窗口的计算结果就可以被丢弃了。如果此时再有数据落入到已经被丢弃的时间窗<br>
口，则该迟到的数据会被Spark放弃更新，也就是丢弃。</p>
<blockquote>
<p>Watermarking=<code>max event time seen by engine - late threshold</code></p>
</blockquote>
<h3 id="watermarking保障机制">Watermarking保障机制：</h3>
<ul>
<li>能够保证在window的EndTime &gt; 水位线的窗口的状态Spark会存储起来，这个时候如果有迟到的<br>
数据再水位线没有淹没window之前Spark可以保障迟到的数据能正常的处理。</li>
<li>如果水位线已经没过了窗口的end时间，那么后续迟到数据不一定能够被处理，换句话说，迟到越<br>
久的数据 被处理的几率越小。</li>
</ul>
<blockquote>
<p>如果是使用<code>水位线计算</code> ，输出模式必须是Update或者Append,否则系统不会删除。</p>
</blockquote>
<pre><code class="language-java">		Logger.getLogger(&quot;org.apache.spark&quot;).setLevel(Level.WARN)
		Logger.getLogger(&quot;org.apache.jetty.server&quot;).setLevel(Level.OFF)

		//1.创建SparkSession
		val spark=SparkSession
			.builder()
			.master(&quot;local[*]&quot;)
			.appName(&quot;printline&quot;)
			.getOrCreate()

		import spark.implicits._
		//字符,时间戳
		var df=spark.readStream .format(&quot;socket&quot;)
			.option(&quot;host&quot;, &quot;localhost&quot;)
			.option(&quot;port&quot;, &quot;9999&quot;)
			.load()

		import org.apache.spark.sql.functions._
		var sdf=new SimpleDateFormat(&quot;mm:ss&quot;)

		val wordCounts=df.select(&quot;value&quot;)
			.as[String]
			.map(_.split(&quot;,&quot;))
			// 这里的Timestamp导如java.sql的依赖
			.map(tokens=&gt;(tokens(0),new Timestamp(tokens(1).toLong)))
			.toDF(&quot;word&quot;,&quot;timestamp&quot;)
			// 与上面窗口的API相比，多了水位线的设置
			.withWatermark(&quot;timestamp&quot;, &quot;5 seconds&quot;)
			.groupBy(
				window($&quot;timestamp&quot;,&quot;10 seconds&quot;,&quot;5 seconds&quot;),
				$&quot;word&quot;
			)
			.count()
			.map(r=&gt;
				(sdf.format(r.getStruct(0).getTimestamp(0)),
					sdf.format(r.getStruct(0).getTimestamp(1)),
					r.getString(1),r.getLong(2)))
			.toDF(&quot;start&quot;,&quot;end&quot;,&quot;word&quot;,&quot;count&quot;)

		val query = wordCounts.writeStream
			.outputMode(OutputMode.Update())
			.format(&quot;console&quot;)
			.start()


		query.awaitTermination()
</code></pre>
<h3 id="spark清除window聚合状态条件">Spark清除window聚合状态条件</h3>
<ul>
<li>Output mode 必须是 Append 或者 Update.，如果是Update 只要窗口有数据更新即可有输出。<br>
如果是Append，必须当水位线没过window的时候才会将Result写出。</li>
</ul>
<h4 id="update-mode">Update Mode</h4>
<figure data-type="image" tabindex="1"><img src="https://img-blog.csdnimg.cn/20200615175338128.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L00yODM1OTIzMzg=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述" loading="lazy"></figure>
<h4 id="append-mode">Append Mode</h4>
<figure data-type="image" tabindex="2"><img src="https://img-blog.csdnimg.cn/20200615175542447.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L00yODM1OTIzMzg=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述" loading="lazy"></figure>
<ul>
<li>必须在分组出现聚合使用时间column/window列</li>
<li>withWaterMaker的时间column必须和groupBy后面时间column保持一致，例如： 错误实例<code>df.withWatermark(&quot;time&quot;, &quot;1 min&quot;).groupBy(&quot;time2&quot;).count()</code>。</li>
<li>一定要在分组聚合之前调用withWaterMaking，例如<code>df.groupBy(&quot;time&quot;).count().withWatermark(&quot;time&quot;, &quot;1 min&quot;)</code> 错误实例<br>
<code>df.withWatermark(&quot;time&quot;, &quot;1 min&quot;).groupBy(&quot;time&quot;).count()</code>正确写法。</li>
</ul>
<h2 id="join-操作">Join 操作</h2>
<p>Structured Streaming 不仅仅支持对静态的 Dataset/DataFrame 做join操作，也支持对streaming<br>
Dataset/DataFrame实现join操作。</p>
<ul>
<li>Stream-static Joins <code>spark-2.0</code> 支持</li>
<li>Stream-stream Joins <code>Spark 2.3</code> 支持</li>
</ul>
<h3 id="stream-static-joins">Stream-static Joins</h3>
<pre><code class="language-java">		//1.创建SparkSession
		val spark=SparkSession
			.builder()
			.master(&quot;local[6]&quot;)
			.appName(&quot;printline&quot;)
			.getOrCreate()

		import spark.implicits._

		import org.apache.spark.sql.functions._

		/**
		 * +---+------+---+
		 * | id| name|age|
		 * +---+------+---+
		 * | 1| 张三 | 18|
		 * | 2| lisi| 28|
		 * | 3|wangwu| 38|
		 * +---+------+---+
		 */
		val userDF=spark.read
			.format(&quot;json&quot;)
			.load(&quot;/Users/mashikang/IdeaProjects/structured_stream/src/main/resources/json&quot;)
			.selectExpr(&quot;CAST(id AS INTEGER)&quot;,&quot;name&quot;,&quot;CAST(age AS INTEGER)&quot;)

		//1 apple 1 4.5
		var orderItemDF= spark.readStream
			.format(&quot;socket&quot;)
			.option(&quot;host&quot;,&quot;localhost&quot;)
			.option(&quot;port&quot;,9999)
			.load() .as[String]
			.map(line=&gt;line.split(&quot;\\s+&quot;))
			.map(tokens=&gt;(tokens(0).toInt,tokens(1),tokens(2).toInt,tokens(3).toDouble))
			.toDF(&quot;uid&quot;,&quot;item&quot;,&quot;count&quot;,&quot;price&quot;)

		val jointResults = orderItemDF.join(userDF,$&quot;id&quot;===$&quot;uid&quot;,&quot;left_outer&quot;)

		val query = jointResults
			.writeStream
			.format(&quot;console&quot;)
			.outputMode(OutputMode.Append())
			.start()

		query.awaitTermination()
</code></pre>
<h3 id="stream-stream-joins">Stream-stream Joins</h3>
<blockquote>
<ul>
<li>两边流都必须声明watermarker，告知引擎什么是可以清楚状态（默认取最低）。</li>
<li>需要在连接条件中添加eventTime column的时间约束，这样引擎就知道什么时候可以清除后续<br>
的流的状态。
<ul>
<li>Time range join conditions</li>
<li>Join on event-time windows</li>
</ul>
</li>
</ul>
</blockquote>
<h4 id="inner-join">inner join</h4>
<ul>
<li><strong>方案1 Time range join conditions</strong></li>
</ul>
<pre><code class="language-java">		//1.创建SparkSession
		val spark = SparkSession
			.builder()
			.master(&quot;local[*]&quot;)
			.appName(&quot;printline&quot;)
			.getOrCreate()

		import spark.implicits._

		//001 apple 1 4.5 1566529410000
		val orderDF = spark
			.readStream
			.format(&quot;socket&quot;)
			.option(&quot;host&quot;, &quot;localhost&quot;)
			.option(&quot;port&quot;, 9999)
			.load()
			.map(row =&gt; row.getAs[String](&quot;value&quot;).split(&quot;\\s+&quot;))
			.map(t =&gt; (t(0), t(1), (t(2).toInt * t(3).toDouble), new Timestamp(t(4).toLong)))
			.toDF(&quot;uid&quot;, &quot;item&quot;, &quot;cost&quot;, &quot;order_time&quot;)

		//001 zhangsan 1566529410000
		val userDF = spark
			.readStream
			.format(&quot;socket&quot;)
			.option(&quot;host&quot;, &quot;localhost&quot;)
			.option(&quot;port&quot;, 8888)
			.load()
			.map(row =&gt; row.getAs[String](&quot;value&quot;).split(&quot;\\s+&quot;))
			.map(t =&gt; (t(0), t(1), new Timestamp(t(2).toLong)))
			.toDF(&quot;id&quot;, &quot;name&quot;, &quot;login_time&quot;)

		import org.apache.spark.sql.functions._

		//用户的登陆数据缓存 2 seconds 订单数据缓存4秒
		val userWatermarker = userDF.withWatermark(&quot;login_time&quot;, &quot;2 seconds&quot;)
		val orderWaterMarker = orderDF.withWatermark(&quot;order_time&quot;, &quot;4 seconds&quot;)

		//连接 用户登陆以后将2秒以内的购买行为和用进行join 
		val joinDF = userWatermarker.join(orderWaterMarker,
			expr(
				&quot;&quot;&quot;
				  |id=uid and order_time &gt;= login_time and order_time &lt;= login_time + interval 2 seconds 
				&quot;&quot;&quot;.stripMargin)
		)
		val query = joinDF.writeStream
			.format(&quot;console&quot;)
			.outputMode(OutputMode.Append()).start() 
		
		query.awaitTermination()
</code></pre>
<ul>
<li><strong>方案2Join on event-time windows</strong></li>
</ul>
<pre><code class="language-java">	//1.创建SparkSession
		val spark = SparkSession
			.builder()
			.master(&quot;local[*]&quot;)
			.appName(&quot;printline&quot;)
			.getOrCreate()

		import spark.implicits._

		//001 apple 1 4.5 1566529410000
		val orderDF = spark
			.readStream
			.format(&quot;socket&quot;)
			.option(&quot;host&quot;, &quot;localhost&quot;)
			.option(&quot;port&quot;, 9999)
			.load()
			.map(row =&gt; row.getAs[String](&quot;value&quot;).split(&quot;\\s+&quot;))
			.map(t =&gt; (t(0), t(1), (t(2).toInt * t(3).toDouble), new Timestamp(t(4).toLong)))
			.toDF(&quot;uid&quot;, &quot;item&quot;, &quot;cost&quot;, &quot;order_time&quot;)

		//001 zhangsan 1566529410000
		val userDF = spark
			.readStream
			.format(&quot;socket&quot;)
			.option(&quot;host&quot;, &quot;localhost&quot;)
			.option(&quot;port&quot;, 8888)
			.load()
			.map(row =&gt; row.getAs[String](&quot;value&quot;).split(&quot;\\s+&quot;))
			.map(t =&gt; (t(0), t(1), new Timestamp(t(2).toLong)))
			.toDF(&quot;id&quot;, &quot;name&quot;, &quot;login_time&quot;)

		import org.apache.spark.sql.functions._

		//用户的登陆数据缓存 2 seconds 订单数据缓存4秒
		val userWatermarker = userDF.withWatermark(&quot;login_time&quot;, &quot;2 seconds&quot;)
			.select(
				window($&quot;login_time&quot;, &quot;5 seconds&quot;),
				$&quot;id&quot;, $&quot;name&quot;, $&quot;login_time&quot;)
			.withColumnRenamed(&quot;window&quot;, &quot;leftWindow&quot;)

		val orderWaterMarker = orderDF.withWatermark(&quot;order_time&quot;, &quot;4 seconds&quot;)
			.select(
				window($&quot;order_time&quot;, &quot;5 seconds&quot;),
				$&quot;uid&quot;, $&quot;item&quot;, $&quot;cost&quot;, $&quot;order_time&quot;)
			.withColumnRenamed(&quot;window&quot;, &quot;rightWindow&quot;)

		//连接用户登陆以后将2秒以内的购买行为和用进行join
		val joinDF = userWatermarker
			.join(
				orderWaterMarker,
				expr(
					&quot;&quot;&quot;
					  |id=uid and leftWindow = rightWindow
					&quot;&quot;&quot;.stripMargin)
			)
			
		val query = joinDF.writeStream
			.format(&quot;console&quot;)
			.outputMode(OutputMode.Append()).start() 
		
		query.awaitTermination()
</code></pre>
<h4 id="outer-join">outer join</h4>
<pre><code class="language-java">	//1.创建SparkSession
		val spark = SparkSession
			.builder()
			.master(&quot;local[*]&quot;)
			.appName(&quot;printline&quot;)
			.getOrCreate()

		import spark.implicits._

		//001 apple 1 4.5 1566529410000
		val orderDF = spark
			.readStream
			.format(&quot;socket&quot;)
			.option(&quot;host&quot;, &quot;localhost&quot;)
			.option(&quot;port&quot;, 9999)
			.load()
			.map(row =&gt; row.getAs[String](&quot;value&quot;).split(&quot;\\s+&quot;))
			.map(t =&gt; (t(0), t(1), (t(2).toInt * t(3).toDouble), new Timestamp(t(4).toLong)))
			.toDF(&quot;uid&quot;, &quot;item&quot;, &quot;cost&quot;, &quot;order_time&quot;)

		//001 zhangsan 1566529410000
		val userDF = spark
			.readStream
			.format(&quot;socket&quot;)
			.option(&quot;host&quot;, &quot;localhost&quot;)
			.option(&quot;port&quot;, 8888)
			.load()
			.map(row =&gt; row.getAs[String](&quot;value&quot;).split(&quot;\\s+&quot;))
			.map(t =&gt; (t(0), t(1), new Timestamp(t(2).toLong)))
			.toDF(&quot;id&quot;, &quot;name&quot;, &quot;login_time&quot;)

		import org.apache.spark.sql.functions._

		//系统分别会对 user 和 order 缓存 最近 1 seconds 和 2 seconds 数据,
		// 一旦时间过去，系统就无 法保证数据状态继续保留
		val loginWatermarker=userDF.withWatermark(&quot;login_time&quot;,&quot;1 second&quot;)
		val orderWatermarker=orderDF.withWatermark(&quot;order_time&quot;,&quot;2 seconds&quot;)

		//计算订单的时间 &amp; 用户 登陆之后的0~1 seconds 关联 数据 并且进行join
		val joinDF = loginWatermarker
		.join(
				orderWatermarker,
				expr(&quot;uid=id and order_time &gt;= login_time and order_time &lt;= login_time + interval 1 seconds&quot;),
				&quot;leftOuter&quot;
			)
			
		val query = joinDF.writeStream
			.format(&quot;console&quot;)
			.outputMode(OutputMode.Append()).start() 
		
		query.awaitTermination()
</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Spark入门(七)——最全的Saprk SQL算子介绍与使用(下)]]></title>
        <id>https://mask0407.github.io/spark06/</id>
        <link href="https://mask0407.github.io/spark06/">
        </link>
        <updated>2020-06-29T03:16:05.000Z</updated>
        <content type="html"><![CDATA[<p><ul class="markdownIt-TOC">
<li>
<ul>
<li><a href="#sql%E8%AF%AD%E6%B3%95%E6%9F%A5%E8%AF%A2">SQL语法查询</a>
<ul>
<li><a href="#%E5%8D%95%E8%A1%8C%E6%9F%A5%E8%AF%A2">单行查询</a></li>
<li><a href="#%E6%A8%A1%E7%B3%8A%E6%9F%A5%E8%AF%A2">模糊查询</a></li>
<li><a href="#%E6%8E%92%E5%BA%8F%E6%9F%A5%E8%AF%A2">排序查询</a></li>
<li><a href="#limit%E6%9F%A5%E8%AF%A2">limit查询</a></li>
<li><a href="#%E5%88%86%E7%BB%84%E6%9F%A5%E8%AF%A2">分组查询</a></li>
<li><a href="#having%E8%BF%87%E6%BB%A4">having过滤</a></li>
<li><a href="#case-when">case-when</a></li>
<li><a href="#%E8%A1%8C%E8%BD%AC%E5%88%97">行转列</a></li>
<li><a href="#pivot">pivot</a></li>
<li><a href="#cube%E8%AE%A1%E7%AE%97">Cube计算</a></li>
<li><a href="#join%E8%A1%A8%E8%BF%9E%E6%8E%A5">Join表连接</a></li>
<li><a href="#%E5%AD%90%E6%9F%A5%E8%AF%A2">子查询</a></li>
<li><a href="#%E5%BC%80%E7%AA%97%E5%87%BD%E6%95%B0">开窗函数</a>
<ul>
<li><a href="#%E5%BC%80%E7%AA%97%E5%87%BD%E6%95%B0sql%E8%A7%A3%E8%AF%BB">开窗函数SQL解读</a></li>
<li><a href="#row_num">ROW_NUM</a></li>
<li><a href="#rank">RANK()</a></li>
<li><a href="#dense_rank-%E5%AF%86%E9%9B%86%E6%8E%92%E5%90%8D">DENSE_RANK() /密集排名</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#%E8%87%AA%E5%AE%9A%E4%B9%89%E5%87%BD%E6%95%B0">自定义函数</a>
<ul>
<li><a href="#%E5%8D%95%E8%A1%8C%E5%87%BD%E6%95%B0">单行函数</a></li>
<li><a href="#%E8%81%9A%E5%90%88%E5%87%BD%E6%95%B0untyped">聚合函数（untyped）</a></li>
</ul>
</li>
<li><a href="#loadsave">Load/Save</a>
<ul>
<li><a href="#paquet">Paquet</a></li>
<li><a href="#json">JSON</a></li>
<li><a href="#orc%E5%AD%98%E5%82%A8%E5%8E%8B%E7%BC%A9%E6%A0%BC%E5%BC%8F%E6%AF%94%E8%BE%83%E8%8A%82%E7%9C%81%E7%A9%BA%E9%97%B4">ORC(存储压缩格式，比较节省空间)</a></li>
<li><a href="#csv">CSV</a></li>
<li><a href="#jdbc">JDBC</a></li>
<li><a href="#dataframe%E8%BD%AC%E4%B8%BArdd">DataFrame转为RDD</a></li>
</ul>
</li>
</ul>
</li>
</ul>
(Spark SQL 查询)</p>
<h2 id="sql语法查询">SQL语法查询</h2>
<h3 id="单行查询">单行查询</h3>
<pre><code class="language-java">// 单行查询
var userDF = List((1, &quot;张三&quot;, true, 18, 15000, 1))
	.toDF(&quot;id&quot;, &quot;name&quot;, &quot;sex&quot;, &quot;age&quot;, &quot;salary&quot;, &quot;dept&quot;)
	
userDF.createTempView(&quot;t_employee&quot;)

val sql = &quot;select * from t_employee where name = '张三'&quot;
spark.sql(sql)
	.show()
</code></pre>
<pre><code>+---+----+----+---+------+----+
| id|name| sex|age|salary|dept|
+---+----+----+---+------+----+
|  1|张三|true| 18| 15000|   1|
+---+----+----+---+------+----+
</code></pre>
<h3 id="模糊查询">模糊查询</h3>
<pre><code class="language-java">var userDF= List((1,&quot;张三&quot;,true,18,15000,1))
	.toDF(&quot;id&quot;,&quot;name&quot;,&quot;sex&quot;,&quot;age&quot;,&quot;salary&quot;,&quot;dept&quot;)
	
userDF.createTempView(&quot;t_employee&quot;)

val sql=&quot;select * from t_employee where name like '%三%'&quot;
spark.sql(sql)
	.show()
</code></pre>
<pre><code>+---+----+----+---+------+----+
| id|name| sex|age|salary|dept|
+---+----+----+---+------+----+
|  1|张三|true| 18| 15000|   1|
+---+----+----+---+------+----+
</code></pre>
<h3 id="排序查询">排序查询</h3>
<pre><code class="language-java">var userDF = List((1, &quot;张三&quot;, true, 18, 15000, 1), (2, &quot;ls&quot;, false, 18, 12000, 1))
	.toDF(&quot;id&quot;, &quot;name&quot;, &quot;sex&quot;, &quot;age&quot;, &quot;salary&quot;, &quot;dept&quot;)
//构建视图
userDF.createTempView(&quot;t_employee&quot;)
val sql =
	&quot;&quot;&quot;
	  |select * from t_employee where salary &gt; 10000 order by salary desc
	&quot;&quot;&quot;
	.stripMargin
spark.sql(sql)
	.show()
</code></pre>
<pre><code>+---+----+-----+---+------+----+
| id|name|  sex|age|salary|dept|
+---+----+-----+---+------+----+
|  1|张三| true| 18| 15000|   1|
|  2|李四|false| 18| 12000|   1|
+---+----+-----+---+------+----+
</code></pre>
<h3 id="limit查询">limit查询</h3>
<pre><code class="language-java">var userDF= List( (1,&quot;张三&quot;,true,18,15000,1), 
	(2,&quot;李四&quot;,false,18,12000,1), 
	(3,&quot;王五&quot;,false,18,16000,2)
) .toDF(&quot;id&quot;,&quot;name&quot;,&quot;sex&quot;,&quot;age&quot;,&quot;salary&quot;,&quot;dept&quot;)
 
//构建视图 
userDF.createTempView(&quot;t_employee&quot;) 
val sql=
	&quot;&quot;&quot;
	  |select * from t_employee where salary &gt; 10000 order by salary desc limit 2 
	&quot;&quot;&quot;.stripMargin
spark.sql(sql)
	.show()
</code></pre>
<pre><code>+---+----+-----+---+------+----+
| id|name|  sex|age|salary|dept|
+---+----+-----+---+------+----+
|  3|王五|false| 18| 16000|   2|
|  1|张三| true| 18| 15000|   1|
+---+----+-----+---+------+----+
</code></pre>
<h3 id="分组查询">分组查询</h3>
<pre><code class="language-java">var userDF= List( (1,&quot;张三&quot;,true,18,15000,1),
	(2,&quot;李四&quot;,false,18,12000,1),
	(3,&quot;王五&quot;,false,18,16000,2)
) .toDF(&quot;id&quot;,&quot;name&quot;,&quot;sex&quot;,&quot;age&quot;,&quot;salary&quot;,&quot;dept&quot;)

//构建视图
userDF.createTempView(&quot;t_employee&quot;)
val sql=
	&quot;&quot;&quot;
	  |select dept ,avg(salary) as avg_slalary from t_employee
	  |group by dept order by avg_slalary desc
	&quot;&quot;&quot;.stripMargin
spark.sql(sql)
	.show()
</code></pre>
<pre><code>+----+-----------+
|dept|avg_slalary|
+----+-----------+
|   2|    16000.0|
|   1|    13500.0|
+----+-----------+
</code></pre>
<h3 id="having过滤">having过滤</h3>
<pre><code class="language-java">var userDF= List( (1,&quot;张三&quot;,true,18,15000,1),
	(2,&quot;李四&quot;,false,18,12000,1),
	(3,&quot;王五&quot;,false,18,16000,2)
) .toDF(&quot;id&quot;,&quot;name&quot;,&quot;sex&quot;,&quot;age&quot;,&quot;salary&quot;,&quot;dept&quot;)

//构建视图
userDF.createTempView(&quot;t_employee&quot;)
val sql=
	&quot;&quot;&quot;
	  | select dept ,avg(salary) as avg_slalary
	  | from t_employee group by dept 
	  | having avg_slalary &gt; 13500 
	  | order by avg_slalary desc
	&quot;&quot;&quot;.stripMargin
spark.sql(sql)
	.show()
</code></pre>
<pre><code>+----+-----------+
|dept|avg_slalary|
+----+-----------+
|   2|    16000.0|
+----+-----------+
</code></pre>
<h3 id="case-when">case-when</h3>
<pre><code class="language-java">var userDF= List( (1,&quot;张三&quot;,true,18,15000,1),
	(2,&quot;李四&quot;,false,18,12000,1),
	(3,&quot;王五&quot;,false,18,16000,2)
) .toDF(&quot;id&quot;,&quot;name&quot;,&quot;sex&quot;,&quot;age&quot;,&quot;salary&quot;,&quot;dept&quot;)

//构建视图
userDF.createTempView(&quot;t_employee&quot;)
val sql=
	&quot;&quot;&quot;
	  |select id,name,case sex when true then '男' else '女' end as sex_alias
	  |from t_employee
	&quot;&quot;&quot;.stripMargin
spark.sql(sql)
	.show()
</code></pre>
<pre><code>+---+----+---------+
| id|name|sex_alias|
+---+----+---------+
|  1|张三|       男|
|  2|李四|       女|
|  3|王五|       女|
+---+----+---------+
</code></pre>
<h3 id="行转列">行转列</h3>
<pre><code class="language-java">// 行转列
var scoreDF = List(
	(1, &quot;语文&quot;, 100),
	(1, &quot;数学&quot;, 100),
	(1, &quot;英语&quot;, 100),
	(2, &quot;数学&quot;, 79),
	(2, &quot;语文&quot;, 80),
	(2, &quot;英语&quot;, 100)
).toDF(&quot;id&quot;, &quot;course&quot;, &quot;score&quot;)
scoreDF.createOrReplaceTempView(&quot;t_course&quot;)

val sql =
	&quot;&quot;&quot;
	  |  select id,
	  |  max(case course when '数学' then score else 0 end) as math,
	  |  max(case course when '英语' then score else 0 end) as english,
	  |  max(case course when '语文' then score else 0 end) as chinese
	  |  from t_course group by id
	&quot;&quot;&quot;.stripMargin
	
spark.sql(sql)
	.show()
</code></pre>
<pre><code>+---+----+-------+-------+
| id|math|english|chinese|
+---+----+-------+-------+
|  1| 100|    100|    100|
|  2|  79|    100|     80|
+---+----+-------+-------+
</code></pre>
<h3 id="pivot">pivot</h3>
<pre><code class="language-java">var scoreDF = List(
	(1, &quot;语文&quot;, 100),
	(1, &quot;数学&quot;, 100),
	(1, &quot;英语&quot;, 100),
	(2, &quot;数学&quot;, 79),
	(2, &quot;语文&quot;, 80),
	(2, &quot;英语&quot;, 100)
).toDF(&quot;id&quot;, &quot;course&quot;, &quot;score&quot;)
scoreDF.createOrReplaceTempView(&quot;t_course&quot;)

val sql =
	&quot;&quot;&quot;
	  |select * 
	  |from t_course 
	  |pivot(max(score) for course in ('数学' ,'语文','英语'))
	  |
	&quot;&quot;&quot;.stripMargin
	  
spark.sql(sql)
	.show()
</code></pre>
<pre><code>+---+----+----+----+
| id|数学|语文|英语|
+---+----+----+----+
|  1| 100| 100| 100|
|  2|  79|  80| 100|
+---+----+----+----+
</code></pre>
<blockquote>
<p>在书写SQL的时候除去聚合字段和输出列明字段，其他字段作为groupby后的隐藏字段。</p>
</blockquote>
<h3 id="cube计算">Cube计算</h3>
<pre><code class="language-java">// Cube计算
val frame = List(
	(110, 50, 80, 80),
	(120, 60, 95, 75),
	(120, 50, 96, 70)
) .toDF(&quot;height&quot;, &quot;weight&quot;, &quot;uiq&quot;, &quot;ueq&quot;)
frame.createTempView(&quot;t_user&quot;)

val sql=
	&quot;&quot;&quot;
	  |select height,weight,avg(uiq),avg(ueq)
	  |from t_user
	  |group by cube(height,weight)
	&quot;&quot;&quot;.stripMargin
	
spark.sql(sql)
	.show()
</code></pre>
<pre><code>+------+------+-----------------+--------+
|height|weight|         avg(uiq)|avg(ueq)|
+------+------+-----------------+--------+
|   110|    50|             80.0|    80.0|
|   120|  null|             95.5|    72.5|
|   120|    60|             95.0|    75.0|
|  null|    60|             95.0|    75.0|	 // weight 是60的所有数据 的uiq、ueq平均值
|  null|  null|90.33333333333333|    75.0|   // 所有数据的uiq、ueq平均值
|   120|    50|             96.0|    70.0|
|   110|  null|             80.0|    80.0|
|  null|    50|             88.0|    75.0|
+------+------+-----------------+--------+
</code></pre>
<h3 id="join表连接">Join表连接</h3>
<pre><code class="language-java">// join
val userCatagoryCostDF=List(
	(1,&quot;电脑配件&quot;,100),
	(1,&quot;母婴用品&quot;,100), 
	(1,&quot;生活用品&quot;,100),
	(2,&quot;居家美食&quot;,79),
	(2,&quot;消费电子&quot;,80),
	(2,&quot;生活用品&quot;,100)
).toDF(&quot;uid&quot;,&quot;category&quot;,&quot;cost&quot;)

val usersDF= List(
	(1,&quot;张晓三&quot;,true,18,15000),
	(2,&quot;李晓四&quot;,true,18,18000),
	(3,&quot;王晓五&quot;,false,18,10000)
).toDF(&quot;id&quot;,&quot;name&quot;,&quot;sex&quot;,&quot;age&quot;,&quot;salary&quot;)

usersDF.createTempView(&quot;t_user&quot;)
userCatagoryCostDF.createTempView(&quot;t_user_cost&quot;)

val sql =
	&quot;&quot;&quot;
	  |select u.*,o.*
	  |from t_user u
	  |left join t_user_cost o
	  |on u.id=o.uid
	  |where uid is not null
	&quot;&quot;&quot;.stripMargin
	
spark.sql(sql)
	.show()
</code></pre>
<pre><code>+---+------+----+---+------+---+--------+----+
| id|  name| sex|age|salary|uid|category|cost|
+---+------+----+---+------+---+--------+----+
|  1|张晓三|true| 18| 15000|  1|电脑配件| 100|
|  1|张晓三|true| 18| 15000|  1|母婴用品| 100|
|  1|张晓三|true| 18| 15000|  1|生活用品| 100|
|  2|李晓四|true| 18| 18000|  2|居家美食|  79|
|  2|李晓四|true| 18| 18000|  2|消费电子|  80|
|  2|李晓四|true| 18| 18000|  2|生活用品| 100|
+---+------+----+---+------+---+--------+----+
</code></pre>
<h3 id="子查询">子查询</h3>
<pre><code class="language-java">// 子查询
var df=List(
	(1,&quot;zs&quot;,true,1,15000),
	(2,&quot;ls&quot;,false,2,18000),
	(3,&quot;ww&quot;,false,2,14000),
	(4,&quot;zl&quot;,false,1,18000),
	(5,&quot;win7&quot;,false,1,16000)
).toDF(&quot;id&quot;,&quot;name&quot;,&quot;sex&quot;,&quot;dept&quot;,&quot;salary&quot;)
df.createTempView(&quot;t_employee&quot;)

val sql=
	&quot;&quot;&quot;
	  |select id,name,salary,dept,
	  |(select avg(salary) from t_employee t2 where t1.dept=t2.dept) as avg_salary
	  |from t_employee t1
	  |order by dept desc
	&quot;&quot;&quot;.stripMargin
	
spark.sql(sql)
	.show()
</code></pre>
<pre><code>+---+----+------+----+------------------+
| id|name|salary|dept|        avg_salary|
+---+----+------+----+------------------+
|  2|  ls| 18000|   2|           16000.0|
|  3|  ww| 14000|   2|           16000.0|
|  5|win7| 16000|   1|16333.333333333334|
|  1|  zs| 15000|   1|16333.333333333334|
|  4|  zl| 18000|   1|16333.333333333334|
+---+----+------+----+------------------+
</code></pre>
<blockquote>
<p>在spark SQL不允许在子查询中使用非等值连接。（MySQL|Oracle支持）</p>
</blockquote>
<h3 id="开窗函数">开窗函数</h3>
<p>在正常的统计分析中 ，通常使用聚合函数作为分析，聚合分析函数的特点是将n行记录合并成一行，在数据库的统计当中 还有一种统计称为开窗统计，开窗函数可以实现将一行变成多行。可以将数据库查询的每一条记录比作是一幢高楼的一 层, 开窗函数就是在每一层开一扇窗, 让每一层能看到整装楼的全貌或一部分。</p>
<pre><code class="language-java">// 开窗函数
var df=List(
	(1,&quot;zs&quot;,true,1,15000),
	(2,&quot;ls&quot;,false,2,18000),
	(3,&quot;ww&quot;,false,2,14000),
	(4,&quot;zl&quot;,false,1,18000),
	(5,&quot;win7&quot;,false,1,16000)
).toDF(&quot;id&quot;,&quot;name&quot;,&quot;sex&quot;,&quot;dept&quot;,&quot;salary&quot;)
df.createTempView(&quot;t_employee&quot;)

val sql=
	&quot;&quot;&quot;
	  |select id,name,salary,dept,
	  |count(id) over(partition by dept order by salary desc) as rank,
	  |(count(id) over(partition by dept order by salary desc rows between current row and unbounded following) - 1) as low_than_me,
	  |avg(salary) over(partition by dept rows between unbounded preceding and unbounded following) as avg_salary,
	  |avg(salary) over() as all_avg_salary 
	  |from t_employee t1 order by dept desc
	&quot;&quot;&quot;.stripMargin
	
spark.sql(sql)
	.show()
spark.stop()
</code></pre>
<pre><code>+---+----+------+----+----+-----------+------------------+--------------+
| id|name|salary|dept|rank|low_than_me|        avg_salary|all_avg_salary|
+---+----+------+----+----+-----------+------------------+--------------+
|  2|  ls| 18000|   2|   1|          1|           16000.0|       16200.0|
|  3|  ww| 14000|   2|   2|          0|           16000.0|       16200.0|
|  4|  zl| 18000|   1|   1|          2|16333.333333333334|       16200.0|
|  5|win7| 16000|   1|   2|          1|16333.333333333334|       16200.0|
|  1|  zs| 15000|   1|   3|          0|16333.333333333334|       16200.0|
+---+----+------+----+----+-----------+------------------+--------------+
</code></pre>
<h4 id="开窗函数sql解读">开窗函数SQL解读</h4>
<pre><code class="language-sql">	  select id,name,salary,dept,
	  # 按部门分组、工资倒叙排序展示 当前部门的id总数
	  count(id) over(partition by dept order by salary desc) as rank,
	  # 按部门分组、工资倒叙排序展示当前行至最后一行id总数-1
	  (count(id) over(partition by dept order by salary desc rows between current row and unbounded following) - 1) as low_than_me,
	  # 按部门分组展示首行至尾行的平均工资 如：2部门平均工资16000 1部门平均工资16333.333333333334
	  avg(salary) over(partition by dept rows between unbounded preceding and unbounded following) as avg_salary,
	  # 展示所有员工的平均工资
	  avg(salary) over() as all_avg_salary 
	  from t_employee t1 order by dept desc
</code></pre>
<ul>
<li>总结</li>
</ul>
<blockquote>
<p>聚合函数(字段) over ([[partition by 字段] order by 字段 asc [rows between 起始行偏移量 and 终止偏移量]] )</p>
</blockquote>
<ul>
<li>其中：偏移量的取值</li>
</ul>
<blockquote>
<p>preceding：用于累加前N行（分区之内）。若是从分区第一行头开始，则为 unbounded。 N为：相对当前行向前 的偏移量<code>负数</code>。<br>
following:与preceding相反，累加后N行（分区之内）。若是累加到该分区结束则为unbounded。N为：相对当 前行向后的偏移量<code>正数</code> current row：顾名思义，当前行，偏移量为0</p>
</blockquote>
<h4 id="row_num">ROW_NUM</h4>
<p>统计当前记录所在的行号</p>
<pre><code class="language-java">// ROW_NUM
var df=List(
	(1,&quot;zs&quot;,true,1,15000),
	(2,&quot;ls&quot;,false,2,18000),
	(3,&quot;ww&quot;,false,2,14000),
	(4,&quot;zl&quot;,false,1,18000),
	(5,&quot;win7&quot;,false,1,16000)
).toDF(&quot;id&quot;,&quot;name&quot;,&quot;sex&quot;,&quot;dept&quot;,&quot;salary&quot;)
df.createTempView(&quot;t_employee&quot;)

val sql=
	&quot;&quot;&quot;
	  |select id,name,salary,dept,
	  |ROW_NUMBER() over(partition by dept order by salary desc) as rank
	  |from t_employee t1
	  |order by dept desc
	&quot;&quot;&quot;.stripMargin
	
spark.sql(sql)
	.show()
</code></pre>
<pre><code>+---+----+------+----+----+
| id|name|salary|dept|rank|
+---+----+------+----+----+
|  2|  ls| 18000|   2|   1|
|  3|  ww| 14000|   2|   2|
|  4|  zl| 18000|   1|   1|
|  5|win7| 16000|   1|   2|
|  1|  zs| 15000|   1|   3|
+---+----+------+----+----+
</code></pre>
<blockquote>
<p>如果部门存在相同薪资此时ROW_NUMBER只能表示当前记录在窗口行标</p>
</blockquote>
<h4 id="rank">RANK()</h4>
<pre><code class="language-java">// RANK
var df=List(
	(1,&quot;zs&quot;,true,1,15000),
	(2,&quot;ls&quot;,false,2,18000),
	(3,&quot;ww&quot;,false,2,14000),
	(4,&quot;zl&quot;,false,1,18000),
	(6,&quot;zl1&quot;,true,1,18000),
	(5,&quot;win7&quot;,false,1,16000)
).toDF(&quot;id&quot;,&quot;name&quot;,&quot;sex&quot;,&quot;dept&quot;,&quot;salary&quot;)
df.createTempView(&quot;t_employee&quot;)

val sql=
	&quot;&quot;&quot;
	  |select id,name,salary,dept,
	  |RANK() over(partition by dept order by salary desc) as rank
	  |from t_employee t1
	  |order by dept desc
	&quot;&quot;&quot;.stripMargin
	
spark.sql(sql)
	.show()
</code></pre>
<pre><code>+---+----+------+----+----+
| id|name|salary|dept|rank|
+---+----+------+----+----+
|  2|  ls| 18000|   2|   1|
|  3|  ww| 14000|   2|   2|
|  4|  zl| 18000|   1|   1|
|  6| zl1| 18000|   1|   1|
|  5|win7| 16000|   1|   3| //因为出现两个排名为1的，所有这里是3，故而排名序号不连续
|  1|  zs| 15000|   1|   4|
+---+----+------+----+----+
</code></pre>
<blockquote>
<p>与ROW_NUM相比，排名特点是不连续。</p>
</blockquote>
<h4 id="dense_rank-密集排名">DENSE_RANK() /密集排名</h4>
<pre><code class="language-java">// DENSE_RANK/密集排名
var df = List(
	(1, &quot;zs&quot;, true, 1, 15000),
	(2, &quot;ls&quot;, false, 2, 18000),
	(3, &quot;ww&quot;, false, 2, 14000),
	(4, &quot;zl&quot;, false, 1, 18000),
	(6, &quot;zl1&quot;, true, 1, 18000),
	(5, &quot;win7&quot;, false, 1, 16000)
).toDF(&quot;id&quot;, &quot;name&quot;, &quot;sex&quot;, &quot;dept&quot;, &quot;salary&quot;)
df.createTempView(&quot;t_employee&quot;)

val sql =
	&quot;&quot;&quot;
	  |select id,name,salary,dept,
	  |DENSE_RANK() over(partition by dept order by salary desc) as rank
	  |from t_employee t1
	  |order by dept desc
	&quot;&quot;&quot;.stripMargin
	
spark.sql(sql)
	.show()
</code></pre>
<pre><code>+---+----+------+----+----+
| id|name|salary|dept|rank|
+---+----+------+----+----+
|  3|  ww| 14000|   2|   2|
|  2|  ls| 18000|   2|   1|
|  4|  zl| 18000|   1|   1|
|  6| zl1| 18000|   1|   1|
|  1|  zs| 15000|   1|   3|
|  5|win7| 16000|   1|   2|
+---+----+------+----+----+
</code></pre>
<h2 id="自定义函数">自定义函数</h2>
<h3 id="单行函数">单行函数</h3>
<pre><code class="language-java">// 自定义单行函数
var df = List(
	(1, &quot;zs&quot;, true, 1, 15000),
	(2, &quot;ls&quot;, false, 2, 18000),
	(3, &quot;ww&quot;, false, 2, 14000),
	(4, &quot;zl&quot;, false, 1, 18000),
	(6, &quot;zl1&quot;, true, 1, 18000),
	(5, &quot;win7&quot;, false, 1, 16000)
).toDF(&quot;id&quot;, &quot;name&quot;, &quot;sex&quot;, &quot;dept&quot;, &quot;salary&quot;)
df.createTempView(&quot;t_employee&quot;)

spark.udf
	.register(&quot;convertSex&quot;, (sex: Boolean) =&gt; {
		sex match {
			case true =&gt; &quot;男&quot;
			case false =&gt; &quot;女&quot;
		}
	})
	
val sql =
	&quot;&quot;&quot;
	  |select id,name,convertSex(sex) as usex
	  |from t_employee
	&quot;&quot;&quot;.stripMargin
	
spark.sql(sql)
	.show()
</code></pre>
<pre><code>+---+----+----+
| id|name|usex|
+---+----+----+
|  1|  zs|  男|
|  2|  ls|  女|
|  3|  ww|  女|
|  4|  zl|  女|
|  6| zl1|  男|
|  5|win7|  女|
+---+----+----+
</code></pre>
<h3 id="聚合函数untyped">聚合函数（untyped）</h3>
<p>只需要写一个类继承 <code>UserDefinedAggregateFunction</code> 即可。</p>
<pre><code class="language-java">import org.apache.spark.sql.Row
import org.apache.spark.sql.expressions.{MutableAggregationBuffer, UserDefinedAggregateFunction}
import org.apache.spark.sql.types.{DataType, DoubleType, StructType}

class CustomSum extends UserDefinedAggregateFunction {
	//1.输入的字段类型信息 name属性 叫什么无所谓
	override def inputSchema: StructType = {
		new StructType().add(&quot;salary&quot;, DoubleType)
	}

	//2.中间结果变量类型
	override def bufferSchema: StructType = {
		new StructType().add(&quot;taotalsalary&quot;, DoubleType)
	}

	//3.最终返回结果的类型
	override def dataType: DataType = DoubleType

	//4.设置返回结果类型是否固定
	override def deterministic: Boolean = true

	//5.初始化中间结果
	override def initialize(buffer: MutableAggregationBuffer): Unit = {
		//第0个位置元素是0.0
		buffer.update(0, 0.0)
	}

	//6.将传如的数值添加到中间结果变量中
	override def update(buffer: MutableAggregationBuffer, input: Row): Unit = {
		val history = buffer.getAs[Double](0)
		val current = input.getAs[Double](0)
		buffer.update(0, history + current)
	}

	//7.将局部结果聚合到buffer1中
	override def merge(buffer1: MutableAggregationBuffer, buffer2: Row): Unit = {
		val result = buffer1.getAs[Double](0) + buffer2.getAs[Double](0)
		buffer1.update(0, result)
	}

	//8.返回最终结果 
	override def evaluate(buffer: Row): Any = {
		buffer.getAs[Double](0)
	}
}
</code></pre>
<ul>
<li>spark 代码</li>
</ul>
<pre><code class="language-java">// 自定义聚合函数（untyped）
var df = List(
	(1, &quot;zs&quot;, true, 1, 15000),
	(2, &quot;ls&quot;, false, 2, 18000),
	(3, &quot;ww&quot;, false, 2, 14000),
	(4, &quot;zl&quot;, false, 1, 18000),
	(6, &quot;zl1&quot;, true, 1, 18000),
	(5, &quot;win7&quot;, false, 1, 16000)
).toDF(&quot;id&quot;, &quot;name&quot;, &quot;sex&quot;, &quot;dept&quot;, &quot;salary&quot;)
df.createTempView(&quot;t_employee&quot;)

spark.udf
	.register(&quot;customSum&quot;, new CustomSum)
val sql =
	&quot;&quot;&quot;
	  |select dept,customSum(salary)
	  |from t_employee
	  |group by dept
	&quot;&quot;&quot;.stripMargin
spark.sql(sql)
	.show()
</code></pre>
<pre><code>+----+---------------------------------+
|dept|customsum(CAST(salary AS DOUBLE))|
+----+---------------------------------+
|   1|                          67000.0|
|   2|                          32000.0|
+----+---------------------------------+
</code></pre>
<h2 id="loadsave">Load/Save</h2>
<h3 id="paquet">Paquet</h3>
<ul>
<li>Parquet简介</li>
</ul>
<blockquote>
<p>Parquet是面向分析型业务的列式存储格式，由Twitter和Cloudera合作开发，2015年5月从Apache的孵化器里毕业成为Apache顶级项目<br>
<a href="http://parquet.apache.org/">http://parquet.apache.org/ </a></p>
</blockquote>
<pre><code class="language-java">// paquet
var df=List(
	(1,&quot;zs&quot;,true,1,15000),
	(2,&quot;ls&quot;,false,2,18000),
	(3,&quot;ww&quot;,false,2,14000),
	(4,&quot;zl&quot;,false,1,18000),
	(6,&quot;zl1&quot;,true,1,18000),
	(5,&quot;win7&quot;,false,1,16000)
).toDF(&quot;id&quot;,&quot;name&quot;,&quot;sex&quot;,&quot;dept&quot;,&quot;salary&quot;)

df.write
	.format(&quot;parquet&quot;)
	.save(&quot;file:////Users/mashikang/IdeaProjects/spark_sql/src/main/resources/parquet&quot;)
	
spark.read
	.parquet(&quot;file:////Users/mashikang/IdeaProjects/spark_sql/src/main/resources/parquet&quot;)
	.show()
</code></pre>
<pre><code>+---+----+-----+----+------+
| id|name|  sex|dept|salary|
+---+----+-----+----+------+
|  5|win7|false|   1| 16000|
|  6| zl1| true|   1| 18000|
|  4|  zl|false|   1| 18000|
|  3|  ww|false|   2| 14000|
|  1|  zs| true|   1| 15000|
|  2|  ls|false|   2| 18000|
+---+----+-----+----+------+
</code></pre>
<ul>
<li>存储文件样式<br>
<img src="https://img-blog.csdnimg.cn/20200612151145861.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L00yODM1OTIzMzg=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述" loading="lazy"></li>
</ul>
<h3 id="json">JSON</h3>
<pre><code class="language-java">// json
var df=List(
	(1,&quot;zs&quot;,true,1,15000),
	(2,&quot;ls&quot;,false,2,18000),
	(3,&quot;ww&quot;,false,2,14000),
	(4,&quot;zl&quot;,false,1,18000),
	(6,&quot;zl1&quot;,true,1,18000),
	(5,&quot;win7&quot;,false,1,16000)
).toDF(&quot;id&quot;,&quot;name&quot;,&quot;sex&quot;,&quot;dept&quot;,&quot;salary&quot;)

df.write
	.format(&quot;json&quot;)
	.save(&quot;file:///Users/mashikang/IdeaProjects/spark_sql/src/main/resources/json&quot;)
spark.read
	.json(&quot;file:///Users/mashikang/IdeaProjects/spark_sql/src/main/resources/json&quot;)
	.show()
</code></pre>
<pre><code>+----+---+----+------+-----+
|dept| id|name|salary|  sex|
+----+---+----+------+-----+
|   1|  5|win7| 16000|false|
|   2|  3|  ww| 14000|false|
|   1|  4|  zl| 18000|false|
|   2|  2|  ls| 18000|false|
|   1|  6| zl1| 18000| true|
|   1|  1|  zs| 15000| true|
+----+---+----+------+-----+
</code></pre>
<h3 id="orc存储压缩格式比较节省空间">ORC(存储压缩格式，比较节省空间)</h3>
<pre><code>// ORC
var df=List(
	(1,&quot;zs&quot;,true,1,15000),
	(2,&quot;ls&quot;,false,2,18000),
	(3,&quot;ww&quot;,false,2,14000),
	(4,&quot;zl&quot;,false,1,18000),
	(6,&quot;zl1&quot;,true,1,18000),
	(5,&quot;win7&quot;,false,1,16000)
).toDF(&quot;id&quot;,&quot;name&quot;,&quot;sex&quot;,&quot;dept&quot;,&quot;salary&quot;)

df.write
	.format(&quot;orc&quot;)
	.save(&quot;file:///Users/mashikang/IdeaProjects/spark_sql/src/main/resources/orc&quot;)
	
spark.read
	.orc(&quot;file:///Users/mashikang/IdeaProjects/spark_sql/src/main/resources/orc&quot;)
	.show()
</code></pre>
<pre><code>+---+----+-----+----+------+
| id|name|  sex|dept|salary|
+---+----+-----+----+------+
|  5|win7|false|   1| 16000|
|  4|  zl|false|   1| 18000|
|  3|  ww|false|   2| 14000|
|  6| zl1| true|   1| 18000|
|  1|  zs| true|   1| 15000|
|  2|  ls|false|   2| 18000|
+---+----+-----+----+------+
</code></pre>
<h3 id="csv">CSV</h3>
<pre><code>// CSV
var df=List(
	(1,&quot;zs&quot;,true,1,15000),
	(2,&quot;ls&quot;,false,2,18000),
	(3,&quot;ww&quot;,false,2,14000),
	(4,&quot;zl&quot;,false,1,18000),
	(6,&quot;zl1&quot;,true,1,18000),
	(5,&quot;win7&quot;,false,1,16000)
).toDF(&quot;id&quot;,&quot;name&quot;,&quot;sex&quot;,&quot;dept&quot;,&quot;salary&quot;)

df.write
	.format(&quot;csv&quot;) 
	.option(&quot;sep&quot;, &quot;,&quot;) 
	.option(&quot;inferSchema&quot;, &quot;true&quot;)
	.option(&quot;header&quot;, &quot;true&quot;) 
	.save(&quot;file:///Users/mashikang/IdeaProjects/spark_sql/src/main/resources/csv&quot;) 
spark.read 
	.option(&quot;sep&quot;, &quot;,&quot;) 
	.option(&quot;inferSchema&quot;, &quot;true&quot;) 
	.option(&quot;header&quot;, &quot;true&quot;) 
	.csv(&quot;file:///Users/mashikang/IdeaProjects/spark_sql/src/main/resources/csv&quot;) 
	.show()
</code></pre>
<pre><code>+---+----+-----+----+------+
| id|name|  sex|dept|salary|
+---+----+-----+----+------+
|  5|win7|false|   1| 16000|
|  4|  zl|false|   1| 18000|
|  3|  ww|false|   2| 14000|
|  2|  ls|false|   2| 18000|
|  6| zl1| true|   1| 18000|
|  1|  zs| true|   1| 15000|
+---+----+-----+----+------+
</code></pre>
<h3 id="jdbc">JDBC</h3>
<pre><code class="language-java">// JDBC
val usersDF = List(
	(1, &quot;张晓三&quot;, 1, 15000),
	(2, &quot;李晓四&quot;, 1, 18000),
	(3, &quot;王晓五&quot;, 1, 10000)
).toDF(&quot;id&quot;, &quot;name&quot;, &quot;dept&quot;, &quot;salary&quot;)

usersDF.write
	.format(&quot;jdbc&quot;)
	.mode(SaveMode.Overwrite)
	.option(&quot;user&quot;, &quot;root&quot;)
	.option(&quot;password&quot;, &quot;root&quot;)
	.option(&quot;url&quot;, &quot;jdbc:mysql://localhost:3306/test&quot;)
	.option(&quot;dbtable&quot;, &quot;t_user&quot;)
	.save()
	
val props = new Properties()
props.put(&quot;user&quot;, &quot;root&quot;)
props.put(&quot;password&quot;, &quot;root&quot;)

spark.read
	.jdbc(&quot;jdbc:mysql://localhost:3306/test&quot;, &quot;t_user&quot;, props)
	.show()
</code></pre>
<p>或者</p>
<pre><code class="language-java">val usersDF = List(
	(1, &quot;张晓三&quot;, 1, 15000),
	(2, &quot;李晓四&quot;, 1, 18000),
	(3, &quot;王晓五&quot;, 1, 10000)
).toDF(&quot;id&quot;, &quot;name&quot;, &quot;dept&quot;, &quot;salary&quot;)
usersDF.write
	.format(&quot;jdbc&quot;)
	.mode(SaveMode.Overwrite)
	.option(&quot;user&quot;, &quot;root&quot;)
	.option(&quot;password&quot;, &quot;root&quot;)
	.option(&quot;url&quot;, &quot;jdbc:mysql://localhost:3306/test&quot;)
	.option(&quot;dbtable&quot;, &quot;t_user&quot;)
	.save()

spark.read.format(&quot;jdbc&quot;)
			.option(&quot;user&quot;, &quot;root&quot;)
			.option(&quot;password&quot;, &quot;root&quot;)
			.option(&quot;url&quot;, &quot;jdbc:mysql://localhost:3306/test&quot;)
			.option(&quot;dbtable&quot;, &quot;t_user&quot;)
			.load()
			.show()
</code></pre>
<h3 id="dataframe转为rdd">DataFrame转为RDD</h3>
<pre><code class="language-java">val usersDF = List(
	(1, &quot;张晓三&quot;, 1, 15000.0),
	(2, &quot;李晓四&quot;, 1, 18000.0),
	(3, &quot;王晓五&quot;, 1, 10000.0)
).toDF(&quot;id&quot;, &quot;name&quot;, &quot;dept&quot;, &quot;salary&quot;)
usersDF.rdd.foreachPartition(its =&gt; {
	its.foreach(row =&gt; {
		val id = row.getAs[Int](&quot;id&quot;)
		val name = row.getAs[String](&quot;name&quot;)
		val salary = row.getAs[Double](&quot;salary&quot;)
		println(s&quot;$id,$name,$salary&quot;)
	})
})
</code></pre>
<pre><code>2,李晓四,18000.0
3,王晓五,10000.0
1,张晓三,15000.0
</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Spark入门(六)——最全的Saprk SQL算子介绍与使用(上)]]></title>
        <id>https://mask0407.github.io/spark05/</id>
        <link href="https://mask0407.github.io/spark05/">
        </link>
        <updated>2020-06-29T03:15:21.000Z</updated>
        <content type="html"><![CDATA[<p><ul class="markdownIt-TOC">
<li>
<ul>
<li><a href="#datasets-dataframes%E7%AE%80%E4%BB%8B">Datasets &amp; DataFrames简介</a>
<ul>
<li><a href="#%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8">快速入门</a></li>
</ul>
</li>
<li><a href="#dataset-dataframe%E5%AE%9E%E6%88%98">Dataset &amp; DataFrame实战</a>
<ul>
<li><a href="#dataset-create">Dataset create</a>
<ul>
<li><a href="#case-class">case-class</a></li>
<li><a href="#tuple%E5%85%83%E7%BB%84">Tuple(元组)</a></li>
<li><a href="#json%E6%95%B0%E6%8D%AE">json数据</a></li>
<li><a href="#rdd">RDD</a></li>
</ul>
</li>
<li><a href="#dataframe-create">Dataframe create</a>
<ul>
<li><a href="#json%E6%96%87%E4%BB%B6">json文件</a></li>
<li><a href="#case-class-2">case-class</a></li>
<li><a href="#tuple%E5%85%83%E7%BB%84-2">Tuple(元组)</a></li>
<li><a href="#rdd%E8%BD%AC%E6%8D%A2">RDD转换</a></li>
</ul>
</li>
<li><a href="#dataframe-operationsuntypeddataframe%E6%97%A0%E7%B1%BB%E5%9E%8B%E6%93%8D%E4%BD%9C">DataFrame Operations（Untyped）DataFrame无类型操作</a>
<ul>
<li><a href="#printschema-%E6%89%93%E5%8D%B0dataframe%E7%9A%84%E8%A1%A8%E7%BB%93%E6%9E%84%E8%A1%A8%E5%A4%B4">printSchema 打印Dataframe的表结构（表头）</a></li>
<li><a href="#show">show</a></li>
<li><a href="#select">select</a></li>
<li><a href="#selectexpr">selectExpr</a></li>
<li><a href="#withcolumn">withColumn</a></li>
<li><a href="#withcolumnrenamed">withColumnRenamed</a></li>
<li><a href="#drop">drop</a></li>
<li><a href="#dropduplicates">dropDuplicates</a></li>
<li><a href="#orderbysort">orderBy|sort</a></li>
<li><a href="#groupby">groupBy</a></li>
<li><a href="#agg">agg</a></li>
<li><a href="#limit">limit</a></li>
<li><a href="#where">where</a></li>
<li><a href="#pivot%E8%A1%8C%E8%BD%AC%E5%88%97">pivot（行转列）</a></li>
<li><a href="#na%E6%9B%BF%E6%8D%A2%E5%BD%93%E5%89%8D%E4%B8%BAnull%E7%9A%84%E5%80%BC">na(替换当前为null的值)</a></li>
<li><a href="#join">join</a></li>
<li><a href="#cube%E5%A4%9A%E7%BB%B4%E5%BA%A6">cube(多维度)</a></li>
</ul>
</li>
<li><a href="#dataset-oprations-strong-typed-%E6%95%B0%E6%8D%AE%E9%9B%86%E6%93%8D%E4%BD%9C-%E5%BC%BA%E7%B1%BB%E5%9E%8B">Dataset Oprations (Strong typed) 数据集操作-强类型</a></li>
</ul>
</li>
</ul>
</li>
</ul>
(Spark SQL)</p>
<p>Spark SQL是构建在RDD之上的ETL（Extract Transform Load）工具。SparkSQL在RDD之上抽象出来<code>Dataset/Dataframe</code> 这两个类提供了类似RDD的功能，也就意味用户可以使用map、faltMap、filter等高阶算子，同时也通过了基于列的命名查询，也就是说Dataset/DataFrame提供了两套操作数据的API，这些API可以给Saprk引擎要提供更多信息，系统可可以根据这些信息对计算实现一定的优化。目前Spark SQL提供了两种交互方式</p>
<ol>
<li><code>SQL 脚本</code></li>
<li><code>Dataset API</code>(strong-typed类型、untyped类型操作)</li>
</ol>
<h2 id="datasets-dataframes简介">Datasets &amp; DataFrames简介</h2>
<p>Dataset是一个分布式数据集，Dataset是在spark-1.6提出新的API，该API构建在RDD（strong type，使用lambda表达式）之上同时可以借助于Spark SQL对执行引擎的优点，使得使用Dateset执行一些数据的转换比直接使用RDD算子功能和性能都有所提升。因此我们可以认为Dateset就是一个加强版本的RDD。 Dataset除了可以使用JVM中数组|集合对象创建之外，也可以将任意的一个RDD转换为Dataset.<br>
<code>Python does not have the support for the Dataset API.</code></p>
<p>DataFrames 是Dataset的一种特殊情况。比如 Dataset中可以存储任意 对象类型的数据作为Dataset的元素。但是Dataframe的元素只有一种类型Row类型，这种基于Row查询和传统数据库中ResultSet操作极其相似。因为Row类型的数据表示Dataframe的一个元素，类似数据库中的一行，这些行中的元素可以通过下标或者column name访问。由于 Dateset是API的兼容或者支持度上不是多么的好，但是Dataframe在 API层面支持的Scala、Java、R、Python支持比较全面。</p>
<h3 id="快速入门">快速入门</h3>
<ul>
<li>引入依赖</li>
</ul>
<pre><code class="language-xml">&lt;dependency&gt; 
	&lt;groupId&gt;org.apache.spark&lt;/groupId&gt; 
	&lt;artifactId&gt;spark-core_2.11&lt;/artifactId&gt;
    &lt;version&gt;2.4.3&lt;/version&gt; 
&lt;/dependency&gt; 
&lt;dependency&gt; 
	&lt;groupId&gt;org.apache.spark&lt;/groupId&gt; 
	&lt;artifactId&gt;spark-sql_2.11&lt;/artifactId&gt; 
	&lt;version&gt;2.4.3&lt;/version&gt; 
&lt;/dependency&gt;
</code></pre>
<ul>
<li>创建字符统计（untyped）</li>
</ul>
<pre><code class="language-java">def main(args: Array[String]): Unit = {
		//1.创建SparkSeesion
		val spark = SparkSession
			.builder()
			.appName(&quot;wordcount&quot;)
			.master(&quot;local[6]&quot;)
			.getOrCreate()

		//2.导入spark定义隐式增强|转换
		import spark.implicits._

		//3.创建dataset
		val lines = Array(&quot;this is a demo&quot;, &quot;hello spark&quot;)
		val wordRDD = spark.sparkContext
			.makeRDD(lines)
			.flatMap(_.split(&quot;\\s+&quot;))
			.map((_, 1))

		val ds: Dataset[(String, Int)] = wordRDD.toDS()
		
		//4.对Dataset执行sql算子操作
		ds.groupBy($&quot;_1&quot;)
			//无类型操作
			.sum(&quot;_2&quot;)
			.as(&quot;total&quot;)
			.withColumnRenamed(&quot;_1&quot;, &quot;word&quot;)
			.withColumnRenamed(&quot;sum(_2)&quot;, &quot;total&quot;)
			.show()
		//5.关闭spark
		spark.stop()
}
</code></pre>
<ul>
<li>创建字符统计（strong typed）</li>
</ul>
<pre><code class="language-java">def main(args: Array[String]): Unit = {
		//1.创建SparkSeesion 
		val spark = SparkSession
			.builder()
			.appName(&quot;wordcount&quot;)
			.master(&quot;local[6]&quot;)
			.getOrCreate()

		//2.导入spark定义隐式增强|转换

		import spark.implicits._

		//3.创建dataset 
		val lines = Array(&quot;this is a demo&quot;, &quot;hello spark&quot;)
		val wordRDD = spark.sparkContext
			.makeRDD(lines)
			.flatMap(_.split(&quot;\\s+&quot;))
			.map((_, 1))
		val ds: Dataset[(String, Int)] = wordRDD.toDS()

		//4.对Dataset执行sql算子操作 
		ds.groupByKey(t =&gt; t._1)
			.agg(typed.sum[(String, Int)](tuple =&gt; tuple._2).name(&quot;total&quot;))
			.show()

		//5.关闭spark 
		spark.stop()
}
</code></pre>
<h2 id="dataset-dataframe实战">Dataset &amp; DataFrame实战</h2>
<h3 id="dataset-create">Dataset create</h3>
<p>Dataset类似于RDD，不同的是Spark SQL有一套自己的序列化规范独立于Spark RDD（Java/Kryo序列化）之上称为Encoders。不同于SparkRDD序列化，由于Dataset支持无类型操作，用户无需获取操作的类型，操作仅仅是列名，因为Spark SQL在执行算子操作的时候可以省略反序列化的步骤，继而提升程序执行<br>
效率。</p>
<h4 id="case-class">case-class</h4>
<pre><code class="language-java">// case-class
case class Person(id: Int, name: String, age: Int, sex: Boolean)
</code></pre>
<pre><code class="language-java">/**
* case-class
*/
val person: Dataset[Person] = List(
   Person(1, &quot;zhangsan&quot;, 18, true),
   Person(2, &quot;lisi&quot;, 28, true)
)
   .toDS()
person.select($&quot;id&quot;, $&quot;name&quot;)
   .show()
</code></pre>
<blockquote>
<p>注：</p>
<ul>
<li>加入隐式转换 import spark.implicits._   该语句需要放在获取spark对象的语句之后</li>
<li>case class Person(id: Int, name: String, age: Int, sex: Boolean) 的定义需要放在方法的作用域之外（即<code>Scala/Java</code>的成员变量位置）</li>
<li>如果case类是java编写，则java bean需要实现 <code>Serializable</code>，以及增加<code>get、set</code>方法</li>
</ul>
</blockquote>
<p>结果:</p>
<pre><code>+---+--------+
| id|    name|
+---+--------+
|  1|zhangsan|
|  2|    lisi|
+---+--------+
</code></pre>
<h4 id="tuple元组">Tuple(元组)</h4>
<pre><code class="language-java">/**
 * tuple
 */
val person: Dataset[(Int, String, Int, Boolean)] = List(
	(1, &quot;zhangsan&quot;, 18, true),
	(2, &quot;lisi&quot;, 28, true)
)
	.toDS()
person.select($&quot;_1&quot;, $&quot;_2&quot;).show()
</code></pre>
<p>结果:</p>
<pre><code>+---+--------+
| _1|      _2|
+---+--------+
|  1|zhangsan|
|  2|    lisi|
+---+--------+
</code></pre>
<h4 id="json数据">json数据</h4>
<p>数据：</p>
<pre><code>{&quot;name&quot;:&quot;张三&quot;,&quot;age&quot;:18}
{&quot;name&quot;:&quot;李四&quot;,&quot;age&quot;:28}
{&quot;name&quot;:&quot;王五&quot;,&quot;age&quot;:38}
</code></pre>
<pre><code class="language-java">//数值默认是long类型
case class User(name: String, age: Long)
</code></pre>
<pre><code class="language-java">/**
 * json格式
 */
spark.read
	.json(&quot;file:///Users/mashikang/IdeaProjects/spark_sql/src/main/resources/json/&quot;)
	.as[User]
	.show()
</code></pre>
<p>结果:</p>
<pre><code>+---+----+
|age|name|
+---+----+
| 18|张三|
| 28|李四|
| 38|王五|
+---+----+
</code></pre>
<h4 id="rdd">RDD</h4>
<ul>
<li>Tuple(元组)</li>
</ul>
<pre><code class="language-java">/**
 * RDD 元组
 */
val userRDD = spark.sparkContext.makeRDD(List((1,&quot;张三&quot;,true,18,15000.0)))
userRDD.toDS().show()
</code></pre>
<p>结果:</p>
<pre><code>+---+----+----+---+-------+
| _1|  _2|  _3| _4|     _5|
+---+----+----+---+-------+
|  1|张三|true| 18|15000.0|
+---+----+----+---+-------+
</code></pre>
<ul>
<li>case-class</li>
</ul>
<pre><code class="language-java">// case-class
case class Person(id: Int, name: String, age: Int, sex: Boolean)
</code></pre>
<pre><code class="language-java">/**
 * RDD case-class
 */
val userRDD = spark.sparkContext.makeRDD(List(Person(1,&quot;张三&quot;,18,true)))
userRDD.toDS().show()
</code></pre>
<p>结果:</p>
<pre><code>+---+----+---+----+
| id|name|age| sex|
+---+----+---+----+
|  1|张三| 18|true|
+---+----+---+----+
</code></pre>
<h3 id="dataframe-create">Dataframe create</h3>
<p>DataFrame是一个命名列的数据集，用户可以直接操作 <code>column</code> 因此几乎所有DataFrame推荐操作都是<code>无类型操作</code> 。用户也可以把一个DataFrame看做是 <code>Dataset[Row]</code> 类型的数据集。</p>
<h4 id="json文件">json文件</h4>
<pre><code class="language-java">/**
 * json
 */
val dataFrame: DataFrame = spark.read
	.json(&quot;file:///Users/mashikang/IdeaProjects/spark_sql/src/main/resources/json/&quot;)
dataFrame.printSchema()
dataFrame.show()
</code></pre>
<p>结果:</p>
<pre><code>root
 |-- age: long (nullable = true)
 |-- name: string (nullable = true)

+---+----+
|age|name|
+---+----+
| 18|张三|
| 28|李四|
| 38|王五|
+---+----+
</code></pre>
<h4 id="case-class-2">case-class</h4>
<pre><code class="language-java">case class User(id:Int,name:String,sex:Boolean)
</code></pre>
<pre><code class="language-java">/**
 * case-class
 */
var userDF=List(User(1,&quot;张三&quot;,true))
	.toDF()
userDF.show()
</code></pre>
<p>结果:</p>
<pre><code>+---+----+----+
| id|name| sex|
+---+----+----+
|  1|张三|true|
+---+----+----+
</code></pre>
<h4 id="tuple元组-2">Tuple(元组)</h4>
<pre><code class="language-java">/**
 * Tuple
 */
var userDF=List((1,&quot;张三&quot;,true))
	.toDF(&quot;id&quot;,&quot;name&quot;,&quot;sex&quot;) 
userDF.show()
</code></pre>
<p>结果:</p>
<pre><code>+---+----+----+
| id|name| sex|
+---+----+----+
|  1|张三|true|
+---+----+----+
</code></pre>
<h4 id="rdd转换">RDD转换</h4>
<ul>
<li>Tuple</li>
</ul>
<pre><code class="language-java">/**
 * RDD Tuple
 */
var userDF = spark.sparkContext.parallelize(List((1, &quot;张三&quot;, true)))
	.toDF(&quot;id&quot;, &quot;name&quot;, &quot;sex&quot;) //可以指定列
userDF.show()
</code></pre>
<p>结果:</p>
<pre><code>+---+----+----+
| id|name| sex|
+---+----+----+
|  1|张三|true|
+---+----+----+
</code></pre>
<ul>
<li>case-class</li>
</ul>
<pre><code class="language-java">/**
 * RDD case-class
 */
var userDF = spark.sparkContext.parallelize(List(User(1, &quot;张三&quot;, true)))
	.toDF()
userDF.show()
</code></pre>
<p>结果:</p>
<pre><code>+---+----+----+
| id|name| sex|
+---+----+----+
|  1|张三|true|
+---+----+----+
</code></pre>
<ul>
<li>RDD[Row]类型转为DataFrame</li>
</ul>
<pre><code class="language-java">/**
 * RDD[Row]转换
 */
var userRDD: RDD[Row] = spark.sparkContext.parallelize(List(User(1, &quot;张三&quot;, true)))
	.map(u =&gt; Row(u.id, u.name, u.sex))
var schema = new StructType()
	.add(&quot;id&quot;, IntegerType)
	.add(&quot;name&quot;, StringType)
	.add(&quot;sex&quot;, BooleanType)
var userDF = spark.createDataFrame(userRDD, schema)
userDF.show()
</code></pre>
<p>结果:</p>
<pre><code>+---+----+----+
| id|name| sex|
+---+----+----+
|  1|张三|true|
+---+----+----+
</code></pre>
<ul>
<li>RDD case-class</li>
</ul>
<pre><code class="language-java">/**
 * RDD case-class
 */
var userRDD: RDD[User] = spark.sparkContext
	.makeRDD(List(User(1, &quot;张三&quot;, true)))
var userDF = spark.createDataFrame(userRDD)
userDF.show()
</code></pre>
<p>结果:</p>
<pre><code>+---+----+----+
| id|name| sex|
+---+----+----+
|  1|张三|true|
+---+----+----+
</code></pre>
<ul>
<li>RDD Tuple</li>
</ul>
<pre><code class="language-java">/**
 * RDD Tuple
 */
var userRDD:RDD[(Int,String,Boolean)]=spark.sparkContext
	.makeRDD(List((1,&quot;张三&quot;,true)))
var userDF=spark.createDataFrame(userRDD)
userDF.show()
</code></pre>
<p>结果:</p>
<pre><code>+---+----+----+
| _1|  _2|  _3|
+---+----+----+
|  1|张三|true|
+---+----+----+
</code></pre>
<h3 id="dataframe-operationsuntypeddataframe无类型操作">DataFrame Operations（Untyped）DataFrame无类型操作</h3>
<h4 id="printschema-打印dataframe的表结构表头">printSchema 打印Dataframe的表结构（表头）</h4>
<pre><code class="language-java">var df = List((1, &quot;张三&quot;, true))
	.toDF(&quot;id&quot;, &quot;name&quot;, &quot;sex&quot;)
df.printSchema()
</code></pre>
<pre><code>root
 |-- id: integer (nullable = false)
 |-- name: string (nullable = true)
 |-- sex: boolean (nullable = false)
</code></pre>
<h4 id="show">show</h4>
<pre><code class="language-java">var df = List(
	(1, &quot;zs&quot;, true, 1, 15000),
	(2, &quot;ls&quot;, false, 1, 15000)
)
	.toDF(&quot;id&quot;, &quot;name&quot;, &quot;sex&quot;, &quot;dept&quot;, &quot;salary&quot;)
	
df.select($&quot;id&quot;, $&quot;name&quot;, $&quot;salary&quot;)
	.show()
</code></pre>
<pre><code>+---+----+------+
| id|name|salary|
+---+----+------+
|  1|  zs| 15000|
|  2|  ls| 15000|
+---+----+------+
</code></pre>
<h4 id="select">select</h4>
<pre><code class="language-java">var df = List(
	(1, &quot;zs&quot;, true, 1, 15000),
	(2, &quot;ls&quot;, false, 1, 18000)
)
	.toDF(&quot;id&quot;, &quot;name&quot;, &quot;sex&quot;, &quot;dept&quot;, &quot;salary&quot;)
df.select($&quot;id&quot;,$&quot;name&quot;,$&quot;sex&quot;,$&quot;dept&quot;,$&quot;salary&quot; * 12 as &quot;annual_salary&quot;)
	.show()
</code></pre>
<pre><code>+---+----+-----+----+-------------+
| id|name|  sex|dept|annual_salary|
+---+----+-----+----+-------------+
|  1|  zs| true|   1|       180000|
|  2|  ls|false|   1|       216000|
+---+----+-----+----+-------------+
</code></pre>
<h4 id="selectexpr">selectExpr</h4>
<pre><code class="language-java">var df = List(
	(1, &quot;zs&quot;, true, 1, 15000),
	(2, &quot;ls&quot;, false, 1, 18000)
)
	.toDF(&quot;id&quot;, &quot;name&quot;, &quot;sex&quot;, &quot;dept&quot;, &quot;salary&quot;)
//等价 df.select($&quot;id&quot;,$&quot;name&quot;,$&quot;sex&quot;,$&quot;dept&quot;,$&quot;salary&quot; * 12 as &quot;annual_salary&quot;)
df.selectExpr(&quot;id&quot;, &quot;name&quot;, &quot;sex&quot;, &quot;dept&quot;, &quot;salary * 12 as annual_salary&quot;)
	.show()
</code></pre>
<pre><code>+---+----+-----+----+-------------+
| id|name|  sex|dept|annual_salary|
+---+----+-----+----+-------------+
|  1|  zs| true|   1|       180000|
|  2|  ls|false|   1|       216000|
+---+----+-----+----+-------------+
</code></pre>
<h4 id="withcolumn">withColumn</h4>
<pre><code class="language-java">var df = List(
	(1, &quot;zs&quot;, true, 1, 15000),
	(2, &quot;ls&quot;, false, 1, 18000)
)
	.toDF(&quot;id&quot;, &quot;name&quot;, &quot;sex&quot;, &quot;dept&quot;, &quot;salary&quot;)
df.select($&quot;id&quot;, $&quot;name&quot;, $&quot;sex&quot;, $&quot;dept&quot;, $&quot;salary&quot;)
	.withColumn(&quot;annual_salary&quot;, $&quot;salary&quot; * 12)
	.show()
</code></pre>
<pre><code>+---+----+-----+----+------+-------------+
| id|name|  sex|dept|salary|annual_salary|
+---+----+-----+----+------+-------------+
|  1|  zs| true|   1| 15000|       180000|
|  2|  ls|false|   1| 18000|       216000|
+---+----+-----+----+------+-------------+
</code></pre>
<h4 id="withcolumnrenamed">withColumnRenamed</h4>
<pre><code class="language-java">var df = List(
	(1, &quot;zs&quot;, true, 1, 15000),
	(2, &quot;ls&quot;, false, 1, 18000)
)
	.toDF(&quot;id&quot;, &quot;name&quot;, &quot;sex&quot;, &quot;dept&quot;, &quot;salary&quot;)

df.select($&quot;id&quot;, $&quot;name&quot;, $&quot;sex&quot;, $&quot;dept&quot;, $&quot;salary&quot;)
	.withColumn(&quot;annula_salary&quot;, $&quot;salary&quot; * 12)
	.withColumnRenamed(&quot;dept&quot;, &quot;department&quot;)
	.withColumnRenamed(&quot;name&quot;, &quot;username&quot;)
	.show()
</code></pre>
<pre><code>+---+--------+-----+----------+------+-------------+
| id|username|  sex|department|salary|annula_salary|
+---+--------+-----+----------+------+-------------+
|  1|      zs| true|         1| 15000|       180000|
|  2|      ls|false|         1| 18000|       216000|
+---+--------+-----+----------+------+-------------+
</code></pre>
<h4 id="drop">drop</h4>
<pre><code class="language-java">var df = List(
	(1, &quot;zs&quot;, true, 1, 15000),
	(2, &quot;ls&quot;, false, 1, 18000)
)
	.toDF(&quot;id&quot;, &quot;name&quot;, &quot;sex&quot;, &quot;dept&quot;, &quot;salary&quot;)

df.select($&quot;id&quot;,$&quot;name&quot;,$&quot;sex&quot;,$&quot;dept&quot;,$&quot;salary&quot;)
	.withColumn(&quot;annula_salary&quot;,$&quot;salary&quot; * 12)
	.withColumnRenamed(&quot;dept&quot;,&quot;department&quot;)
	.withColumnRenamed(&quot;name&quot;,&quot;username&quot;)
	.drop(&quot;sex&quot;)
	.show()
</code></pre>
<pre><code>+---+--------+----------+------+-------------+
| id|username|department|salary|annula_salary|
+---+--------+----------+------+-------------+
|  1|      zs|         1| 15000|       180000|
|  2|      ls|         1| 18000|       216000|
+---+--------+----------+------+-------------+
</code></pre>
<h4 id="dropduplicates">dropDuplicates</h4>
<pre><code class="language-java">var df = List(
	(1, &quot;zs&quot;, true, 1, 15000),
	(2, &quot;ls&quot;, false, 1, 18000),
	(3, &quot;ww&quot;, false, 1, 19000),
	(4, &quot;zl&quot;, false, 1, 18000)
)
	.toDF(&quot;id&quot;, &quot;name&quot;, &quot;sex&quot;, &quot;dept&quot;, &quot;salary&quot;)
df.select($&quot;id&quot;, $&quot;name&quot;, $&quot;sex&quot;, $&quot;dept&quot;, $&quot;salary&quot;)
	.dropDuplicates(&quot;sex&quot;, &quot;salary&quot;)
	.show()
</code></pre>
<pre><code>+---+----+-----+----+------+
| id|name|  sex|dept|salary|
+---+----+-----+----+------+
|  3|  ww|false|   1| 19000|
|  1|  zs| true|   1| 15000|
|  2|  ls|false|   1| 18000|
+---+----+-----+----+------+
</code></pre>
<h4 id="orderbysort">orderBy|sort</h4>
<pre><code class="language-java">var df = List(
	(1, &quot;zs&quot;, true, 1, 15000),
	(2, &quot;ls&quot;, false, 2, 18000),
	(3, &quot;ww&quot;, false, 2, 14000),
	(4, &quot;zl&quot;, false, 1, 18000),
	(5, &quot;zl&quot;, false, 1, 16000)
)
	.toDF(&quot;id&quot;, &quot;name&quot;, &quot;sex&quot;, &quot;dept&quot;, &quot;salary&quot;)
	
df.select($&quot;id&quot;, $&quot;name&quot;, $&quot;sex&quot;, $&quot;dept&quot;, $&quot;salary&quot;)
	.orderBy($&quot;salary&quot; desc, $&quot;id&quot; asc)
	//.sort($&quot;salary&quot; desc,$&quot;id&quot; asc)
	.show()
</code></pre>
<pre><code>+---+----+-----+----+------+
| id|name|  sex|dept|salary|
+---+----+-----+----+------+
|  2|  ls|false|   2| 18000|
|  4|  zl|false|   1| 18000|
|  5|  zl|false|   1| 16000|
|  1|  zs| true|   1| 15000|
|  3|  ww|false|   2| 14000|
+---+----+-----+----+------+
</code></pre>
<h4 id="groupby">groupBy</h4>
<pre><code class="language-java">var df = List(
	(1, &quot;zs&quot;, true, 1, 15000),
	(2, &quot;ls&quot;, false, 2, 18000),
	(3, &quot;ww&quot;, false, 2, 14000),
	(4, &quot;zl&quot;, false, 1, 18000),
	(5, &quot;zl&quot;, false, 1, 16000)
)
	.toDF(&quot;id&quot;, &quot;name&quot;, &quot;sex&quot;, &quot;dept&quot;, &quot;salary&quot;)
	
df.select($&quot;id&quot;, $&quot;name&quot;, $&quot;sex&quot;, $&quot;dept&quot;, $&quot;salary&quot;)
	.groupBy($&quot;dept&quot;)
	.max(&quot;salary&quot;)
	.show()
</code></pre>
<pre><code>+----+-----------+
|dept|max(salary)|
+----+-----------+
|   1|      18000|
|   2|      18000|
+----+-----------+
</code></pre>
<blockquote>
<p>类似的算子还有 max、min、avg|mean、sum、count</p>
</blockquote>
<h4 id="agg">agg</h4>
<pre><code class="language-java">var df = List(
	(1, &quot;zs&quot;, true, 1, 15000),
	(2, &quot;ls&quot;, false, 2, 18000),
	(3, &quot;ww&quot;, false, 2, 14000),
	(4, &quot;zl&quot;, false, 1, 18000),
	(5, &quot;zl&quot;, false, 1, 16000)
)
	.toDF(&quot;id&quot;, &quot;name&quot;, &quot;sex&quot;, &quot;dept&quot;, &quot;salary&quot;)
	
import org.apache.spark.sql.functions._

df.select($&quot;id&quot;, $&quot;name&quot;, $&quot;sex&quot;, $&quot;dept&quot;, $&quot;salary&quot;)
	.groupBy($&quot;dept&quot;)
	.agg(max(&quot;salary&quot;) as &quot;max_salary&quot;, avg(&quot;salary&quot;) as &quot;avg_salary&quot;)
	.show()
</code></pre>
<pre><code>+----+----------+------------------+
|dept|max_salary|        avg_salary|
+----+----------+------------------+
|   1|     18000|16333.333333333334|
|   2|     18000|           16000.0|
+----+----------+------------------+
</code></pre>
<p>agg 还可以传递表达式</p>
<pre><code class="language-java">var df = List(
	(1, &quot;zs&quot;, true, 1, 15000),
	(2, &quot;ls&quot;, false, 2, 18000),
	(3, &quot;ww&quot;, false, 2, 14000),
	(4, &quot;zl&quot;, false, 1, 18000),
	(5, &quot;zl&quot;, false, 1, 16000)
)
	.toDF(&quot;id&quot;, &quot;name&quot;, &quot;sex&quot;, &quot;dept&quot;, &quot;salary&quot;)
	
import org.apache.spark.sql.functions._

df.select($&quot;id&quot;, $&quot;name&quot;, $&quot;sex&quot;, $&quot;dept&quot;, $&quot;salary&quot;)
	.groupBy($&quot;dept&quot;)
	.agg(Map(&quot;salary&quot;-&gt;&quot;max&quot;,&quot;id&quot;-&gt;&quot;count&quot;))
	.show()
</code></pre>
<pre><code>+----+-----------+---------+
|dept|max(salary)|count(id)|
+----+-----------+---------+
|   1|      18000|        3|
|   2|      18000|        2|
+----+-----------+---------+
</code></pre>
<h4 id="limit">limit</h4>
<pre><code class="language-java">var df = List(
	(1, &quot;zs&quot;, true, 1, 15000),
	(2, &quot;ls&quot;, false, 2, 18000),
	(3, &quot;ww&quot;, false, 2, 14000),
	(4, &quot;zl&quot;, false, 1, 18000),
	(5, &quot;zl&quot;, false, 1, 16000)
)
	.toDF(&quot;id&quot;, &quot;name&quot;, &quot;sex&quot;, &quot;dept&quot;, &quot;salary&quot;)

df.select($&quot;id&quot;, $&quot;name&quot;, $&quot;sex&quot;, $&quot;dept&quot;, $&quot;salary&quot;)
	.orderBy($&quot;id&quot; desc)
	.limit(4)
	.show()
</code></pre>
<pre><code>+---+----+-----+----+------+
| id|name|  sex|dept|salary|
+---+----+-----+----+------+
|  5|  zl|false|   1| 16000|
|  4|  zl|false|   1| 18000|
|  3|  ww|false|   2| 14000|
|  2|  ls|false|   2| 18000|
+---+----+-----+----+------+
</code></pre>
<h4 id="where">where</h4>
<pre><code class="language-java">var df = List(
	(1, &quot;zs&quot;, true, 1, 15000),
	(2, &quot;ls&quot;, false, 2, 18000),
	(3, &quot;ww&quot;, false, 2, 14000),
	(4, &quot;zl&quot;, false, 1, 18000),
	(5, &quot;win7&quot;, false, 1, 16000)
)
	.toDF(&quot;id&quot;, &quot;name&quot;, &quot;sex&quot;, &quot;dept&quot;, &quot;salary&quot;)
	
df.select($&quot;id&quot;, $&quot;name&quot;, $&quot;sex&quot;, $&quot;dept&quot;, $&quot;salary&quot;)
	//where(&quot;(name like '%s%' and salary &gt; 15000) or name = 'win7'&quot;) 
	.where(($&quot;name&quot; like &quot;%s%&quot; and $&quot;salary&quot; &gt; 15000) or $&quot;name&quot; === &quot;win7&quot;)
	.show()
</code></pre>
<pre><code>+---+----+-----+----+------+
| id|name|  sex|dept|salary|
+---+----+-----+----+------+
|  2|  ls|false|   2| 18000|
|  5|win7|false|   1| 16000|
+---+----+-----+----+------+
</code></pre>
<h4 id="pivot行转列">pivot（行转列）</h4>
<pre><code class="language-java">var scoreDF = List(
	(1, &quot;math&quot;, 85),
	(1, &quot;chinese&quot;, 80),
	(1, &quot;english&quot;, 90),
	(2, &quot;math&quot;, 90),
	(2, &quot;chinese&quot;, 80)
)
	.toDF(&quot;id&quot;, &quot;course&quot;, &quot;score&quot;)
	
import org.apache.spark.sql.functions._

//select id,max(case course when 'math' then score else 0 end )as math ,max(case course when 'chinese' then score else 0 end) as chinese from t_course group by id;
scoreDF.selectExpr(&quot;id&quot;, &quot;case course when 'math' then score else 0 end as math&quot;, &quot;case course when 'chinese' then score else 0 end as chinese&quot;, &quot;case course when 'english' then score else 0 end as english&quot;)
	.groupBy(&quot;id&quot;)
	.agg(max($&quot;math&quot;), max($&quot;chinese&quot;), max($&quot;english&quot;))
	.show()
</code></pre>
<pre><code>+---+---------+------------+------------+
| id|max(math)|max(chinese)|max(english)|
+---+---------+------------+------------+
|  1|       85|          80|          90|
|  2|       90|          80|           0|
+---+---------+------------+------------+
</code></pre>
<p>简易写法</p>
<pre><code class="language-java">var scoreRDD = List(
	(1, &quot;math&quot;, 85),
	(1, &quot;chinese&quot;, 80),
	(1, &quot;english&quot;, 90),
	(2, &quot;math&quot;, 90),
	(2, &quot;chinese&quot;, 80)
)
scoreRDD.toDF(&quot;id&quot;, &quot;course&quot;, &quot;score&quot;)
	.groupBy(&quot;id&quot;) 
	// 					行转列 				可选值
	.pivot(&quot;course&quot;, scoreRDD.map(t =&gt; t._2).distinct)
	.max(&quot;score&quot;)
	.show()
</code></pre>
<pre><code>+---+----+-------+-------+
| id|math|chinese|english|
+---+----+-------+-------+
|  1|  85|     80|     90|
|  2|  90|     80|   null|
+---+----+-------+-------+
</code></pre>
<h4 id="na替换当前为null的值">na(替换当前为null的值)</h4>
<pre><code class="language-java">var scoreRDD = List(
	(1, &quot;math&quot;, 85),
	(1, &quot;chinese&quot;, 80),
	(1, &quot;english&quot;, 90),
	(2, &quot;math&quot;, 90),
	(2, &quot;chinese&quot;, 80),
	(3, &quot;math&quot;, 100)
)

scoreRDD.toDF(&quot;id&quot;, &quot;course&quot;, &quot;score&quot;)
	.groupBy(&quot;id&quot;)
	// 					行转列 				可选值
	.pivot(&quot;course&quot;, scoreRDD.map(t =&gt; t._2).distinct)
	.max(&quot;score&quot;)
	.na.fill(Map(&quot;english&quot; -&gt; -1, &quot;chinese&quot; -&gt; 0))
	.show()
</code></pre>
<pre><code>+---+----+-------+-------+
| id|math|chinese|english|
+---+----+-------+-------+
|  1|  85|     80|     90|
|  3| 100|      0|     -1|
|  2|  90|     80|     -1|
+---+----+-------+-------+
</code></pre>
<h4 id="join">join</h4>
<pre><code class="language-java">case class UserCost(id: Int, category: String, totalCost: Double)
case class User(id: Int, name: String, sex: Boolean, age: Int, salary: Double)
</code></pre>
<pre><code class="language-java">var userCostDF = spark.sparkContext
	.parallelize(List(
		UserCost(1, &quot;电脑配件&quot;, 100),
		UserCost(1, &quot;母婴用品&quot;, 100),
		UserCost(1, &quot;生活用品&quot;, 100),
		UserCost(2, &quot;居家美食&quot;, 79),
		UserCost(2, &quot;消费电子&quot;, 80),
		UserCost(2, &quot;生活用品&quot;, 100)
	))
	.toDF()
	.withColumnRenamed(&quot;id&quot;, &quot;uid&quot;)
val categories = userCostDF
	.select(&quot;category&quot;)
	.as[(String)]
	.rdd
	.distinct
	.collect()
var userDF = spark.sparkContext
	.parallelize(List(
		User(1, &quot;张晓三&quot;, true, 18, 15000),
		User(2, &quot;李晓四&quot;, true, 18, 18000),
		User(3, &quot;王晓五&quot;, false, 18, 10000)
	))
	.toDF()
userDF.join(userCostDF, $&quot;id&quot; === $&quot;uid&quot;, &quot;left_outer&quot;)
	.drop(&quot;uid&quot;)
	.groupBy(&quot;id&quot;, &quot;name&quot;)
	.pivot($&quot;category&quot;, categories)
	.sum(&quot;totalCost&quot;)
	.na.fill(0.0)
	.show()
</code></pre>
<pre><code>+---+------+--------+--------+--------+--------+--------+
| id|  name| 电脑配件| 生活用品| 母婴用品| 居家美食|  消费电子|
+---+------+--------+--------+--------+--------+--------+
|  1|张晓三|   100.0|   100.0|   100.0|     0.0|     0.0|
|  3|王晓五|     0.0|     0.0|     0.0|     0.0|     0.0|
|  2|李晓四|     0.0|   100.0|     0.0|    79.0|    80.0|
+---+------+--------+--------+--------+--------+--------+
</code></pre>
<h4 id="cube多维度">cube(多维度)</h4>
<pre><code class="language-java">import org.apache.spark.sql.functions._
List(
	(110, 50, 80, 80),
	(120, 60, 95, 75),
	(120, 50, 96, 70)
)
	.toDF(&quot;height&quot;, &quot;weight&quot;, &quot;IQ&quot;, &quot;EQ&quot;)
	.cube($&quot;height&quot;, $&quot;weight&quot;)
	.agg(avg(&quot;IQ&quot;), avg(&quot;EQ&quot;))
	.show()
</code></pre>
<pre><code>+------+------+-----------------+-------+
|height|weight|          avg(IQ)|avg(EQ)|
+------+------+-----------------+-------+
|   110|    50|             80.0|   80.0|
|   120|  null|             95.5|   72.5|
|   120|    60|             95.0|   75.0|
|  null|    60|             95.0|   75.0|
|  null|  null|90.33333333333333|   75.0|
|   120|    50|             96.0|   70.0|
|   110|  null|             80.0|   80.0|
|  null|    50|             88.0|   75.0|
+------+------+-----------------+-------+
</code></pre>
<h3 id="dataset-oprations-strong-typed-数据集操作-强类型">Dataset Oprations (Strong typed) 数据集操作-强类型</h3>
<p>由于强类型操作都是基于类型操作，Spark SQL的操作都是推荐使用Dataframe基于列操作，因此一般情况下不推荐使用。</p>
<pre><code class="language-java">val lines = Array(&quot;this is a demo&quot;, &quot;hello spark&quot;)
val wordRDD = spark.sparkContext.makeRDD(lines)
	.flatMap(_.split(&quot;\\s+&quot;))
	.map((_, 1))
	
import org.apache.spark.sql.expressions.scalalang.typed

val ds: Dataset[(String, Int)] = wordRDD.toDS()
ds.groupByKey(t =&gt; t._1)
	.agg(typed.sum[(String, Int)](tuple =&gt; tuple._2).name(&quot;total&quot;))
	.filter(tuple =&gt; tuple._1.contains(&quot;o&quot;))
	.show()
</code></pre>
<pre><code>+-----+-----+
|value|total|
+-----+-----+
|hello|  1.0|
| demo|  1.0|
+-----+-----+
</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Spark入门(五)——Spark Streaming]]></title>
        <id>https://mask0407.github.io/spark04/</id>
        <link href="https://mask0407.github.io/spark04/">
        </link>
        <updated>2020-06-29T03:14:37.000Z</updated>
        <content type="html"><![CDATA[<p><ul class="markdownIt-TOC">
<li>
<ul>
<li><a href="#spark-streaming%E6%B5%81%E5%A4%84%E7%90%86">Spark Streaming(流处理)</a>
<ul>
<li><a href="#%E4%BB%80%E4%B9%88%E6%98%AF%E6%B5%81%E5%A4%84%E7%90%86">什么是流处理？</a></li>
<li><a href="#%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8">快速入门</a></li>
<li><a href="#%E6%A6%82%E5%BF%B5%E4%BB%8B%E7%BB%8D">概念介绍</a>
<ul>
<li><a href="#%E5%88%9D%E5%A7%8B%E5%8C%96-streamingcontext">初始化 StreamingContext</a></li>
<li><a href="#discretized-streams-dstreams">Discretized Streams (DStreams)</a></li>
</ul>
</li>
<li><a href="#inputstream-receivers">InputStream &amp; Receivers</a>
<ul>
<li><a href="#basic-sources">Basic Sources</a></li>
<li><a href="#queue-of-rdds-as-a-stream%E6%B5%8B%E8%AF%95">Queue of RDDs as a Stream(测试)</a></li>
<li><a href="#advance-source-kafka">Advance Source Kafka</a></li>
</ul>
</li>
<li><a href="#spark-stream-%E7%AE%97%E5%AD%90">Spark Stream 算子</a>
<ul>
<li><a href="#updatestatebykey">UpdateStateByKey</a></li>
</ul>
</li>
<li><a href="#%E9%87%8D%E6%95%85%E9%9A%9C%E4%B8%AD%E9%87%8D%E5%90%AF%E4%B8%AD%E7%8A%B6%E6%80%81%E6%81%A2%E5%A4%8D">重故障中|重启中状态恢复</a></li>
<li><a href="#%E7%AA%97%E5%8F%A3%E5%87%BD%E6%95%B0-window">窗口函数 - window</a></li>
<li><a href="#output-operations">Output Operations</a></li>
</ul>
</li>
</ul>
</li>
</ul>
(Spark Streaming（流处理）)</p>
<h2 id="spark-streaming流处理">Spark Streaming(流处理)</h2>
<h3 id="什么是流处理">什么是流处理？</h3>
<p>一般流式计算会与批量计算相比较。在流式计算模型中，输入是持续的，可以认为在时间上是无界的，也就意味着，永远拿不到全量数据去做计算。同时，计算结果是持续输出的，也即计算结果在时间上也是无界的。流式计算一般对实时性要求较高，同时一般是先定义目标计算，然后数据到来之后将计算逻辑应用于数据。同时为了提高计算效率，往往尽可能采用增量计算代替全量计算。批量处理模型中，一般先有全量数据集，然后定义计算逻辑，并将计算应用于全量数据。特点是全量计算，并且计算结果一次性全量输出。</p>
<figure data-type="image" tabindex="1"><img src="https://img-blog.csdnimg.cn/20200527123328230.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L00yODM1OTIzMzg=,size_16,color_FFFFFF,t_70" alt="" loading="lazy"></figure>
<p>Spark Streaming是核心Spark API的扩展，可实现实时数据流的可扩展，高吞吐量，容错流处理。数据可以从许多来源（如Kafka，Flume，Kinesis或TCP套接字）中获取，并且可以使用以高级函数（如map，reduce，join和window）表示的复杂算法进行处理。最后，处理后的数据可以推送到文件系统，数据库和实时dashboards。<br>
<img src="https://img-blog.csdnimg.cn/20200527123413593.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L00yODM1OTIzMzg=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述" loading="lazy"><br>
在内部，它的工作原理如下。 Spark Streaming接收实时输入数据流并将数据分成批处理，然后由Spark引擎处理以批量生成最终结果流。<br>
<img src="https://img-blog.csdnimg.cn/20200527123436181.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L00yODM1OTIzMzg=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述" loading="lazy"><br>
Spark Streaming提供称为离散流或DStream的高级抽象，表示连续的数据流。DStream可以从来自Kafka，Flume和Kinesis等源的输入数据流创建，也可以通过在其他DStream上应用高级操作来创建。在内部DStream表示为一系列RDD。</p>
<blockquote>
<p>备注:Spark Streaming 因为底层使用批处理模拟流处理，因此在实时性上大打折扣，这就导致了Spark Streaming在流处理领域有者着先天的劣势。虽然Spark Streaming 在实时性上不如一些专业的流处理引擎(Storm/Flink)但是Spark Stream在使用吸取RDD设计经验，提供了比较友好的API算子，使得使用RDD做批处理的程序员可以平滑的过渡到流处理。</p>
<p>针对于Spark Streaming的微观的批处理问题，目前大数据处理领域又诞生了新秀<code>Flink</code>，该大数据处理引擎，在API易用性上和实时性上都有一定的兼顾，但是与spark最大的差异是Flink底层的处理引擎是流处理引擎，因此Flink天生就是流处理，但是Spark因为底层是批处理，导致了Spark Streaming在实时性上就没法和其他的专业流处理框架对比了。</p>
</blockquote>
<h3 id="快速入门">快速入门</h3>
<ul>
<li>pom.xml</li>
</ul>
<pre><code class="language-xml">&lt;dependency&gt;
    &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;
    &lt;artifactId&gt;spark-core_2.11&lt;/artifactId&gt;
    &lt;version&gt;2.4.3&lt;/version&gt;
&lt;/dependency&gt;
&lt;dependency&gt;
    &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;
    &lt;artifactId&gt;spark-streaming_2.11&lt;/artifactId&gt;
    &lt;version&gt;2.4.3&lt;/version&gt;
&lt;/dependency&gt;
</code></pre>
<ul>
<li>SparkStreamWordCounts</li>
</ul>
<pre><code class="language-java">val conf = new SparkConf().setMaster(&quot;local[2]&quot;).setAppName(&quot;NetworkWordCount&quot;)
    val ssc = new StreamingContext(conf, Seconds(5))
    ssc.sparkContext.setLogLevel(&quot;FATAL&quot;)//关闭日志打印

    ssc.socketTextStream(&quot;CentOS&quot;,9999)
      .flatMap(_.split(&quot; &quot;))
      .map((_,1))
      .reduceByKey(_+_)
      .print()

    ssc.start()
    ssc.awaitTermination()//等待系统发送指定关闭流计算
</code></pre>
<blockquote>
<p>需要注意：<code>[root@CentOS ~]# yum install -y nc</code>启动nc服务<code>[root@CentOS ~]# nc -lk 9999</code>,注意在调用改程序的时候，需要设置local[n],<code>n&gt;1</code></p>
</blockquote>
<h3 id="概念介绍">概念介绍</h3>
<p>通过上述案例的运行，现在我们来一起探讨一些流处理的概念。在处理流计算的时候，除去spark-core依赖以外我们还需要引入spark-streaming模块。要从Spark Streaming核心API中不存在的Kafka，Flume和Kinesis等源中提取数据，您必须将相应的工件spark-streaming-xxx_2.11添加到依赖项中。xxx例如Kafka</p>
<pre><code class="language-xml">&lt;dependency&gt;
    &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;
    &lt;artifactId&gt;spark-streaming-kafka-0-10_2.11&lt;/artifactId&gt;
    &lt;version&gt;2.4.3&lt;/version&gt;
&lt;/dependency&gt;
</code></pre>
<h4 id="初始化-streamingcontext">初始化 StreamingContext</h4>
<p>要初始化Spark Streaming程序，必须创建一个StreamingContext对象，它是所有Spark Streaming功能的主要入口点。</p>
<pre><code class="language-java">import org.apache.spark._
import org.apache.spark.streaming._

val conf = new SparkConf().setAppName(appName).setMaster(master)
val ssc = new StreamingContext(conf, Seconds(1))
</code></pre>
<p>appName参数是应用程序在集群UI上显示的名称。 master是Spark，YARN群集URL，或者是在本地模式下运行的特殊<code>local [*]</code>字符串。实际上，在群集上运行时，您不希望在程序中对master进行硬编码，而是使用spark-submit启动指定–master配置。但是，对于本地测试和单元测试，您可以传递<code>local [*]</code>以在进程中运行Spark Streaming（系统会自动检测本地系统的核的数目）。</p>
<p>请注意ssc会在内部创建一个SparkContext（所有Spark功能的起点），如果需要获取SparkContext对象用户可以调用ssc.sparkContext访问。例如用户使用SparkContext关闭日志。</p>
<pre><code class="language-java">val conf = new SparkConf()
	.setMaster(&quot;local[5]&quot;)
	.setAppName(&quot;wordCount&quot;)
val ssc = new StreamingContext(conf,Seconds(1))
//关闭其他日志
ssc.sparkContext.setLogLevel(&quot;FATAL&quot;)
</code></pre>
<p>必须根据应用程序的延迟要求和可用的群集资源设置批处理间隔。要使群集上运行的Spark Streaming应用程序保持稳定，系统应该能够以接收数据的速度处理数据。换句话说，批处理数据应该在生成时尽快处理。通过监视流式Web UI中的处理时间可以找到是否适用于应用程序，其中批处理时间应小于批处理间隔。</p>
<pre><code class="language-java">val conf = new SparkConf()
.setMaster(&quot;local[5]&quot;)
.setAppName(&quot;wordCount&quot;)
val sc = new SparkContext(conf)
val ssc = new StreamingContext(sc,Seconds(1))
</code></pre>
<p>当用户创建完StreamingContext对象之后，用户需要完成以下步骤</p>
<blockquote>
<ul>
<li>定义数据源，用于创建输入的 DStreams.</li>
<li>定义流计算算子，通过定义这些算子实现对DStream数据转换和输出</li>
<li>调用streamingContext.start()启动数据.</li>
<li>等待计算结束 (人工结束或者是错误) 调用 streamingContext.awaitTermination().</li>
<li>如果是人工结束，程序应当调用 streamingContext.stop()结束流计算.</li>
</ul>
</blockquote>
<p>重要因素需要谨记</p>
<ul>
<li>一旦流计算启动，无法再往计算流程中添加任何计算算子</li>
<li>一旦SparkContext对象被stop后，无法重启。</li>
<li>一个JVM系统中只能实例化一个StreamingContext对象。</li>
<li>SparkContext被stop()后，内部创建的SparkContext也会被stop.如果仅仅是想Stop StreamingContext, 可以设置stop() 中的可选参数 stopSparkContext=false即可.</li>
</ul>
<pre><code class="language-java">ssc.stop(stopSparkContext = false)
</code></pre>
<ul>
<li>一个SparkContext 可以重复使用并且创建多个StreamingContexts, 前提是上一个启动的StreamingContext 被停止了(但是并没有关闭 SparkContext对象) 。</li>
</ul>
<h4 id="discretized-streams-dstreams">Discretized Streams (DStreams)</h4>
<p>Discretized Stream或DStream是Spark Streaming提供的基本抽象。它表示连续的数据流，可以是从源接收的输入数据流，也可以是通过转换输入流生成的已处理数据流。在内部，DStream由一系列连续的RDD表示，这是Spark对不可变分布式数据集的抽象。DStream中的每个RDD都包含来自特定时间间隔的数据，如下图所示。<br>
<img src="https://img-blog.csdnimg.cn/20200527123819156.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L00yODM1OTIzMzg=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述" loading="lazy"><br>
应用于DStream的任何操作都转换为底层RDD上的操作。例如，在先前<code>Quick Start</code>示例中，flatMap操作应用于行DStream中的每个RDD以生成单词DStream的RDD。如下图所示。<br>
<img src="https://img-blog.csdnimg.cn/20200527123857900.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L00yODM1OTIzMzg=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述" loading="lazy"><br>
这些底层RDD转换由Spark引擎计算。 DStream操作隐藏了大部分细节，并为开发人员提供了更高级别的API以方便使用。</p>
<h3 id="inputstream-receivers">InputStream &amp; Receivers</h3>
<p>Input DStream 表示流计算的输入，Spark中默认提供了两类的InputStream：</p>
<ul>
<li>Baisc Source ：例如 filesystem、scoket</li>
<li>Advance Source：例如：Kafka、Flume等外围系统的数据。</li>
</ul>
<p>除<code>filesystem</code>以外，其他的Input DStream默认都会占用一个<code>Core</code>(计算资源)，在测试或者生产环境下，分配给计算应用的Core数目必须大于Receivers个数。（本质上除filesystem源以外，其他的输入都是Receiver抽象类的实现。）了例如<code>socketTextStream</code>底层封装了<code>SocketReceiver</code></p>
<h4 id="basic-sources">Basic Sources</h4>
<p>因为在<code>快速入门</code>案例中已经使用了socketTextStream，后续我们只测试一下<code>filesystem</code>对于从与HDFS API兼容的任何文件系统（即HDFS，S3，NFS等）上的文件读取数据，可以通过StreamingContext.fileStream [KeyClass，ValueClass，InputFormatClass]创建DStream。文件流不需要运行Receiver，因此不需要为接收文件数据分配任何core。对于简单的文本文件，最简单的方法是<code>StreamingContext.textFileStream（dataDirectory）</code></p>
<pre><code class="language-java"> val conf = new SparkConf().setMaster(&quot;local[2]&quot;).setAppName(&quot;FileSystemWordCount&quot;)
    val ssc = new StreamingContext(conf, Seconds(5))
    ssc.sparkContext.setLogLevel(&quot;FATAL&quot;)//关闭日志打印

    val lines = ssc.textFileStream(&quot;hdfs://CentOS:9000/demo/words&quot;)

    lines.flatMap(_.split(&quot; &quot;))
      .map((_,1))
      .reduceByKey(_+_)
      .print()

    ssc.start()
    ssc.awaitTermination()
</code></pre>
<p>在HDFS上创建目录</p>
<pre><code class="language-shell">[root@CentOS ~]# hdfs dfs -mkdir -p /demo/words
[root@CentOS ~]# hdfs dfs -put install.log /demo/words
</code></pre>
<h4 id="queue-of-rdds-as-a-stream测试">Queue of RDDs as a Stream(测试)</h4>
<p>为了使用测试数据测试Spark Streaming应用程序，还可以使用streamingContext.queueStream（queueOfRDDs）基于RDD队列创建DStream。推入队列的每个RDD将被视为DStream中的一批数据，并像流一样处理。</p>
<pre><code class="language-java">val conf = new SparkConf().setMaster(&quot;local[2]&quot;).setAppName(&quot;FileSystemWordCount&quot;)
val ssc = new StreamingContext(conf, Seconds(1))
ssc.sparkContext.setLogLevel(&quot;FATAL&quot;)//关闭日志打印

val queue=new mutable.Queue[RDD[String]]();
val lines = ssc.queueStream(queue)

lines.flatMap(_.split(&quot; &quot;))
.map((_,1))
.reduceByKey(_+_)
.print()

ssc.start()
for(i &lt;- 1 to 30){
    queue += ssc.sparkContext.makeRDD(List(&quot;this is a demo&quot;,&quot;hello how are you&quot;))
    Thread.sleep(1000)
}
ssc.stop()
</code></pre>
<h4 id="advance-source-kafka">Advance Source Kafka</h4>
<ul>
<li>pom.xml</li>
</ul>
<pre><code class="language-xml">&lt;dependency&gt;
    &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;
    &lt;artifactId&gt;spark-streaming-kafka-0-10_2.11&lt;/artifactId&gt;
    &lt;version&gt;2.4.3&lt;/version&gt;
&lt;/dependency&gt;
</code></pre>
<ul>
<li>Kafka对接Spark Streaming</li>
</ul>
<pre><code class="language-java">val conf = new SparkConf().setMaster(&quot;local[2]&quot;).setAppName(&quot;FileSystemWordCount&quot;)
    val ssc = new StreamingContext(conf, Seconds(1))
    ssc.sparkContext.setLogLevel(&quot;FATAL&quot;)//关闭日志打印

    val kafkaParams = Map[String, Object](
      &quot;bootstrap.servers&quot; -&gt; &quot;CentOS:9092&quot;,
      &quot;key.deserializer&quot; -&gt; classOf[StringDeserializer],
      &quot;value.deserializer&quot; -&gt; classOf[StringDeserializer],
      &quot;group.id&quot; -&gt; &quot;group1&quot;,
      &quot;enable.auto.commit&quot; -&gt; (false: java.lang.Boolean)
    )

    KafkaUtils.createDirectStream(ssc,
      LocationStrategies.PreferConsistent,//设置加载数据位置策略，
      Subscribe[String,String](Array(&quot;topic01&quot;),kafkaParams))
        .map(record =&gt; record.value())
        .flatMap(_.split(&quot; &quot;))
        .map((_,1))
        .reduceByKey(_+_)
        .print()

    ssc.start()
    ssc.awaitTermination()
</code></pre>
<h3 id="spark-stream-算子">Spark Stream 算子</h3>
<ul>
<li><strong>transform</strong>(<em>func</em>)</li>
</ul>
<p>该算子可以将DStream的数据转变成RDD，用户操作流数据就像操作RDD感觉是一样的。</p>
<pre><code class="language-java"> val conf = new SparkConf().setMaster(&quot;local[2]&quot;).setAppName(&quot;KafkaStreamWordCount&quot;)
      val ssc = new StreamingContext(conf, Seconds(1))
      val kafkaParams = Map[String, Object](
        &quot;bootstrap.servers&quot; -&gt; &quot;CentOS:9092&quot;,
        &quot;key.deserializer&quot; -&gt; classOf[StringDeserializer],
        &quot;value.deserializer&quot; -&gt; classOf[StringDeserializer],
        &quot;group.id&quot; -&gt; &quot;group1&quot;,
        &quot;enable.auto.commit&quot; -&gt; (false: java.lang.Boolean)
      )
   
    val cacheRDD = ssc.sparkContext.makeRDD(List(&quot;001 zhangsan&quot;, &quot;002 lisi&quot;, &quot;003 zhaoliu&quot;))
      .map(item =&gt; (item.split(&quot;\\s+&quot;)(0), item.split(&quot;\\s+&quot;)(1)))
      .distinct()
      .cache()


      //001 apple
      KafkaUtils.createDirectStream(ssc,
        LocationStrategies.PreferConsistent,
        Subscribe[String,String](Array(&quot;topic01&quot;),kafkaParams))
        .map(record =&gt; record.value())
        .map(value=&gt;{
          val tokens = value.split(&quot;\\s+&quot;)
          (tokens(0),tokens(1))
        })
       .transform(rdd=&gt;{rdd.rightOuterJoin(cacheRDD)})
      .filter(t=&gt; {t._2._1!=None})
        .print()

    ssc.sparkContext.setLogLevel(&quot;FATAL&quot;)//关闭日志打印
    ssc.start()
    ssc.awaitTermination()
</code></pre>
<ul>
<li>
<h4 id="updatestatebykey">UpdateStateByKey</h4>
</li>
</ul>
<pre><code class="language-java">val conf = new SparkConf().setMaster(&quot;local[2]&quot;).setAppName(&quot;KafkaStreamWordCount&quot;)
val ssc = new StreamingContext(conf, Seconds(1))
ssc.checkpoint(&quot;hdfs://CentOS:9000/checkpoints&quot;)//在JVM启动参数中添加-D HADOOP_USER_NAME=root
def updateFun(newValues: Seq[Int], runningCount: Option[Int]): Option[Int] = {
   var total= newValues.sum+runningCount.getOrElse(0)
   Some(total)
}
ssc.socketTextStream(&quot;CentOS&quot;,9999)
    .flatMap(_.split(&quot;\\s+&quot;))
    .map((_,1))
    .updateStateByKey(updateFun)
    .print()

ssc.sparkContext.setLogLevel(&quot;FATAL&quot;)//关闭日志打印
ssc.start()
ssc.awaitTermination()
</code></pre>
<blockquote>
<p>因为UpdateStateByKey 算子每一次的输出都是全量输出，在做状态更新的时候代价较高，因此推荐大家使用<code>mapWithState</code></p>
</blockquote>
<ul>
<li>mapWithState</li>
</ul>
<pre><code class="language-java">val conf = new SparkConf().setMaster(&quot;local[2]&quot;).setAppName(&quot;KafkaStreamWordCount&quot;)
val ssc = new StreamingContext(conf, Seconds(1))
ssc.checkpoint(&quot;hdfs://CentOS:9000/checkpoints&quot;)
def updateFun(newValues: Seq[Int], runningCount: Option[Int]): Option[Int] = {
    var total= newValues.sum+runningCount.getOrElse(0)
    Some(total)
}
ssc.socketTextStream(&quot;CentOS&quot;,9999)
.flatMap(_.split(&quot;\\s+&quot;))
.map((_,1))
.mapWithState(StateSpec.function((k:String,v:Option[Int],state:State[Int])=&gt;{
    var total=0
    if(state.exists()){
        total=state.getOption().getOrElse(0)
    }
    total += v.getOrElse(1)
    state.update(total)//更新状态
    (k,total)
}))
.print()

ssc.sparkContext.setLogLevel(&quot;FATAL&quot;)//关闭日志打印
ssc.start()
ssc.awaitTermination()
</code></pre>
<h3 id="重故障中重启中状态恢复">重故障中|重启中状态恢复</h3>
<pre><code class="language-java">var checkpointPath=&quot;hdfs://CentOS:9000/checkpoints&quot;//设置检查点目录，一般将检查点目录存储在HDFS上
var ssc=StreamingContext.getOrCreate(checkpointPath,()=&gt;{//第一次启时候初始化，一旦书写完成后，无法进行修改！
    println(&quot;=======init=======&quot;)
    val conf = new SparkConf().setMaster(&quot;local[2]&quot;).setAppName(&quot;KafkaStreamWordCount&quot;)
    val ssc = new StreamingContext(conf, Seconds(5))
    ssc.checkpoint(checkpointPath)
    def updateFun(newValues: Seq[Int], runningCount: Option[Int]): Option[Int] = {
        var total= newValues.sum+runningCount.getOrElse(0)
        Some(total)
    }
    ssc.socketTextStream(&quot;CentOS&quot;,9999)
    .flatMap(_.split(&quot;\\s+&quot;))
    .map((_,1))
    .mapWithState(StateSpec.function((k:String,v:Option[Int],state:State[Int])=&gt;{
        var total=0
        if(state.exists()){
            total=state.getOption().getOrElse(0)
        }
        total += v.getOrElse(1)
        state.update(total)//更新状态
        (k,total)
    }))
    .checkpoint(Seconds(5))//设置状态持久化的频率，该频率不能高于 微批 拆分频率 ts&gt;=5s
    .print()
    ssc
})
ssc.sparkContext.setLogLevel(&quot;FATAL&quot;)//关闭日志打印
ssc.start()
ssc.awaitTermination()
</code></pre>
<h3 id="窗口函数-window">窗口函数 - window</h3>
<p>Spark Streaming还提供窗口计算，允许您在滑动数据窗口上应用转换。下图说明了此滑动窗口。<br>
<img src="https://img-blog.csdnimg.cn/20200527124949462.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L00yODM1OTIzMzg=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述" loading="lazy"><br>
以上描述了窗口长度是3个时间单位的<code>微批</code>,窗口的滑动间隔是2个时间单位的<code>微批</code>，注意：Spark的流处理中要求窗口的长度以及滑动间隔必须是微批的整数倍。</p>
<pre><code class="language-java">val conf = new SparkConf().setMaster(&quot;local[2]&quot;).setAppName(&quot;KafkaStreamWordCount&quot;)
    val ssc = new StreamingContext(conf, Seconds(1))

    ssc.socketTextStream(&quot;CentOS&quot;,9999)
      .flatMap(_.split(&quot;\\s+&quot;))
      .map((_,1))
      .reduceByKeyAndWindow((v1:Int,v2:Int)=&gt; v1+v2,Seconds(5),Seconds(5))
      .print()

    ssc.sparkContext.setLogLevel(&quot;FATAL&quot;)//关闭日志打印
    ssc.start()
    ssc.awaitTermination()
</code></pre>
<h3 id="output-operations">Output Operations</h3>
<ul>
<li><strong>foreachRDD</strong>(<em>func</em>)</li>
</ul>
<pre><code class="language-java">val conf = new SparkConf().setMaster(&quot;local[2]&quot;).setAppName(&quot;KafkaStreamWordCount&quot;)
val ssc = new StreamingContext(conf, Seconds(1))

ssc.socketTextStream(&quot;CentOS&quot;,9999)
  .flatMap(_.split(&quot;\\s+&quot;))
  .map((_,1))
  .window(Seconds(5),Seconds(5))
  .reduceByKey((v1:Int,v2:Int)=&gt; v1+v2)
  .foreachRDD(rdd=&gt;{
    rdd.foreachPartition(items=&gt;{
        var jedisPool=new JedisPool(&quot;CentOS&quot;,6379)
        val jedis = jedisPool.getResource
        val pipeline = jedis.pipelined()//jedis批处理

        val map = items.map(t=&gt;(t._1,t._2+&quot;&quot;)).toMap.asJava
        pipeline.hmset(&quot;wordcount&quot;,map)

         pipeline.sync()
        jedis.close()
    })
  })
  
ssc.sparkContext.setLogLevel(&quot;FATAL&quot;)//关闭日志打印
ssc.start()
ssc.awaitTermination()
</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Spark入门(四)——Spark RDD算子使用方法]]></title>
        <id>https://mask0407.github.io/spark03/</id>
        <link href="https://mask0407.github.io/spark03/">
        </link>
        <updated>2020-06-29T03:12:52.000Z</updated>
        <content type="html"><![CDATA[<h1 id="spark-rdd算子">Spark RDD算子</h1>
<h2 id="rdd算子实战">RDD算子实战</h2>
<h3 id="转换算子">转换算子</h3>
<h4 id="mapfunction">map(function)</h4>
<p>传入的集合元素进行RDD[T]转换 <code>def map(f: T =&gt; U): org.apache.spark.rdd.RDD[U]</code></p>
<pre><code class="language-java">scala&gt;  sc.parallelize(List(1,2,3,4,5),3).map(item =&gt; item*2+&quot; &quot; )
res1: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[2] at map at &lt;console&gt;:25

scala&gt;  sc.parallelize(List(1,2,3,4,5),3).map(item =&gt; item*2+&quot; &quot; ).collect
res2: Array[String] = Array(&quot;2 &quot;, &quot;4 &quot;, &quot;6 &quot;, &quot;8&quot;, &quot;10 &quot;)
</code></pre>
<h4 id="filterfunc"><strong>filter</strong>(<em>func</em>)</h4>
<p>将满足条件结果记录 <code>def filter(f: T=&gt; Boolean): org.apache.spark.rdd.RDD[T]</code></p>
<pre><code class="language-java">scala&gt;  sc.parallelize(List(1,2,3,4,5),3).filter(item=&gt; item%2==0).collect
res3: Array[Int] = Array(2, 4)
</code></pre>
<p><strong>flatMap</strong>(<em>func</em>)</p>
<p>将一个元素转换成元素的数组，然后对数组展开。<code>def flatMap[U](f: T=&gt; TraversableOnce[U]): org.apache.spark.rdd.RDD[U]</code></p>
<pre><code class="language-java">scala&gt;  sc.parallelize(List(&quot;ni hao&quot;,&quot;hello spark&quot;),3).flatMap(line=&gt;line.split(&quot;\\s+&quot;)).collect
res4: Array[String] = Array(ni, hao, hello, spark)
</code></pre>
<h4 id="mappartitionsfunc"><strong>mapPartitions</strong>(<em>func</em>)</h4>
<p>与map类似，但在RDD的每个分区（块）上单独运行，因此当在类型T的RDD上运行时，func必须是<code>Iterator &lt;T&gt; =&gt; Iterator &lt;U&gt;</code>类型</p>
<p><code>def mapPartitions[U](f: Iterator[Int] =&gt; Iterator[U],preservesPartitioning: Boolean): org.apache.spark.rdd.RDD[U]</code></p>
<pre><code class="language-java">scala&gt;  sc.parallelize(List(1,2,3,4,5),3).mapPartitions(items=&gt; for(i&lt;-items;if(i%2==0)) yield i*2 ).collect()
res7: Array[Int] = Array(4, 8)

</code></pre>
<h4 id="mappartitionswithindexfunc"><strong>mapPartitionsWithIndex</strong>(<em>func</em>)</h4>
<p>与mapPartitions类似，但也为func提供了表示分区索引的整数值，因此当在类型T的RDD上运行时，func必须是类型<code>（Int，Iterator &lt;T&gt;）=&gt; Iterator &lt;U&gt;</code>。</p>
<p><code>def mapPartitionsWithIndex[U](f: (Int, Iterator[T]) =&gt; Iterator[U],preservesPartitioning: Boolean): org.apache.spark.rdd.RDD[U]</code></p>
<pre><code class="language-java">scala&gt;  sc.parallelize(List(1,2,3,4,5),3).mapPartitionsWithIndex((p,items)=&gt; for(i&lt;-items) yield (p,i)).collect
res11: Array[(Int, Int)] = Array((0,1), (1,2), (1,3), (2,4), (2,5))
</code></pre>
<h4 id="samplewithreplacement-fraction-seed"><strong>sample</strong>(<em>withReplacement</em>, <em>fraction</em>, <em>seed</em>)</h4>
<p>对数据进行一定比例的采样，使用withReplacement参数控制是否允许重复采样。</p>
<pre><code class="language-java">def sample(withReplacement: Boolean,fraction: Double,seed: Long): org.apache.spark.rdd.RDD[T]

scala&gt;  sc.parallelize(List(1,2,3,4,5,6,7),3).sample(false,0.7,1L).collect
res13: Array[Int] = Array(1, 4, 6, 7)
</code></pre>
<h4 id="unionotherdataset"><strong>union</strong>(<em>otherDataset</em>)</h4>
<p>返回一个新数据集，其中包含源数据集和参数中元素的并集。</p>
<p><code>def union(other: org.apache.spark.rdd.RDD[T]): org.apache.spark.rdd.RDD[T]</code></p>
<pre><code class="language-java">scala&gt; var rdd1=sc.parallelize(Array((&quot;张三&quot;,1000),(&quot;李四&quot;,100),(&quot;赵六&quot;,300)))
scala&gt; var rdd2=sc.parallelize(Array((&quot;张三&quot;,1000),(&quot;王五&quot;,100),(&quot;温七&quot;,300)))
scala&gt; rdd1.union(rdd2).collect
res16: Array[(String, Int)] = Array((张三,1000), (李四,100), (赵六,300), (张三,1000), (王五,100), (温七,300))
</code></pre>
<h4 id="intersectionotherdataset"><strong>intersection</strong>(<em>otherDataset</em>)</h4>
<p>返回包含源数据集和参数中元素交集的新RDD。</p>
<p><code>def intersection(other: org.apache.spark.rdd.RDD[T],numPartitions: Int): org.apache.spark.rdd.RDD[T]</code></p>
<pre><code class="language-java">scala&gt; var rdd1=sc.parallelize(Array((&quot;张三&quot;,1000),(&quot;李四&quot;,100),(&quot;赵六&quot;,300)))
scala&gt; var rdd2=sc.parallelize(Array((&quot;张三&quot;,1000),(&quot;王五&quot;,100),(&quot;温七&quot;,300)))
scala&gt; rdd1.intersection(rdd2).collect
res17: Array[(String, Int)] = Array((张三,1000))

</code></pre>
<h4 id="distinctnumpartitions"><strong>distinct</strong>([<em>numPartitions</em>]))</h4>
<p>返回包含源数据集的不同元素的新数据集。</p>
<pre><code class="language-java">scala&gt;  sc.parallelize(List(1,2,3,3,5,7,2),3).distinct.collect
res19: Array[Int] = Array(3, 1, 7, 5, 2)
</code></pre>
<h4 id="groupbykeynumpartitions"><strong>groupByKey</strong>([<em>numPartitions</em>])</h4>
<p>在（K，V）对的数据集上调用时，返回<code>（K，Iterable &lt;V&gt;）</code>对的数据集。 注意：如果要对每个键执行聚合（例如总和或平均值）进行分组，则使用reduceByKey或aggregateByKey将产生更好的性能。 注意：默认情况下，输出中的并行级别取决于父RDD的分区数。您可以传递可选的numPartitions参数来设置不同数量的任务。</p>
<pre><code class="language-java">scala&gt;  sc.parallelize(List(&quot;ni hao&quot;,&quot;hello spark&quot;),3).flatMap(line=&gt;line.split(&quot;\\s+&quot;)).map(word=&gt;(word,1)).groupByKey(3).map(tuple=&gt;(tuple._1,tuple._2.sum)).collect
</code></pre>
<h4 id="reducebykeyfunc-numpartitions"><strong>reduceByKey</strong>(<em>func</em>, [<em>numPartitions</em>])</h4>
<p>当调用（K，V）对的数据集时，返回（K，V）对的数据集，其中使用给定的reduce函数func聚合每个键的值，该函数必须是类型（V，V）=&gt; V.</p>
<pre><code class="language-java">scala&gt;  sc.parallelize(List(&quot;ni hao&quot;,&quot;hello spark&quot;),3).flatMap(line=&gt;line.split(&quot;\\s+&quot;)).map(word=&gt;(word,1)).reduceByKey((v1,v2)=&gt;v1+v2).collect()
res33: Array[(String, Int)] = Array((hao,1), (hello,1), (spark,1), (ni,1))

scala&gt;  sc.parallelize(List(&quot;ni hao&quot;,&quot;hello spark&quot;),3).flatMap(line=&gt;line.split(&quot;\\s+&quot;)).map(word=&gt;(word,1)).reduceByKey(_+_).collect()
res34: Array[(String, Int)] = Array((hao,1), (hello,1), (spark,1), (ni,1))
</code></pre>
<h4 id="aggregatebykeyzerovalueseqop-combop-numpartitions"><strong>aggregateByKey</strong>(<em>zeroValue</em>)(<em>seqOp</em>, <em>combOp</em>, [<em>numPartitions</em>])</h4>
<p>当调用（K，V）对的数据集时，返回（K，U）对的数据集，其中使用给定的组合函数和中性“零”值聚合每个键的值。允许与输入值类型不同的聚合值类型，同时避免不必要的分配。</p>
<pre><code class="language-java">scala&gt;  sc.parallelize(List(&quot;ni hao&quot;,&quot;hello spark&quot;),3).flatMap(line=&gt;line.split(&quot;\\s+&quot;)).map(word=&gt;(word,1)).aggregateByKey(0L)((z,v)=&gt;z+v,(u1,u2)=&gt;u1+u2).collect
res35: Array[(String, Long)] = Array((hao,1), (hello,1), (spark,1), (ni,1))
</code></pre>
<h4 id="sortbykeyascending-numpartitions"><strong>sortByKey</strong>([<em>ascending</em>], [<em>numPartitions</em>])</h4>
<p>当调用K实现Ordered的（K，V）对数据集时，返回按键升序或降序排序的（K，V）对数据集，如布尔升序参数中所指定。</p>
<pre><code class="language-java">scala&gt;  sc.parallelize(List(&quot;ni hao&quot;,&quot;hello spark&quot;),3).flatMap(line=&gt;line.split(&quot;\\s+&quot;)).map(word=&gt;(word,1)).aggregateByKey(0L)((z,v)=&gt;z+v,(u1,u2)=&gt;u1+u2).sortByKey(false).collect()
res37: Array[(String, Long)] = Array((spark,1), (ni,1), (hello,1), (hao,1))

</code></pre>
<h4 id="sortbyfuncascending-numpartitions"><strong>sortBy</strong>(func,[<em>ascending</em>], [<em>numPartitions</em>])**</h4>
<p>对（K，V）数据集调用sortBy时，用户可以通过指定func指定排序规则，T =&gt; U 要求U必须实现Ordered接口</p>
<pre><code class="language-java">scala&gt;  sc.parallelize(List(&quot;ni hao&quot;,&quot;hello spark&quot;),3).flatMap(line=&gt;line.split(&quot;\\s+&quot;)).map(word=&gt;(word,1)).aggregateByKey(0L)((z,v)=&gt;z+v,(u1,u2)=&gt;u1+u2).sortBy(_._2,true,2).collect
res42: Array[(String, Long)] = Array((hao,1), (hello,1), (spark,1), (ni,1))
</code></pre>
<h4 id="join">join</h4>
<p>当调用类型（K，V）和（K，W）的数据集时，返回（K，（V，W））对的数据集以及每个键的所有元素对。通过leftOuterJoin，rightOuterJoin和fullOuterJoin支持外连接。</p>
<pre><code class="language-java">scala&gt; var rdd1=sc.parallelize(Array((&quot;001&quot;,&quot;张三&quot;),(&quot;002&quot;,&quot;李四&quot;),(&quot;003&quot;,&quot;王五&quot;)))
scala&gt; var rdd2=sc.parallelize(Array((&quot;001&quot;,(&quot;apple&quot;,18.0)),(&quot;001&quot;,(&quot;orange&quot;,18.0))))
scala&gt; rdd1.join(rdd2).collect
res43: Array[(String, (String, (String, Double)))] = Array((001,(张三,(apple,18.0))), (001,(张三,(orange,18.0))))

</code></pre>
<h4 id="cogroup">cogroup</h4>
<p>当调用类型（K，V）和（K，W）的数据集时，返回（K，（Iterable ，Iterable ））元组的数据集。此操作也称为groupWith。</p>
<pre><code class="language-java">scala&gt; var rdd1=sc.parallelize(Array((&quot;001&quot;,&quot;张三&quot;),(&quot;002&quot;,&quot;李四&quot;),(&quot;003&quot;,&quot;王五&quot;)))
scala&gt; var rdd2=sc.parallelize(Array((&quot;001&quot;,&quot;apple&quot;),(&quot;001&quot;,&quot;orange&quot;),(&quot;002&quot;,&quot;book&quot;)))
scala&gt; rdd1.cogroup(rdd2).collect()
res46: Array[(String, (Iterable[String], Iterable[String]))] = Array((001,(CompactBuffer(张三),CompactBuffer(apple, orange))), (002,(CompactBuffer(李四),CompactBuffer(book))), (003,(CompactBuffer(王五),CompactBuffer())))
</code></pre>
<h4 id="cartesian">cartesian</h4>
<p>当调用类型为T和U的数据集时，返回（T，U）对的数据集（所有元素对）。</p>
<pre><code class="language-java">scala&gt; var rdd1=sc.parallelize(List(&quot;a&quot;,&quot;b&quot;,&quot;c&quot;))
scala&gt; var rdd2=sc.parallelize(List(1,2,3,4))
scala&gt; rdd1.cartesian(rdd2).collect()
res47: Array[(String, Int)] = Array((a,1), (a,2), (a,3), (a,4), (b,1), (b,2), (b,3), (b,4), (c,1), (c,2), (c,3), (c,4))
</code></pre>
<h4 id="coalescenumpartitions">coalesce(numPartitions)</h4>
<p>将RDD中的分区数减少为numPartitions。过滤大型数据集后，可以使用概算子减少分区数。</p>
<pre><code class="language-java">scala&gt;  sc.parallelize(List(&quot;ni hao&quot;,&quot;hello spark&quot;),3).coalesce(1).partitions.length
res50: Int = 1

scala&gt;  sc.parallelize(List(&quot;ni hao&quot;,&quot;hello spark&quot;),3).coalesce(1).getNumPartitions
res51: Int = 1

</code></pre>
<h4 id="repartition">repartition</h4>
<p>随机重新调整RDD中的数据以创建更多或更少的分区。</p>
<pre><code class="language-java">scala&gt; sc.parallelize(List(&quot;a&quot;,&quot;b&quot;,&quot;c&quot;),3).mapPartitionsWithIndex((index,values)=&gt;for(i&lt;-values) yield (index,i) ).collect
res52: Array[(Int, String)] = Array((0,a), (1,b), (2,c))

scala&gt; sc.parallelize(List(&quot;a&quot;,&quot;b&quot;,&quot;c&quot;),3).repartition(2).mapPartitionsWithIndex((index,values)=&gt;for(i&lt;-values) yield (index,i) ).collect
res53: Array[(Int, String)] = Array((0,a), (0,c), (1,b))
</code></pre>
<h3 id="动作算子">动作算子</h3>
<h4 id="collect">collect</h4>
<p>用在测试环境下，通常使用collect算子将远程计算的结果拿到Drvier端，注意一般数据量比较小，用于测试。</p>
<pre><code class="language-java">scala&gt; var rdd1=sc.parallelize(List(1,2,3,4,5),3).collect().foreach(println)
</code></pre>
<h4 id="saveastextfile">saveAsTextFile</h4>
<p>将计算结果存储在文件系统中，一般存储在HDFS上</p>
<pre><code class="language-java">scala&gt;  sc.parallelize(List(&quot;ni hao&quot;,&quot;hello spark&quot;),3).flatMap(_.split(&quot;\\s+&quot;)).map((_,1)).reduceByKey(_+_).sortBy(_._2,false,3).saveAsTextFile(&quot;hdfs:///wordcounts&quot;)
</code></pre>
<h4 id="foreach">foreach</h4>
<p>迭代遍历所有的RDD中的元素，通常是将foreach传递的数据写到外围系统中，比如说可以将数据写入到Hbase中。</p>
<p>scala&gt;  sc.parallelize(List(&quot;ni hao&quot;,&quot;hello spark&quot;),3).flatMap(<em>.split(&quot;\s+&quot;)).map((</em>,1)).reduceByKey(<em>+</em>).sortBy(_._2,false,3).foreach(println)<br>
(hao,1)<br>
(hello,1)<br>
(spark,1)<br>
(ni,1)</p>
<blockquote>
<p>注意如果使用以上代码写数据到外围系统，会因为不断创建和关闭连接影响写入效率，一般推荐使用foreachPartition</p>
</blockquote>
<pre><code class="language-java">val lineRDD: RDD[String] = sc.textFile(&quot;file:///E:/demo/words/t_word.txt&quot;)
lineRDD.flatMap(line=&gt;line.split(&quot; &quot;))
    .map(word=&gt;(word,1))
    .groupByKey()
    .map(tuple=&gt;(tuple._1,tuple._2.sum))
    .sortBy(tuple=&gt;tuple._2,false,3)
    .foreachPartition(items=&gt;{
        //创建连接
        items.foreach(t=&gt;println(&quot;存储到数据库&quot;+t))
        //关闭连接
    })
</code></pre>
<h3 id="共享变量">共享变量</h3>
<h4 id="变量广播">变量广播</h4>
<p>通常情况下，当一个RDD的很多操作都需要使用driver中定义的变量时，每次操作，driver都要把变量发送给worker节点一次，如果这个变量中的数据很大的话，会产生很高的传输负载，导致执行效率降低。使用广播变量可以使程序高效地将一个很大的<code>只读</code>数据发送给多个worker节点，而且对每个worker节点只需要传输一次，每次操作时executor可以直接获取本地保存的数据副本，不需要多次传输。</p>
<pre><code class="language-java">val conf = new SparkConf().setAppName(&quot;demo&quot;).setMaster(&quot;local[2]&quot;)
val sc = new SparkContext(conf)

val userList = List(
    &quot;001,张三,28,0&quot;,
    &quot;002,李四,18,1&quot;,
    &quot;003,王五,38,0&quot;,
    &quot;004,zhaoliu,38,-1&quot;
)
val genderMap = Map(&quot;0&quot; -&gt; &quot;女&quot;, &quot;1&quot; -&gt; &quot;男&quot;)
val bcMap = sc.broadcast(genderMap)

sc.parallelize(userList,3)
.map(info=&gt;{
    val prefix = info.substring(0, info.lastIndexOf(&quot;,&quot;))
    val gender = info.substring(info.lastIndexOf(&quot;,&quot;) + 1)
    val genderMapValue = bcMap.value
    val newGender = genderMapValue.getOrElse(gender, &quot;未知&quot;)
    prefix + &quot;,&quot; + newGender
}).collect().foreach(println)

sc.stop() 
</code></pre>
<h4 id="累加器">累加器</h4>
<p>Spark提供的Accumulator，主要用于多个节点对一个变量进行共享性的操作。Accumulator只提供了累加的功能。但是确给我们提供了多个task对一个变量并行操作的功能。但是task只能对Accumulator进行累加操作，不能读取它的值。只有Driver程序可以读取Accumulator的值。</p>
<pre><code class="language-java">scala&gt; var count=sc.longAccumulator(&quot;count&quot;)
scala&gt; sc.parallelize(List(1,2,3,4,5,6),3).foreach(item=&gt; count.add(item))
scala&gt; count.value
res1: Long = 21
</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Spark入门(三)——SparkRDD剖析(面试点)]]></title>
        <id>https://mask0407.github.io/spark02/</id>
        <link href="https://mask0407.github.io/spark02/">
        </link>
        <updated>2020-06-29T03:12:09.000Z</updated>
        <content type="html"><![CDATA[<p><ul class="markdownIt-TOC">
<li>
<ul>
<li><a href="#rdd%E7%AE%80%E4%BB%8B">RDD简介</a>
<ul>
<li><a href="#%E5%A6%82%E4%B8%8B%E6%A1%88%E4%BE%8B">如下案例：</a></li>
</ul>
</li>
<li><a href="#rdd%E5%AE%B9%E9%94%99">RDD容错</a></li>
<li><a href="#rdd-%E5%AE%BD%E7%AA%84%E4%BE%9D%E8%B5%96">RDD 宽窄依赖</a></li>
<li><a href="#sage%E5%88%92%E5%88%86%E9%87%8D%E7%82%B9">Sage划分(重点)</a></li>
<li><a href="#%E5%B0%8F%E7%BB%93">小结</a></li>
<li><a href="#rdd%E7%BC%93%E5%AD%98%E6%9C%BA%E5%88%B6">RDD缓存机制</a></li>
<li><a href="#check-point-%E6%9C%BA%E5%88%B6">Check Point 机制</a></li>
</ul>
</li>
</ul>
(Spark RDD剖析)</p>
<h2 id="rdd简介">RDD简介</h2>
<p>Spark计算中一个重要的概念就是可以跨越多个节点的可伸缩分布式数据集 RDD（resilient distributed<br>
dataset） Spark的内存计算的核心就是RDD的并行计算。RDD可以理解是一个弹性的，分布式、不可变的、带有分区的数据集合，所谓的Spark的批处理，实际上就是正对RDD的集合操作，RDD有以下特点：</p>
<blockquote>
<ul>
<li>任意一个RDD都包含分区数（决定程序某个阶段计算并行度）</li>
<li>RDD所谓的分布式计算是在分区内部计算的</li>
<li>因为RDD是只读的，RDD之间的变换存着依赖关系（宽依赖、窄依赖）</li>
<li>针对于k-v类型的RDD，一般可以指定分区策略（一般系统提供）</li>
<li>针对于存储在HDFS上的文件，系统可以计算最优位置，计算每个切片。（了解）</li>
</ul>
</blockquote>
<h3 id="如下案例">如下案例：</h3>
<p><img src="https://img-blog.csdnimg.cn/20200522171658139.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L00yODM1OTIzMzg=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述" loading="lazy"><br>
通过上述的代码中不难发现，Spark的整个任务的计算无外乎围绕RDD的三种类型操作RDD创建、RDD转换、RDD Action.通常习惯性的将flatMap/map/reduceByKey称为RDD的转换算子，collect触发任务执行，因此被人们称为动作算子。在Spark中所有的Transform算子都是lazy执行的，只有在Action算子的时候，Spark才会真正的运行任务，也就是说只有遇到Action算子的时候，SparkContext才会对任务做DAG状态拆分，系统才会计算每个状态下任务的TaskSet，继而TaskSchedule才会将任务提交给Executors执行。现将以上字符统计计算流程描述如下：</p>
<figure data-type="image" tabindex="1"><img src="https://img-blog.csdnimg.cn/20200522171841951.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L00yODM1OTIzMzg=,size_16,color_FFFFFF,t_70" alt="DAG划分" loading="lazy"></figure>
<p><code>textFile(&quot;路径&quot;，分区数)</code>-&gt; <code>flatMap</code> -&gt; <code>map</code> -&gt; <code>reduceByKey</code> -&gt; <code>sortBy</code>在这些转换中其中<code>flatMap/map</code>、<code>reduceByKey</code>、<code>sotBy</code>都是转换算子，所有的转换算子都是<code>Lazy</code>执行的。程序在遇到<code>collect（Action 算子）</code>系统会触发job执行。Spark底层会按照RDD的依赖关系将整个计算拆分成若干个阶段，我们通常将RDD的依赖关系称为<code>RDD的血统-lineage</code>。血统的依赖通常包含：<code>宽依赖</code>、<code>窄依赖</code>。</p>
<h2 id="rdd容错">RDD容错</h2>
<p>在理解DAGSchedule如何做状态划分的前提是需要大家了解一个专业术语lineage通常被人们称为RDD的血统。在了解什么是RDD的血统之前，先来看看程序猿进化过程。<br>
<img src="https://img-blog.csdnimg.cn/20200522172008430.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L00yODM1OTIzMzg=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述" loading="lazy"><br>
上图中描述了一个程序猿起源变化的过程，我们可以近似的理解类似于RDD的转换也是一样的，Spark的计算本质就是对RDD做各种转换，因为RDD是一个不可变只读的集合，因此每次的转换都需要上一次的RDD作为本次转换的输入，因此RDD的lineage描述的是RDD间的相互依赖关系。为了保证RDD中数据的健壮性，RDD数据集通过所谓的血统关系(Lineage)记住了它是如何从其它RDD中演变过来的。Spark将RDD之间的关系归类为宽依赖和窄依赖。Spark会根据Lineage存储的RDD的依赖关系对RDD计算做故障容错，目前Saprk的容错策略更具RDD依赖关系重新计算、对RDD做Cache、对RDD做Checkpoint手段完成RDD计算的故障容错。</p>
<h2 id="rdd-宽窄依赖">RDD 宽窄依赖</h2>
<p>RDD在Lineage依赖方面分为两种<code>Narrow Dependencies</code>与<code>Wide Dependencies</code>用来解决数据容错的高效性。<code>Narrow Dependencies</code>是指父RDD的每一个分区最多被一个子RDD的分区所用，表现为一个父RDD的分区对应于一个子RDD的分区或多个父RDD的分区对应于子RDD的一个分区，也就是说一个父RDD的一个分区不可能对应一个子RDD的多个分区。<code>Wide Dependencies</code>父RDD的一个分区对应一个子RDD的多个分区。</p>
<p><img src="https://img-blog.csdnimg.cn/20200522172404876.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L00yODM1OTIzMzg=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述" loading="lazy"><br>
对于Wide Dependencies这种计算的输入和输出在不同的节点上，一般需要夸节点做Shuffle，因此如果是RDD在做宽依赖恢复的时候需要多个节点重新计算成本较高。相对于Narrow Dependencies RDD间的计算是在同一个Task当中实现的是线程内部的的计算，因此在RDD分区数据丢失的的时候，也非常容易恢复。</p>
<h2 id="sage划分重点">Sage划分(重点)</h2>
<p>Spark任务阶段的划分是按照RDD的lineage关系逆向生成的这么一个过程，Spark任务提交的流程大致如下图所示：<br>
<img src="https://img-blog.csdnimg.cn/20200522172500694.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L00yODM1OTIzMzg=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述" loading="lazy"><br>
这里可以分析一下DAGScheduel中对State拆分的逻辑代码片段如下所示：</p>
<ul>
<li><code>DAGScheduler.scala</code> 第<code>719</code>行</li>
</ul>
<pre><code class="language-java">def runJob[T, U](
      rdd: RDD[T],
      func: (TaskContext, Iterator[T]) =&gt; U,
      partitions: Seq[Int],
      callSite: CallSite,
      resultHandler: (Int, U) =&gt; Unit,
      properties: Properties): Unit = {
    val start = System.nanoTime
    val waiter = submitJob(rdd, func, partitions, callSite, resultHandler, properties)
    //...
  }
</code></pre>
<ul>
<li><code>DAGScheduler</code> - <code>675</code>行</li>
</ul>
<pre><code class="language-java">  def submitJob[T, U](
      rdd: RDD[T],
      func: (TaskContext, Iterator[T]) =&gt; U,
      partitions: Seq[Int],
      callSite: CallSite,
      resultHandler: (Int, U) =&gt; Unit,
      properties: Properties): JobWaiter[U] = {
      //eventProcessLoop 实现的是一个队列，系统底层会调用 doOnReceive -&gt; case JobSubmitted -&gt; dagScheduler.handleJobSubmitted(951行)
      eventProcessLoop.post(JobSubmitted(
      jobId, rdd, func2, partitions.toArray, callSite, waiter,
      SerializationUtils.clone(properties)))
    waiter
  }
</code></pre>
<ul>
<li><code>DAGScheduler</code> - <code>951</code>行</li>
</ul>
<pre><code class="language-java"> private[scheduler] def handleJobSubmitted(jobId: Int,
      finalRDD: RDD[_],
      func: (TaskContext, Iterator[_]) =&gt; _,
      partitions: Array[Int],
      callSite: CallSite,
      listener: JobListener,
      properties: Properties) {
    var finalStage: ResultStage = null
    try {
      //...
      finalStage = createResultStage(finalRDD, func, partitions, jobId, callSite)
    } catch {
      //...
    }
    submitStage(finalStage)
 }

</code></pre>
<ul>
<li><code>DAGScheduler</code> - <code>1060</code>行</li>
</ul>
<pre><code class="language-java">  private def submitStage(stage: Stage) {
    val jobId = activeJobForStage(stage)
    if (jobId.isDefined) {
      logDebug(&quot;submitStage(&quot; + stage + &quot;)&quot;)
      if (!waitingStages(stage) &amp;&amp; !runningStages(stage) &amp;&amp; !failedStages(stage)) {
         //计算当前State的父Stage
        val missing = getMissingParentStages(stage).sortBy(_.id)
        logDebug(&quot;missing: &quot; + missing)
        if (missing.isEmpty) {
          logInfo(&quot;Submitting &quot; + stage + &quot; (&quot; + stage.rdd + &quot;), which has no missing parents&quot;)
           //如果当前的State没有父Stage，就提交当前Stage中的Task
          submitMissingTasks(stage, jobId.get)
        } else {
          for (parent &lt;- missing) {
            //递归查找当前父Stage的父Stage
            submitStage(parent)
          }
          waitingStages += stage
        }
      }
    } else {
      abortStage(stage, &quot;No active job for stage &quot; + stage.id, None)
    }
  }
</code></pre>
<ul>
<li><code>DAGScheduler</code> - <code>549</code>行 (获取当前State的父State)</li>
</ul>
<pre><code class="language-java"> private def getMissingParentStages(stage: Stage): List[Stage] = {
    val missing = new HashSet[Stage]
    val visited = new HashSet[RDD[_]]
    // We are manually maintaining a stack here to prevent StackOverflowError
    // caused by recursively visiting
    val waitingForVisit = new ArrayStack[RDD[_]]//栈
    def visit(rdd: RDD[_]) {
      if (!visited(rdd)) {
        visited += rdd
        val rddHasUncachedPartitions = getCacheLocs(rdd).contains(Nil)
        if (rddHasUncachedPartitions) {
          for (dep &lt;- rdd.dependencies) {
            dep match {
                //如果是宽依赖ShuffleDependency，就添加一个Stage
              case shufDep: ShuffleDependency，[_, _, _] =&gt;
                val mapStage = getOrCreateShuffleMapStage(shufDep, stage.firstJobId)
                if (!mapStage.isAvailable) {
                  missing += mapStage
                }
                //如果是窄依赖NarrowDependency，将当前的父RDD添加到栈中
              case narrowDep: NarrowDependency[_] =&gt;
                waitingForVisit.push(narrowDep.rdd)
            }
          }
        }
      }
    }
    waitingForVisit.push(stage.rdd)
    while (waitingForVisit.nonEmpty) {//循环遍历栈，计算 stage
      visit(waitingForVisit.pop())
    }
    missing.toList
  }
</code></pre>
<ul>
<li><code>DAGScheduler</code> - <code>1083</code>行 (提交当前Stage的TaskSet)</li>
</ul>
<pre><code class="language-java"> private def submitMissingTasks(stage: Stage, jobId: Int) {
    logDebug(&quot;submitMissingTasks(&quot; + stage + &quot;)&quot;)

    // First figure out the indexes of partition ids to compute.
    val partitionsToCompute: Seq[Int] = stage.findMissingPartitions()

    // Use the scheduling pool, job group, description, etc. from an ActiveJob associated
    // with this Stage
    val properties = jobIdToActiveJob(jobId).properties

    runningStages += stage
    // SparkListenerStageSubmitted should be posted before testing whether tasks are
    // serializable. If tasks are not serializable, a SparkListenerStageCompleted event
    // will be posted, which should always come after a corresponding SparkListenerStageSubmitted
    // event.
    stage match {
      case s: ShuffleMapStage =&gt;
        outputCommitCoordinator.stageStart(stage = s.id, maxPartitionId = s.numPartitions - 1)
      case s: ResultStage =&gt;
        outputCommitCoordinator.stageStart(
          stage = s.id, maxPartitionId = s.rdd.partitions.length - 1)
    }
    val taskIdToLocations: Map[Int, Seq[TaskLocation]] = try {
      stage match {
        case s: ShuffleMapStage =&gt;
          partitionsToCompute.map { id =&gt; (id, getPreferredLocs(stage.rdd, id))}.toMap
        case s: ResultStage =&gt;
          partitionsToCompute.map { id =&gt;
            val p = s.partitions(id)
            (id, getPreferredLocs(stage.rdd, p))
          }.toMap
      }
    } catch {
      case NonFatal(e) =&gt;
        stage.makeNewStageAttempt(partitionsToCompute.size)
        listenerBus.post(SparkListenerStageSubmitted(stage.latestInfo, properties))
        abortStage(stage, s&quot;Task creation failed: $e\n${Utils.exceptionString(e)}&quot;, Some(e))
        runningStages -= stage
        return
    }

    stage.makeNewStageAttempt(partitionsToCompute.size, taskIdToLocations.values.toSeq)

    // If there are tasks to execute, record the submission time of the stage. Otherwise,
    // post the even without the submission time, which indicates that this stage was
    // skipped.
    if (partitionsToCompute.nonEmpty) {
      stage.latestInfo.submissionTime = Some(clock.getTimeMillis())
    }
    listenerBus.post(SparkListenerStageSubmitted(stage.latestInfo, properties))

    // TODO: Maybe we can keep the taskBinary in Stage to avoid serializing it multiple times.
    // Broadcasted binary for the task, used to dispatch tasks to executors. Note that we broadcast
    // the serialized copy of the RDD and for each task we will deserialize it, which means each
    // task gets a different copy of the RDD. This provides stronger isolation between tasks that
    // might modify state of objects referenced in their closures. This is necessary in Hadoop
    // where the JobConf/Configuration object is not thread-safe.
    var taskBinary: Broadcast[Array[Byte]] = null
    var partitions: Array[Partition] = null
    try {
      // For ShuffleMapTask, serialize and broadcast (rdd, shuffleDep).
      // For ResultTask, serialize and broadcast (rdd, func).
      var taskBinaryBytes: Array[Byte] = null
      // taskBinaryBytes and partitions are both effected by the checkpoint status. We need
      // this synchronization in case another concurrent job is checkpointing this RDD, so we get a
      // consistent view of both variables.
      RDDCheckpointData.synchronized {
        taskBinaryBytes = stage match {
          case stage: ShuffleMapStage =&gt;
            JavaUtils.bufferToArray(
              closureSerializer.serialize((stage.rdd, stage.shuffleDep): AnyRef))
          case stage: ResultStage =&gt;
            JavaUtils.bufferToArray(closureSerializer.serialize((stage.rdd, stage.func): AnyRef))
        }

        partitions = stage.rdd.partitions
      }

      taskBinary = sc.broadcast(taskBinaryBytes)
    } catch {
      // In the case of a failure during serialization, abort the stage.
      case e: NotSerializableException =&gt;
        abortStage(stage, &quot;Task not serializable: &quot; + e.toString, Some(e))
        runningStages -= stage

        // Abort execution
        return
      case e: Throwable =&gt;
        abortStage(stage, s&quot;Task serialization failed: $e\n${Utils.exceptionString(e)}&quot;, Some(e))
        runningStages -= stage

        // Abort execution
        return
    }

    val tasks: Seq[Task[_]] = try {
      val serializedTaskMetrics = closureSerializer.serialize(stage.latestInfo.taskMetrics).array()
      stage match {
        case stage: ShuffleMapStage =&gt;
          stage.pendingPartitions.clear()
          partitionsToCompute.map { id =&gt;
            val locs = taskIdToLocations(id)
            val part = partitions(id)
            stage.pendingPartitions += id
            new ShuffleMapTask(stage.id, stage.latestInfo.attemptNumber,
              taskBinary, part, locs, properties, serializedTaskMetrics, Option(jobId),
              Option(sc.applicationId), sc.applicationAttemptId, stage.rdd.isBarrier())
          }

        case stage: ResultStage =&gt;
          partitionsToCompute.map { id =&gt;
            val p: Int = stage.partitions(id)
            val part = partitions(p)
            val locs = taskIdToLocations(id)
            new ResultTask(stage.id, stage.latestInfo.attemptNumber,
              taskBinary, part, locs, id, properties, serializedTaskMetrics,
              Option(jobId), Option(sc.applicationId), sc.applicationAttemptId,
              stage.rdd.isBarrier())
          }
      }
    } catch {
      case NonFatal(e) =&gt;
        abortStage(stage, s&quot;Task creation failed: $e\n${Utils.exceptionString(e)}&quot;, Some(e))
        runningStages -= stage
        return
    }

    if (tasks.size &gt; 0) {
      logInfo(s&quot;Submitting ${tasks.size} missing tasks from $stage (${stage.rdd}) (first 15 &quot; +
        s&quot;tasks are for partitions ${tasks.take(15).map(_.partitionId)})&quot;)
      taskScheduler.submitTasks(new TaskSet(
        tasks.toArray, stage.id, stage.latestInfo.attemptNumber, jobId, properties))
    } else {
      // Because we posted SparkListenerStageSubmitted earlier, we should mark
      // the stage as completed here in case there are no tasks to run
      markStageAsFinished(stage, None)

      stage match {
        case stage: ShuffleMapStage =&gt;
          logDebug(s&quot;Stage ${stage} is actually done; &quot; +
              s&quot;(available: ${stage.isAvailable},&quot; +
              s&quot;available outputs: ${stage.numAvailableOutputs},&quot; +
              s&quot;partitions: ${stage.numPartitions})&quot;)
          markMapStageJobsAsFinished(stage)
        case stage : ResultStage =&gt;
          logDebug(s&quot;Stage ${stage} is actually done; (partitions: ${stage.numPartitions})&quot;)
      }
      submitWaitingChildStages(stage)
    }
  }
</code></pre>
<h2 id="小结">小结</h2>
<p>通过以上源码分析，可以得出Spark所谓宽窄依赖事实上指的是<code>ShuffleDependency</code>或者是<code>NarrowDependency</code>如果是<code>ShuffleDependency</code>系统会生成一个<code>ShuffeMapStage</code>,如果是<code>NarrowDependency</code>则忽略，归为当前Stage。当系统回推到起始RDD的时候因为发现当前RDD或者ShuffleMapStage没有父Stage的时候，当前系统会将当前State下的Task封装成<code>ShuffleMapTask</code>(如果是ResultStage就是<code>ResultTask</code>),当前Task的数目等于当前state分区的分区数。然后将Task封装成TaskSet通过调用taskScheduler.submitTasks将任务提交给集群。</p>
<h2 id="rdd缓存机制">RDD缓存机制</h2>
<p>缓存是一种RDD计算容错的一种手段，程序在RDD数据丢失的时候，可以通过缓存快速计算当前RDD的值，而不需要反推出所有的RDD重新计算，因此Spark在需要对某个RDD多次使用的时候，为了提高程序的执行效率用户可以考虑使用RDD的cache。如下测试：</p>
<pre><code class="language-java">val conf = new SparkConf()
	.setAppName(&quot;word-count&quot;)
	.setMaster(&quot;local[2]&quot;)
val sc = new SparkContext(conf)
val value: RDD[String] = sc.textFile(&quot;file:///D:/demo/words/&quot;)
   .cache()
value.count()

var begin=System.currentTimeMillis()
value.count()
var end=System.currentTimeMillis()
println(&quot;耗时：&quot;+ (end-begin))//耗时：253

//失效缓存
value.unpersist()
begin=System.currentTimeMillis()
value.count()
end=System.currentTimeMillis()
println(&quot;不使用缓存耗时：&quot;+ (end-begin))//2029
sc.stop()
</code></pre>
<p>除了调用cache之外，Spark提供了更细粒度的RDD缓存方案，用户可以根据集群的内存状态选择合适的缓存策略。用户可以使用persist方法指定缓存级别。缓存级别有如下可选项：</p>
<pre><code class="language-java">val NONE = new StorageLevel(false, false, false, false)
val DISK_ONLY = new StorageLevel(true, false, false, false)
val DISK_ONLY_2 = new StorageLevel(true, false, false, false, 2)
val MEMORY_ONLY = new StorageLevel(false, true, false, true)
val MEMORY_ONLY_2 = new StorageLevel(false, true, false, true, 2)
val MEMORY_ONLY_SER = new StorageLevel(false, true, false, false)
val MEMORY_ONLY_SER_2 = new StorageLevel(false, true, false, false, 2)
val MEMORY_AND_DISK = new StorageLevel(true, true, false, true)
val MEMORY_AND_DISK_2 = new StorageLevel(true, true, false, true, 2)
val MEMORY_AND_DISK_SER = new StorageLevel(true, true, false, false)
val MEMORY_AND_DISK_SER_2 = new StorageLevel(true, true, false, false, 2)
val OFF_HEAP = new StorageLevel(true, true, true, false, 1)
</code></pre>
<pre><code class="language-java">xxRDD.persist(StorageLevel.MEMORY_AND_DISK_SER_2)
</code></pre>
<p>其中：</p>
<p><code>MEMORY_ONLY</code>：表示数据完全不经过序列化存储在内存中，效率高，但是有可能导致内存溢出.</p>
<p><code>MEMORY_ONLY_SER</code>和MEMORY_ONLY一样，只不过需要对RDD的数据做序列化，牺牲CPU节省内存，同样会导致内存溢出可能。</p>
<blockquote>
<p>其中<code>_2</code>表示缓存结果有备份，如果大家不确定该使用哪种级别，一般推荐<code>MEMORY_AND_DISK_SER_2</code></p>
</blockquote>
<h2 id="check-point-机制">Check Point 机制</h2>
<p>除了使用缓存机制可以有效的保证RDD的故障恢复，但是如果缓存失效还是会在导致系统重新计算RDD的结果，所以对于一些RDD的lineage较长的场景，计算比较耗时，用户可以尝试使用checkpoint机制存储RDD的计算结果，该种机制和缓存最大的不同在于，使用checkpoint之后被checkpoint的RDD数据直接持久化在文件系统中，一般推荐将结果写在hdfs中，这种checpoint并不会自动清空。注意checkpoint在计算的过程中先是对RDD做mark，在任务执行结束后，再对mark的RDD实行checkpoint，也就是要重新计算被Mark之后的rdd的依赖和结果，因此为了避免Mark RDD重复计算，推荐使用策略</p>
<pre><code class="language-java">val conf = new SparkConf().setMaster(&quot;yarn&quot;).setAppName(&quot;wordcount&quot;)
val sc = new SparkContext(conf)
sc.setCheckpointDir(&quot;hdfs:///checkpoints&quot;)

val lineRDD: RDD[String] = sc.textFile(&quot;hdfs:///words/t_word.txt&quot;)

val cacheRdd = lineRDD.flatMap(line =&gt; line.split(&quot; &quot;))
.map(word =&gt; (word, 1))
.groupByKey()
.map(tuple =&gt; (tuple._1, tuple._2.sum))
.sortBy(tuple =&gt; tuple._2, false, 1)
.cache()
cacheRdd.checkpoint()

cacheRdd.collect().foreach(tuple=&gt;println(tuple._1+&quot;-&gt;&quot;+tuple._2))
cacheRdd.unpersist()
//3.关闭sc
sc.stop()
</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Spark入门(二)——Spark环境搭建与开发环境]]></title>
        <id>https://mask0407.github.io/spark01/</id>
        <link href="https://mask0407.github.io/spark01/">
        </link>
        <updated>2020-06-29T03:11:38.000Z</updated>
        <content type="html"><![CDATA[<p><ul class="markdownIt-TOC">
<li>
<ul>
<li><a href="#standalone%E5%8D%95%E8%8A%82%E7%82%B9%E6%A8%A1%E5%BC%8F">Standalone单节点模式</a></li>
<li><a href="#spark-on-yarn">Spark On Yarn</a></li>
<li><a href="#spark-%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83%E6%9E%84%E5%BB%BA">Spark 开发环境构建</a>
<ul>
<li><a href="#sparkrddwordcount%E6%9C%AC%E5%9C%B0">SparkRDDWordCount(本地)</a></li>
<li><a href="#%E9%9B%86%E7%BE%A4yarn">集群(yarn)</a></li>
<li><a href="#%E9%9B%86%E7%BE%A4standalone">集群(standalone)</a></li>
</ul>
</li>
</ul>
</li>
</ul>
(Spark Standalone与Spark On Yarn环境搭建)</p>
<h2 id="standalone单节点模式">Standalone单节点模式</h2>
<ol>
<li><strong>Hadoop环境(有Hadoop环境的可以直接进入下面Spark环境安装)</strong></li>
</ol>
<ul>
<li>设置CentOS进程数和文件数(重启)</li>
</ul>
<pre><code class="language-shell">[root@CentOS ~]# vi /etc/security/limits.conf
* soft nofile 204800
* hard nofile 204800
* soft nproc 204800
* hard nproc 204800
[root@CentOS ~]# reboot
</code></pre>
<ul>
<li>配置主机名(重启)</li>
</ul>
<pre><code class="language-shell">[root@CentOS ~]# vi /etc/sysconfig/network
NETWORKING=yes
HOSTNAME=CentOS
[root@CentOS ~]# reboot
</code></pre>
<ul>
<li>设置IP映射</li>
</ul>
<pre><code class="language-shell">[root@CentOS ~]# vi /etc/hosts
127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4
::1         localhost localhost.localdomain localhost6 localhost6.localdomain6
192.168.111.132 CentOS
</code></pre>
<ul>
<li>防火墙服务</li>
</ul>
<pre><code class="language-shell">[root@CentOS ~]# service iptables stop
iptables: Setting chains to policy ACCEPT: filter          [  OK  ]
iptables: Flushing firewall rules:                         [  OK  ]
iptables: Unloading modules:                               [  OK  ]
[root@CentOS ~]# chkconfig iptables off
</code></pre>
<ul>
<li>安装JDK1.8+</li>
</ul>
<pre><code class="language-shell">[root@CentOS ~]# rpm -ivh jdk-8u191-linux-x64.rpm
warning: jdk-8u191-linux-x64.rpm: Header V3 RSA/SHA256 Signature, key ID ec551f03: NOKEY
Preparing...                ########################################### [100%]
   1:jdk1.8                 ########################################### [100%]
Unpacking JAR files...
        tools.jar...
        plugin.jar...
        javaws.jar...
        deploy.jar...
        rt.jar...
        jsse.jar...
        charsets.jar...
        localedata.jar...
[root@CentOS ~]# vi .bashrc 
JAVA_HOME=/usr/java/latest
PATH=$PATH:$JAVA_HOME/bin
CLASSPATH=.
export JAVA_HOME
export PATH
export CLASSPATH
[root@CentOS ~]# source ~/.bashrc
</code></pre>
<ul>
<li>SSH配置免密</li>
</ul>
<pre><code class="language-shell">[root@CentOS ~]# ssh-keygen -t rsa
Generating public/private rsa key pair.
Enter file in which to save the key (/root/.ssh/id_rsa):
Created directory '/root/.ssh'.
Enter passphrase (empty for no passphrase):
Enter same passphrase again:
Your identification has been saved in /root/.ssh/id_rsa.
Your public key has been saved in /root/.ssh/id_rsa.pub.
The key fingerprint is:
a5:2d:f5:c3:22:83:cf:13:25:59:fb:c1:f4:63:06:d4 root@CentOS
The key's randomart image is:
+--[ RSA 2048]----+
|          ..+.   |
|         o + oE  |
|        o = o =  |
|       . B + + . |
|      . S o =    |
|       o = . .   |
|        +        |
|         .       |
|                 |
+-----------------+
[root@CentOS ~]# ssh-copy-id CentOS
The authenticity of host 'centos (192.168.111.132)' can't be established.
RSA key fingerprint is fa:1b:c0:23:86:ff:08:5e:83:ba:65:4c:e6:f2:1f:3b.
Are you sure you want to continue connecting (yes/no)? yes
Warning: Permanently added 'centos,192.168.111.132' (RSA) to the list of known hosts.
root@centos's password:`需要输入密码`
Now try logging into the machine, with &quot;ssh 'CentOS'&quot;, and check in:

  .ssh/authorized_keys

to make sure we haven't added extra keys that you weren't expecting.
</code></pre>
<ul>
<li>配置HDFS</li>
</ul>
<p>将<code>hadoop-2.9.2.tar.gz</code>解压到系统的<code>/usr</code>目录下然后配置[core|hdfs]-site.xml配置文件。</p>
<pre><code class="language-shell">[root@CentOS ~]# vi /usr/hadoop-2.9.2/etc/hadoop/core-site.xml
</code></pre>
<pre><code class="language-xml">&lt;!--nn访问入口--&gt;
&lt;property&gt;
    &lt;name&gt;fs.defaultFS&lt;/name&gt;
    &lt;value&gt;hdfs://CentOS:9000&lt;/value&gt;
&lt;/property&gt;
&lt;!--hdfs工作基础目录--&gt;
&lt;property&gt;
    &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;
    &lt;value&gt;/usr/hadoop-2.9.2/hadoop-${user.name}&lt;/value&gt;
&lt;/property&gt;
</code></pre>
<pre><code class="language-shell">[root@CentOS ~]# vi /usr/hadoop-2.9.2/etc/hadoop/hdfs-site.xml
</code></pre>
<pre><code class="language-xml">&lt;!--block副本因子--&gt;
&lt;property&gt;
    &lt;name&gt;dfs.replication&lt;/name&gt;
    &lt;value&gt;1&lt;/value&gt;
&lt;/property&gt;
&lt;!--配置Sencondary namenode所在物理主机--&gt;
&lt;property&gt;
    &lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt;
    &lt;value&gt;CentOS:50090&lt;/value&gt;
&lt;/property&gt;
&lt;!--设置datanode最大文件操作数--&gt;
&lt;property&gt;
        &lt;name&gt;dfs.datanode.max.xcievers&lt;/name&gt;
        &lt;value&gt;4096&lt;/value&gt;
&lt;/property&gt;
&lt;!--设置datanode并行处理能力--&gt;
&lt;property&gt;
        &lt;name&gt;dfs.datanode.handler.count&lt;/name&gt;
        &lt;value&gt;6&lt;/value&gt;
&lt;/property&gt;
</code></pre>
<pre><code class="language-shell">[root@CentOS ~]# vi /usr/hadoop-2.9.2/etc/hadoop/slaves
</code></pre>
<pre><code class="language-tex">CentOS
</code></pre>
<ul>
<li>配置hadoop环境变量</li>
</ul>
<pre><code class="language-shell">[root@CentOS ~]# vi .bashrc
HADOOP_HOME=/usr/hadoop-2.9.2
JAVA_HOME=/usr/java/latest
PATH=$PATH:$JAVA_HOME/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin
CLASSPATH=.
export JAVA_HOME
export PATH
export CLASSPATH
export HADOOP_HOME
[root@CentOS ~]# source .bashrc

</code></pre>
<ul>
<li>启动Hadoop服务</li>
</ul>
<pre><code class="language-shell">[root@CentOS ~]# hdfs namenode -format # 创建初始化所需的fsimage文件
[root@CentOS ~]# start-dfs.sh
</code></pre>
<ol start="2">
<li><strong>Spark环境</strong></li>
</ol>
<p>下载<code>spark-2.4.3-bin-without-hadoop.tgz</code>解压到<code>/usr</code>目录,并且将Spark目录修改名字为<code>spark-2.4.3</code>然后修改<code>spark-env.sh</code>和<code>spark-default.conf</code>文件.</p>
<p>下载地址：http://mirrors.tuna.tsinghua.edu.cn/apache/spark/spark-2.4.3/spark-2.4.3-bin-without-hadoop.tgz</p>
<ul>
<li>解压Spark安装包，并且修改解压文件名</li>
</ul>
<pre><code class="language-shell">[root@CentOS ~]# tar -zxf spark-2.4.3-bin-without-hadoop.tgz -C /usr/
[root@CentOS ~]# mv /usr/spark-2.4.3-bin-without-hadoop/ /usr/spark-2.4.3
[root@CentOS ~]# vi .bashrc
SPARK_HOME=/usr/spark-2.4.3
HADOOP_HOME=/usr/hadoop-2.9.2
JAVA_HOME=/usr/java/latest
PATH=$PATH:$JAVA_HOME/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$SPARK_HOME/bin:$SPARK_HOME/sbin
CLASSPATH=.
export JAVA_HOME
export PATH
export CLASSPATH
export HADOOP_HOME
export SPARK_HOME
[root@CentOS ~]# source .bashrc
</code></pre>
<ul>
<li>配置Spark服务</li>
</ul>
<pre><code class="language-shell">[root@CentOS spark-2.4.3]# cd /usr/spark-2.4.3/conf/
[root@CentOS conf]# mv spark-env.sh.template spark-env.sh
[root@CentOS conf]# mv slaves.template slaves
[root@CentOS conf]# vi slaves
CentOS
[root@CentOS conf]# vi spark-env.sh
SPARK_MASTER_HOST=CentOS
SPARK_MASTER_PORT=7077
SPARK_WORKER_CORES=4
SPARK_WORKER_MEMORY=2g
LD_LIBRARY_PATH=/usr/hadoop-2.9.2/lib/native
SPARK_DIST_CLASSPATH=$(hadoop classpath)

export SPARK_MASTER_HOST
export SPARK_MASTER_PORT
export SPARK_WORKER_CORES
export SPARK_WORKER_MEMORY
export LD_LIBRARY_PATH
export SPARK_DIST_CLASSPATH
</code></pre>
<ul>
<li>启动Spark进程</li>
</ul>
<pre><code class="language-shell">[root@CentOS ~]# cd /usr/spark-2.4.3/
[root@CentOS spark-2.4.3]# ./sbin/start-all.sh
starting org.apache.spark.deploy.master.Master, logging to /usr/spark-2.4.3/logs/spark-root-org.apache.spark.deploy.master.Master-1-CentOS.out
CentOS: starting org.apache.spark.deploy.worker.Worker, logging to /usr/spark-2.4.3/logs/spark-root-org.apache.spark.deploy.worker.Worker-1-CentOS.out
</code></pre>
<ul>
<li>测试Spark</li>
</ul>
<pre><code class="language-shell">[root@CentOS spark-2.4.3]# ./bin/spark-shell 
					--master spark://CentOS:7077 
					--deploy-mode client 
					--executor-cores 2

</code></pre>
<blockquote>
<p><code>executor-cores</code>：在standalone模式表示程序每个Worker节点分配资源数。不能超过单台自大core个数，如果不清每台能够分配的最大core的个数，可以使用<code>--total-executor-cores</code>,该种分配会尽最大可能分配。</p>
</blockquote>
<pre><code class="language-java">scala&gt; sc.textFile(&quot;hdfs:///words/t_words&quot;,5)
    .flatMap(_.split(&quot; &quot;))
    .map((_,1))
    .reduceByKey(_+_)
    .sortBy(_._1,true,3)
    .saveAsTextFile(&quot;hdfs:///results&quot;)

</code></pre>
<ul>
<li>上述任务的任务拆分图：<br>
<img src="https://img-blog.csdnimg.cn/20200521090311893.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L00yODM1OTIzMzg=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述" loading="lazy"></li>
</ul>
<h2 id="spark-on-yarn">Spark On Yarn</h2>
<ol>
<li><strong>Hadoop环境(配置过Yarn的可以直接跳过Hadoop安装)</strong></li>
</ol>
<ul>
<li>设置CentOS进程数和文件数(重启)</li>
</ul>
<pre><code class="language-shell">[root@CentOS ~]# vi /etc/security/limits.conf
* soft nofile 204800
* hard nofile 204800
* soft nproc 204800
* hard nproc 204800
[root@CentOS ~]# reboot
</code></pre>
<ul>
<li>配置主机名(重启)</li>
</ul>
<pre><code class="language-shell">[root@CentOS ~]# vi /etc/sysconfig/network
NETWORKING=yes
HOSTNAME=CentOS
[root@CentOS ~]# reboot
</code></pre>
<ul>
<li>设置IP映射</li>
</ul>
<pre><code class="language-shell">[root@CentOS ~]# vi /etc/hosts
127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4
::1         localhost localhost.localdomain localhost6 localhost6.localdomain6
192.168.111.132 CentOS
</code></pre>
<ul>
<li>防火墙服务</li>
</ul>
<pre><code class="language-shell">[root@CentOS ~]# service iptables stop
iptables: Setting chains to policy ACCEPT: filter          [  OK  ]
iptables: Flushing firewall rules:                         [  OK  ]
iptables: Unloading modules:                               [  OK  ]
[root@CentOS ~]# chkconfig iptables off
</code></pre>
<ul>
<li>安装JDK1.8+</li>
</ul>
<pre><code class="language-shell">[root@CentOS ~]# rpm -ivh jdk-8u191-linux-x64.rpm
warning: jdk-8u191-linux-x64.rpm: Header V3 RSA/SHA256 Signature, key ID ec551f03: NOKEY
Preparing...                ########################################### [100%]
   1:jdk1.8                 ########################################### [100%]
Unpacking JAR files...
        tools.jar...
        plugin.jar...
        javaws.jar...
        deploy.jar...
        rt.jar...
        jsse.jar...
        charsets.jar...
        localedata.jar...
[root@CentOS ~]# vi .bashrc 
JAVA_HOME=/usr/java/latest
PATH=$PATH:$JAVA_HOME/bin
CLASSPATH=.
export JAVA_HOME
export PATH
export CLASSPATH
[root@CentOS ~]# source ~/.bashrc
</code></pre>
<ul>
<li>SSH配置免密</li>
</ul>
<pre><code class="language-shell">[root@CentOS ~]# ssh-keygen -t rsa
Generating public/private rsa key pair.
Enter file in which to save the key (/root/.ssh/id_rsa):
Created directory '/root/.ssh'.
Enter passphrase (empty for no passphrase):
Enter same passphrase again:
Your identification has been saved in /root/.ssh/id_rsa.
Your public key has been saved in /root/.ssh/id_rsa.pub.
The key fingerprint is:
a5:2d:f5:c3:22:83:cf:13:25:59:fb:c1:f4:63:06:d4 root@CentOS
The key's randomart image is:
+--[ RSA 2048]----+
|          ..+.   |
|         o + oE  |
|        o = o =  |
|       . B + + . |
|      . S o =    |
|       o = . .   |
|        +        |
|         .       |
|                 |
+-----------------+
[root@CentOS ~]# ssh-copy-id CentOS
The authenticity of host 'centos (192.168.111.132)' can't be established.
RSA key fingerprint is fa:1b:c0:23:86:ff:08:5e:83:ba:65:4c:e6:f2:1f:3b.
Are you sure you want to continue connecting (yes/no)? yes
Warning: Permanently added 'centos,192.168.111.132' (RSA) to the list of known hosts.
root@centos's password:`需要输入密码`
Now try logging into the machine, with &quot;ssh 'CentOS'&quot;, and check in:

  .ssh/authorized_keys

to make sure we haven't added extra keys that you weren't expecting.
</code></pre>
<ul>
<li>配置HDFS</li>
</ul>
<p>将<code>hadoop-2.9.2.tar.gz</code>解压到系统的<code>/usr</code>目录下然后配置[core|hdfs]-site.xml配置文件。</p>
<pre><code class="language-shell">[root@CentOS ~]# vi /usr/hadoop-2.9.2/etc/hadoop/core-site.xml
</code></pre>
<pre><code class="language-xml">&lt;!--nn访问入口--&gt;
&lt;property&gt;
    &lt;name&gt;fs.defaultFS&lt;/name&gt;
    &lt;value&gt;hdfs://CentOS:9000&lt;/value&gt;
&lt;/property&gt;
&lt;!--hdfs工作基础目录--&gt;
&lt;property&gt;
    &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;
    &lt;value&gt;/usr/hadoop-2.9.2/hadoop-${user.name}&lt;/value&gt;
&lt;/property&gt;
</code></pre>
<pre><code class="language-shell">[root@CentOS ~]# vi /usr/hadoop-2.9.2/etc/hadoop/hdfs-site.xml
</code></pre>
<pre><code class="language-xml">&lt;!--block副本因子--&gt;
&lt;property&gt;
    &lt;name&gt;dfs.replication&lt;/name&gt;
    &lt;value&gt;1&lt;/value&gt;
&lt;/property&gt;
&lt;!--配置Sencondary namenode所在物理主机--&gt;
&lt;property&gt;
    &lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt;
    &lt;value&gt;CentOS:50090&lt;/value&gt;
&lt;/property&gt;
&lt;!--设置datanode最大文件操作数--&gt;
&lt;property&gt;
        &lt;name&gt;dfs.datanode.max.xcievers&lt;/name&gt;
        &lt;value&gt;4096&lt;/value&gt;
&lt;/property&gt;
&lt;!--设置datanode并行处理能力--&gt;
&lt;property&gt;
        &lt;name&gt;dfs.datanode.handler.count&lt;/name&gt;
        &lt;value&gt;6&lt;/value&gt;
&lt;/property&gt;
</code></pre>
<pre><code class="language-shell">[root@CentOS ~]# vi /usr/hadoop-2.9.2/etc/hadoop/slaves
</code></pre>
<pre><code class="language-tex">CentOS
</code></pre>
<pre><code class="language-shell">[root@CentOS ~]# vi /usr/hadoop-2.9.2/etc/hadoop/yarn-site.xml
</code></pre>
<pre><code class="language-xml">&lt;!--配置MapReduce计算框架的核心实现Shuffle-洗牌--&gt;
&lt;property&gt;
    &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;
    &lt;value&gt;mapreduce_shuffle&lt;/value&gt;
&lt;/property&gt;
&lt;!--配置资源管理器所在的目标主机--&gt;
&lt;property&gt;
    &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt;
    &lt;value&gt;CentOS&lt;/value&gt;
&lt;/property&gt;
&lt;!--关闭物理内存检查--&gt;
&lt;property&gt;
        &lt;name&gt;yarn.nodemanager.pmem-check-enabled&lt;/name&gt;
        &lt;value&gt;false&lt;/value&gt;
&lt;/property&gt;
&lt;!--关闭虚拟内存检查--&gt;
&lt;property&gt;
        &lt;name&gt;yarn.nodemanager.vmem-check-enabled&lt;/name&gt;
        &lt;value&gt;false&lt;/value&gt;
&lt;/property&gt;
</code></pre>
<pre><code class="language-shell">[root@CentOS ~]# vi /usr/hadoop-2.9.2/etc/hadoop/mapred-site.xml
</code></pre>
<pre><code class="language-xml">&lt;!--MapRedcue框架资源管理器的实现--&gt;
&lt;property&gt;
    &lt;name&gt;mapreduce.framework.name&lt;/name&gt;
    &lt;value&gt;yarn&lt;/value&gt;
&lt;/property&gt;
</code></pre>
<ul>
<li>配置hadoop环境变量</li>
</ul>
<pre><code class="language-shell">[root@CentOS ~]# vi .bashrc
HADOOP_HOME=/usr/hadoop-2.9.2
JAVA_HOME=/usr/java/latest
PATH=$PATH:$JAVA_HOME/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin
CLASSPATH=.
export JAVA_HOME
export PATH
export CLASSPATH
export HADOOP_HOME
[root@CentOS ~]# source .bashrc

</code></pre>
<ul>
<li>启动Hadoop服务</li>
</ul>
<pre><code class="language-shell">[root@CentOS ~]# hdfs namenode -format # 创建初始化所需的fsimage文件
[root@CentOS ~]# start-all.sh
</code></pre>
<ol start="2">
<li><strong>Spark环境</strong></li>
</ol>
<p>下载<code>spark-2.4.3-bin-without-hadoop.tgz</code>解压到<code>/usr</code>目录,并且将Spark目录修改名字为<code>spark-2.4.3</code>然后修改<code>spark-env.sh</code>和<code>spark-default.conf</code>文件.</p>
<p>下载地址：http://mirrors.tuna.tsinghua.edu.cn/apache/spark/spark-2.4.3/spark-2.4.3-bin-without-hadoop.tgz</p>
<ul>
<li>解压Spark安装包，并且修改解压文件名</li>
</ul>
<pre><code class="language-shell">[root@CentOS ~]# tar -zxf spark-2.4.3-bin-without-hadoop.tgz -C /usr/
[root@CentOS ~]# mv /usr/spark-2.4.3-bin-without-hadoop/ /usr/spark-2.4.3
[root@CentOS ~]# vi .bashrc
SPARK_HOME=/usr/spark-2.4.3
HADOOP_HOME=/usr/hadoop-2.9.2
JAVA_HOME=/usr/java/latest
PATH=$PATH:$JAVA_HOME/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$SPARK_HOME/bin:$SPARK_HOME/sbin
CLASSPATH=.
export JAVA_HOME
export PATH
export CLASSPATH
export HADOOP_HOME
export SPARK_HOME
[root@CentOS ~]# source .bashrc
</code></pre>
<ul>
<li>配置Spark服务</li>
</ul>
<pre><code class="language-shell">[root@CentOS spark-2.4.3]# cd /usr/spark-2.4.3/conf/
[root@CentOS conf]# mv spark-env.sh.template spark-env.sh
[root@CentOS conf]# vi spark-env.sh
HADOOP_CONF_DIR=/usr/hadoop-2.9.2/etc/hadoop
YARN_CONF_DIR=/usr/hadoop-2.9.2/etc/hadoop
SPARK_EXECUTOR_CORES=4
SPARK_EXECUTOR_MEMORY=2G
SPARK_DRIVER_MEMORY=1G
LD_LIBRARY_PATH=/usr/hadoop-2.9.2/lib/native
SPARK_DIST_CLASSPATH=$(hadoop classpath):$SPARK_DIST_CLASSPATH
SPARK_HISTORY_OPTS=&quot;-Dspark.history.fs.logDirectory=hdfs:///spark-logs&quot;

export HADOOP_CONF_DIR
export YARN_CONF_DIR
export SPARK_EXECUTOR_CORES
export SPARK_DRIVER_MEMORY
export SPARK_EXECUTOR_MEMORY
export LD_LIBRARY_PATH
export SPARK_DIST_CLASSPATH
export SPARK_HISTORY_OPTS

[root@CentOS conf]# mv spark-defaults.conf.template spark-defaults.conf
[root@CentOS conf]# vi spark-defaults.conf
spark.eventLog.enabled=true
spark.eventLog.dir=hdfs:///spark-logs
</code></pre>
<p>在HDFS上创建<code>spark-logs</code>目录，用于作为Sparkhistory服务器存储数据的地方。</p>
<pre><code class="language-shell">[root@CentOS ~]# hdfs dfs -mkdir /spark-logs
</code></pre>
<ul>
<li>启动Spark历史服务器(可选)</li>
</ul>
<pre><code class="language-shell">[root@CentOS spark-2.4.3]# ./sbin/start-history-server.sh
starting org.apache.spark.deploy.history.HistoryServer, logging to /usr/spark-2.4.3/logs/spark-root-org.apache.spark.deploy.history.HistoryServer-1-CentOS.out
[root@CentOS spark-2.4.3]# jps
5728 NodeManager
5090 NameNode
5235 DataNode
10531 Jps
5623 ResourceManager
5416 SecondaryNameNode
10459 HistoryServer

</code></pre>
<blockquote>
<p>该进程启动一个内嵌的web ui端口是18080，用户可以访问改页面查看任务执行计划、历史。</p>
</blockquote>
<ul>
<li>测试Spark</li>
</ul>
<pre><code class="language-shell">./bin/spark-shell 
	--master yarn 
	--deploy-mode client 
	--num-executors 2 
	--executor-cores 3
</code></pre>
<blockquote>
<p><code>--num-executors</code>：在Yarn模式下，表示向NodeManager申请的资源数进程，<br>
<code>--executor-cores</code>表示每个进程所能运行线程数。<br>
真个任务计算资源= <code>num-executors * executor-core</code></p>
</blockquote>
<pre><code class="language-java">scala&gt; sc.textFile(&quot;hdfs:///words/t_words&quot;,5)
    .flatMap(_.split(&quot; &quot;))
    .map((_,1))
    .reduceByKey(_+_)
    .sortBy(_._1,true,3)
    .saveAsTextFile(&quot;hdfs:///results&quot;)

</code></pre>
<ul>
<li>上述任务的任务划分图<img src="https://img-blog.csdnimg.cn/20200521091516540.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L00yODM1OTIzMzg=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述" loading="lazy"></li>
<li>本地仿真</li>
</ul>
<p>在该种模式下，无需安装yarn、无需启动Stanalone，一切都是模拟。</p>
<pre><code class="language-shell">[root@CentOS spark-2.4.3]# ./bin/spark-shell --master local[5]
Setting default log level to &quot;WARN&quot;.
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
Spark context Web UI available at http://CentOS:4040
Spark context available as 'sc' (master = local[5], app id = local-1561742649329).
Spark session available as 'spark'.
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /___/ .__/\_,_/_/ /_/\_\   version 2.4.3
      /_/

Using Scala version 2.11.12 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_191)
Type in expressions to have them evaluated.
Type :help for more information.

scala&gt; sc.textFile(&quot;hdfs:///words/t_words&quot;).flatMap(_.split(&quot; &quot;)).map((_,1)).reduceByKey(_+_).sortBy(_._1,false,3).saveAsTextFile(&quot;hdfs:///results1/&quot;)

scala&gt;

</code></pre>
<h2 id="spark-开发环境构建">Spark 开发环境构建</h2>
<ul>
<li>引入开发所需依赖</li>
</ul>
<pre><code class="language-xml">&lt;dependencies&gt;
    &lt;!-- https://mvnrepository.com/artifact/org.apache.spark/spark-core --&gt;
    &lt;dependency&gt;
        &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;
        &lt;artifactId&gt;spark-core_2.11&lt;/artifactId&gt;
        &lt;version&gt;2.4.3&lt;/version&gt;
    &lt;/dependency&gt;
&lt;/dependencies&gt;
&lt;build&gt;
    &lt;plugins&gt;
        &lt;!--scala编译插件--&gt;
        &lt;plugin&gt;
            &lt;groupId&gt;net.alchim31.maven&lt;/groupId&gt;
            &lt;artifactId&gt;scala-maven-plugin&lt;/artifactId&gt;
            &lt;version&gt;4.0.1&lt;/version&gt;
            &lt;executions&gt;
                &lt;execution&gt;
                    &lt;id&gt;scala-compile-first&lt;/id&gt;
                    &lt;phase&gt;process-resources&lt;/phase&gt;
                    &lt;goals&gt;
                        &lt;goal&gt;add-source&lt;/goal&gt;
                        &lt;goal&gt;compile&lt;/goal&gt;
                    &lt;/goals&gt;
                &lt;/execution&gt;
            &lt;/executions&gt;
        &lt;/plugin&gt;
    &lt;/plugins&gt;
&lt;/build&gt;
</code></pre>
<h3 id="sparkrddwordcount本地">SparkRDDWordCount(本地)</h3>
<pre><code class="language-java">   //1.创建SparkContext
    val conf = new SparkConf().setMaster(&quot;local[10]&quot;).setAppName(&quot;wordcount&quot;)
    val sc = new SparkContext(conf)

    val lineRDD: RDD[String] = sc.textFile(&quot;file:///Users/mashikang/demo/words/t_word.txt&quot;)
    lineRDD.flatMap(line=&gt;line.split(&quot; &quot;))
        .map(word=&gt;(word,1))
        .groupByKey()
        .map(tuple=&gt;(tuple._1,tuple._2.sum))
        .sortBy(tuple=&gt;tuple._2,false,1)
        .collect()
        .foreach(tuple=&gt;println(tuple._1+&quot;-&gt;&quot;+tuple._2))

    //3.关闭sc
    sc.stop()
</code></pre>
<h3 id="集群yarn">集群(yarn)</h3>
<pre><code class="language-java">//1.创建SparkContext
val conf = new SparkConf().setMaster(&quot;yarn&quot;).setAppName(&quot;wordcount&quot;)
val sc = new SparkContext(conf)

val lineRDD: RDD[String] = sc.textFile(&quot;hdfs:///words/t_words&quot;)
lineRDD.flatMap(line=&gt;line.split(&quot; &quot;))
.map(word=&gt;(word,1))
.groupByKey()
.map(tuple=&gt;(tuple._1,tuple._2.sum))
.sortBy(tuple=&gt;tuple._2,false,1)
.collect()
.foreach(tuple=&gt;println(tuple._1+&quot;-&gt;&quot;+tuple._2))

//3.关闭sc
sc.stop()
</code></pre>
<p>发布：</p>
<pre><code class="language-shell">[root@CentOS spark-2.4.3]# ./bin/spark-submit --master yarn --deploy-mode client --class com.mask.demo02.SparkRDDWordCount --num-executors 3 --executor-cores 4 /root/sparkrdd-1.0-SNAPSHOT.jar
</code></pre>
<h3 id="集群standalone">集群(standalone)</h3>
<pre><code class="language-java">//1.创建SparkContext
val conf = new SparkConf().setMaster(&quot;spark://CentOS:7077&quot;).setAppName(&quot;wordcount&quot;)
val sc = new SparkContext(conf)

val lineRDD: RDD[String] = sc.textFile(&quot;hdfs:///words/t_words&quot;)
lineRDD.flatMap(line=&gt;line.split(&quot; &quot;))
.map(word=&gt;(word,1))
.groupByKey()
.map(tuple=&gt;(tuple._1,tuple._2.sum))
.sortBy(tuple=&gt;tuple._2,false,1)
.collect()
.foreach(tuple=&gt;println(tuple._1+&quot;-&gt;&quot;+tuple._2))

//3.关闭sc
sc.stop()
</code></pre>
<p>发布：</p>
<pre><code class="language-shell">[root@CentOS spark-2.4.3]# ./bin/spark-submit --master spark://CentOS:7077 --deploy-mode client --class com.mask.demo02.SparkRDDWordCount --num-executors 3 --total-executor-cores 4 /root/sparkrdd-1.0-SNAPSHOT.jar
</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Spark入门(一)——Spark的“前世今生”]]></title>
        <id>https://mask0407.github.io/spark00/</id>
        <link href="https://mask0407.github.io/spark00/">
        </link>
        <updated>2020-06-29T03:10:42.000Z</updated>
        <content type="html"><![CDATA[<p><ul class="markdownIt-TOC">
<li>
<ul>
<li><a href="#spark%E7%AE%80%E4%BB%8B">Spark简介</a></li>
<li><a href="#%E8%AE%A1%E7%AE%97%E6%B5%81%E7%A8%8B">计算流程</a></li>
</ul>
</li>
</ul>
(Spark的诞生)</p>
<h2 id="spark简介">Spark简介</h2>
<blockquote>
<p>Spark 是一个用来实现快速而通用的集群计算的平台。<br>
在速度方面，Spark 扩展了广泛使用的 MapReduce 计算模型，而且高效地支持更多计算模式，包括交互式查询和流处理。在处理大规模数据集时，速度是非常重要的。速度快就意味着我们可以进行交互式的数据操作，否则我们每次操作就需要等待数分钟甚至数小时。Spark 的一个主要特点就是能够在内存中进行计算，因而更快。不过即使是必须在磁盘上进行的复杂计算，Spark 依然比 MapReduce 更加高效。<br>
总的来说，Spark 适用于各种各样原先需要多种不同的分布式平台的场景，包括批处理、迭代算法、交互式查询、流处理。通过在一个统一的框架下支持这些不同的计算，Spark使我们可以简单而低耗地把各种处理流程整合在一起。而这样的组合，在实际的数据分析过程中是很有意义的。不仅如此，Spark 的这种特性还大大减轻了原先需要对各种平台别管理的负担。Spark 所提供的接口非常丰富。除了提供基于 Python、Java、Scala 和 SQL 的简单易用的API 以及内建的丰富的程序库以外，Spark 还能和其他大数据工具密切配合使用。例如，Spark 可以运行在 Hadoop 集群上，访问包括 Cassandra 在内的任意 Hadoop 数据源。</p>
</blockquote>
<ul>
<li><strong>总结</strong></li>
</ul>
<blockquote>
<p>Spark是一个快如闪电的统一分析引擎（计算框架）用于大规模数据集的处理。Spark在做数据的批处理计算，计算性能大约是Hadoop MapReduce的10~100倍，因为Spark使用比较先进的基于DAG 任务调度，可以将一个任务拆分成若干个阶段，然后将这些阶段分批次交给集群计算节点处理。</p>
</blockquote>
<ul>
<li><strong>MapReduce VS Spark</strong></li>
</ul>
<blockquote>
<p>MapReduce作为第一代大数据处理框架，在设计初期只是为了满足基于海量数据级的海量数据计算的迫切需求。自2006年剥离自Nutch（Java搜索引擎）工程，主要解决的是早期人们对大数据的初级认知所面临的问题。</p>
</blockquote>
<figure data-type="image" tabindex="1"><img src="https://img-blog.csdnimg.cn/2020052108453267.jpg" alt="在这里插入图片描述" loading="lazy"></figure>
<blockquote>
<p>整个MapReduce的计算实现的是基于磁盘的IO计算，随着大数据技术的不断普及，人们开始重新定义大数据的处理方式，不仅仅满足于能在合理的时间范围内完成对大数据的计算，还对计算的实效性提出了更苛刻的要求，因为人们开始探索使用Map Reduce计算框架完成一些复杂的高阶算法，往往这些算法通常不能通过1次性的Map Reduce迭代计算完成。由于Map Reduce计算模型总是把结果存储到磁盘中，每次迭代都需要将数据磁盘加载到内存，这就为后续的迭代带来了更多延长。</p>
</blockquote>
<p>2009年Spark在加州伯克利AMP实验室诞生，2010首次开源后该项目就受到很多开发人员的喜爱，2013年6月份开始在Apache孵化，2014年2月份正式成为Apache的顶级项目。Spark发展如此之快是因为Spark在计算层方面明显优于Hadoop的Map Reduce这<code>磁盘迭代计算</code>，因为Spark可以使用<code>内存对数据做计算</code>，而且计算的中间结果也可以缓存在内存中，这就为后续的迭代计算节省了时间，大幅度的提升了针对于海量数据的计算效率。<br>
<img src="https://img-blog.csdnimg.cn/20200521084652985.jpg" alt="在这里插入图片描述" loading="lazy"><br>
Spark也给出了在使用MapReduce和Spark做<code>线性回归计算</code>（算法实现需要n次迭代）上，Spark的速率几乎是MapReduce计算<code>10~100倍</code>这种计算速度。<br>
<img src="https://img-blog.csdnimg.cn/20200521084903362.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L00yODM1OTIzMzg=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述" loading="lazy"><br>
不仅如此Spark在设计理念中也提出了<code>One stack ruled them all</code>战略，并且提供了基于Spark批处理至上的计算服务分支例如:实现基于Spark的交互查询、近实时流处理、机器学习、Grahx 图形关系存储等。<br>
<img src="https://img-blog.csdnimg.cn/20200521084729523.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L00yODM1OTIzMzg=,size_16,color_FFFFFF,t_70" alt="" loading="lazy"><br>
从图中不难看出Apache Spark处于<code>计算层</code>，Spark项目在战略上启到了承上启下的作用，并没有废弃原有以hadoop为主体的大数据解决方案。因为Spark向下可以计算来自于HDFS、HBase、Cassandra和亚马逊S3文件服务器的数据，也就意味着使用Spark作为计算层，用户原有的存储层架构无需改动。</p>
<h2 id="计算流程">计算流程</h2>
<p>因为Spark计算是在MapReduce计算之后诞生，吸取了MapReduce设计经验，极大地规避了MapReduce计算过程中的诟病，先来回顾一下MapReduce计算的流程。<br>
<img src="https://img-blog.csdnimg.cn/2020052108512042.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L00yODM1OTIzMzg=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述" loading="lazy"><br>
总结一下几点缺点：</p>
<blockquote>
<p>1）MapReduce虽然基于矢量编程思想，但是计算状态过于简单，只是简单的将任务分为Map state和Reduce State，没有考虑到迭代计算场景。<br>
2）在Map任务计算的中间结果存储到本地磁盘，IO调用过多，数据读写效率差。<br>
3）MapReduce是先提交任务，然后在计算过程中申请资源。并且计算方式过于笨重。每个并行度都是由一个JVM进程来实现计算。</p>
</blockquote>
<p>通过简单的罗列不难发现MapReduce计算的诟病和问题，因此Spark在计算层面上借鉴了MapReduce计算设计的经验，提出了DGASchedule和TaskSchedual概念，打破了在MapReduce任务中一个job只用Map State和Reduce State的两个阶段，并不适合一些迭代计算次数比较多的场景。因此Spark 提出了一个比较先进的设计理念，任务状态拆分，Spark在任务计算初期首先通过DGASchedule计算任务的State，将每个阶段的Sate封装成一个TaskSet，然后由TaskSchedual将TaskSet提交集群进行计算。可以尝试将Spark计算的流程使用一下的流程图描述如下：</p>
<figure data-type="image" tabindex="2"><img src="https://img-blog.csdnimg.cn/20200520173703812.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L00yODM1OTIzMzg=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述" loading="lazy"></figure>
<p>相比较于MapReduce计算，Spark计算有以下优点：</p>
<blockquote>
<p>1）智能DAG任务拆分，将一个复杂计算拆分成若干个State，满足迭代计算场景</p>
<p>2）Spark提供了计算的缓存和容错策略，将计算结果存储在内存或者磁盘，加速每个state的运行，提升运行效率</p>
<p>3）Spark在计算初期，就已经申请好计算资源。任务并行度是通过在Executor进程中启动线程实现，相比较于MapReduce计算更加轻快。</p>
</blockquote>
<p>目前Spark提供了Cluster Manager的实现由Yarn、Standalone、Messso、kubernates等实现。其中企业常用的有Yarn和Standalone方式的管理。</p>
<ul>
<li>Application就是你写的代码。</li>
<li>Driver节点上的Driver程序运行的是main函数</li>
<li>Worker节点就是工作节点，上面运行Executor进程。</li>
<li>Executor进程负责运行Task。</li>
</ul>
]]></content>
    </entry>
</feed>