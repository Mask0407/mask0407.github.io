{"posts":[{"title":"GeoHash算法原理及实现","content":" GeoHash算法原理 样例数据（基于15次区域分割） GeoHash算法思想 GeoHash算法原理 后续问题 GeoHash算法代码实现 写在最后 (GeoHash原理与Java实现) GeoHash算法原理 GeoHash是目前比较主流实现位置服务的技术，Geohash算法将经纬度二维数据编码为一个字符串，本质是一个降维的过程 样例数据（基于15次区域分割） 位置 经纬度 Geohash 北京站 116.433589,39.910508 wx4g19 天安门 116.403874,39.913884 wx4g0f 首都机场 116.606819,40.086109 wx4uj3 GeoHash算法思想 我们知道，经度范围是东经180到西经180，纬度范围是南纬90到北纬90，我们设定西经为负，南纬为负，所以地球上的经度范围就是[-180， 180]，纬度范围就是[-90，90]。如果以本初子午线、赤道为界，地球可以分成4个部分。 GeoHash的思想就是将地球划分的四部分映射到二维坐标上。 [-90˚,0˚)代表0，(0˚,90˚]代表1，[-180˚,0)代表0，(0˚,180˚]代表1 映射到二维空间划分为四部分则如下图 但是这么粗略的划分没有什么意义，想要更精确的使用GeoHash就需要再进一步二分切割 通过上图可看出，进一步二分切割将原本大略的划分变为细致的区域划分，这样就会更加精确。GeoHash算法就是基于这种思想，递归划分的次数越多，所计算出的数据越精确。 GeoHash算法原理 GeoHash算法大体上分为三步：1. 计算经纬度的二进制、2. 合并经纬度的二进制、3. 通过Base32对合并后的二进制进行编码。 计算经纬度的二进制 //根据经纬度和范围，获取对应的二进制 private BitSet getBits(double l, double floor, double ceiling) { BitSet buffer = new BitSet(numbits); for (int i = 0; i &lt; numbits; i++) { double mid = (floor + ceiling) / 2; if (l &gt;= mid) { buffer.set(i); floor = mid; } else { ceiling = mid; } } return buffer; } 上述代码numbits为：private static int numbits = 3 * 5; //经纬度单独编码长度也就是说将地球进行15次二分切割 注： 这里需要对BitSet类进行一下剖析，没了解过该类的话指定懵。 了解BitSet只需了去了解它的set()、get()方法就足够了 BitSet的set方法 /** * Sets the bit at the specified index to {@code true}. * * @param bitIndex a bit index * @throws IndexOutOfBoundsException if the specified index is negative * @since JDK1.0 */ public void set(int bitIndex) { if (bitIndex &lt; 0) throw new IndexOutOfBoundsException(&quot;bitIndex &lt; 0: &quot; + bitIndex); int wordIndex = wordIndex(bitIndex); expandTo(wordIndex); words[wordIndex] |= (1L &lt;&lt; bitIndex); // Restores invariants checkInvariants(); } set方法内wordIndex(bitIndex)底层将bitIndex右移6位然后返回，ADDRESS_BITS_PER_WORD为常量6 /** * Given a bit index, return word index containing it. */ private static int wordIndex(int bitIndex) { return bitIndex &gt;&gt; ADDRESS_BITS_PER_WORD; } set方法内的expandTo(wordIndex)只是一个判断数组是否需要扩容的方法 /** * Ensures that the BitSet can accommodate a given wordIndex, * temporarily violating the invariants. The caller must * restore the invariants before returning to the user, * possibly using recalculateWordsInUse(). * @param wordIndex the index to be accommodated. */ private void expandTo(int wordIndex) { int wordsRequired = wordIndex+1; if (wordsInUse &lt; wordsRequired) { ensureCapacity(wordsRequired); wordsInUse = wordsRequired; } } set内重要的一行代码words[wordIndex] |= (1L &lt;&lt; bitIndex),这里只解释一下|= a|=b就是a=a|b,就是说将a、b转为二进制按位与，同0为0，否则为1 BitSet的get方法 /** * Returns the value of the bit with the specified index. The value * is {@code true} if the bit with the index {@code bitIndex} * is currently set in this {@code BitSet}; otherwise, the result * is {@code false}. * * @param bitIndex the bit index * @return the value of the bit with the specified index * @throws IndexOutOfBoundsException if the specified index is negative */ public boolean get(int bitIndex) { if (bitIndex &lt; 0) throw new IndexOutOfBoundsException(&quot;bitIndex &lt; 0: &quot; + bitIndex); checkInvariants(); int wordIndex = wordIndex(bitIndex); return (wordIndex &lt; wordsInUse) &amp;&amp; ((words[wordIndex] &amp; (1L &lt;&lt; bitIndex)) != 0); } get方法用一句话概括就是：如果传入的下标有值，返回true；反之为false 以天安门坐标为例：39.913884, 116.403874 BitSet latbits = getBits(lat, -90, 90); BitSet lonbits = getBits(lon, -180, 180); // 纬度 for (int i = 0; i &lt; numbits; i++) { System.out.print(latbits.get(i) + &quot; &quot;); } // 经度 for (int i = 0; i &lt; numbits; i++) { System.out.print(lonbits.get(i) + &quot; &quot;); } 纬度经过转换为： true false true true true false false false true true false false false true false 转为二进制： 1 0 1 1 1 0 0 0 1 1 0 0 0 1 0 经度经过转换为： true true false true false false true false true true false false false true true 转为二进制： 1 1 0 1 0 0 1 0 1 1 0 0 0 1 1 合并经纬度二进制 合并原则：经度占偶数位，纬度占奇数位。也就是说经纬度交替合并，首位0位置为经度的0位置 合并后二进制编码为： 11100 11101 00100 01111 00000 01110 使用Base32对合并后的经纬度二进制进行编码 代码实现 // Base32进行编码 public String encode(double lat, double lon) { BitSet latbits = getBits(lat, -90, 90); BitSet lonbits = getBits(lon, -180, 180); StringBuilder buffer = new StringBuilder(); for (int i = 0; i &lt; numbits; i++) { buffer.append((lonbits.get(i)) ? '1' : '0'); buffer.append((latbits.get(i)) ? '1' : '0'); } String code = base32(Long.parseLong(buffer.toString(), 2)); return code; } 本文案例经纬度编码后 wx4g0f 后续问题 如果要使用此功能实现附近的人。假如红点为使用者，经过Geohash算法分割后只会推荐同区域0011中的绿点，但是如下图所示，蓝色点相对于绿色点更接近用户，所以区域划分的弊端就展现在这里。 针对上述问题，我们可以人为获取红色用户所在的0011区域周边八个区域中的用户，即获取0011的同时还要获取0100,0110,1100,0001,1001,0000,0010,1000 代码实现 public ArrayList&lt;String&gt; getArroundGeoHash(double lat, double lon) { ArrayList&lt;String&gt; list = new ArrayList&lt;&gt;(); double uplat = lat + minLat; double downLat = lat - minLat; double leftlng = lon - minLng; double rightLng = lon + minLng; String leftUp = encode(uplat, leftlng); list.add(leftUp); String leftMid = encode(lat, leftlng); list.add(leftMid); String leftDown = encode(downLat, leftlng); list.add(leftDown); String midUp = encode(uplat, lon); list.add(midUp); String midMid = encode(lat, lon); list.add(midMid); String midDown = encode(downLat, lon); list.add(midDown); String rightUp = encode(uplat, rightLng); list.add(rightUp); String rightMid = encode(lat, rightLng); list.add(rightMid); String rightDown = encode(downLat, rightLng); list.add(rightDown); return list; } 然后根据球体两点间的距离计算红色用户与周边区域用户距离，从而进行附近的人功能实现 通过两经纬度计算距离java代码实现 static double getDistance(double lat1, double lon1, double lat2, double lon2) { // 经纬度（角度）转弧度。弧度用作参数，以调用Math.cos和Math.sin double radiansAX = Math.toRadians(lon1); // A经弧度 double radiansAY = Math.toRadians(lat1); // A纬弧度 double radiansBX = Math.toRadians(lon2); // B经弧度 double radiansBY = Math.toRadians(lat2); // B纬弧度 // 公式中“cosβ1cosβ2cos（α1-α2）+sinβ1sinβ2”的部分，得到∠AOB的cos值 double cos = Math.cos(radiansAY) * Math.cos(radiansBY) * Math.cos(radiansAX - radiansBX) + Math.sin(radiansAY) * Math.sin(radiansBY); double acos = Math.acos(cos); // 反余弦值 return EARTH_RADIUS * acos; // 最终结果 } GeoHash算法代码实现 public class GeoHash { public static final double MINLAT = -90; public static final double MAXLAT = 90; public static final double MINLNG = -180; public static final double MAXLNG = 180; private static int numbits = 3 * 5; //经纬度单独编码长度 private static double minLat; private static double minLng; private final static char[] digits = {'0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'j', 'k', 'm', 'n', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z'}; //定义编码映射关系 final static HashMap&lt;Character, Integer&gt; lookup = new HashMap&lt;Character, Integer&gt;(); //初始化编码映射内容 static { int i = 0; for (char c : digits) lookup.put(c, i++); } public GeoHash() { setMinLatLng(); } // Base32进行编码 public String encode(double lat, double lon) { BitSet latbits = getBits(lat, -90, 90); BitSet lonbits = getBits(lon, -180, 180); StringBuilder buffer = new StringBuilder(); for (int i = 0; i &lt; numbits; i++) { buffer.append((lonbits.get(i)) ? '1' : '0'); buffer.append((latbits.get(i)) ? '1' : '0'); } String code = base32(Long.parseLong(buffer.toString(), 2)); return code; } public ArrayList&lt;String&gt; getArroundGeoHash(double lat, double lon) { ArrayList&lt;String&gt; list = new ArrayList&lt;&gt;(); double uplat = lat + minLat; double downLat = lat - minLat; double leftlng = lon - minLng; double rightLng = lon + minLng; String leftUp = encode(uplat, leftlng); list.add(leftUp); String leftMid = encode(lat, leftlng); list.add(leftMid); String leftDown = encode(downLat, leftlng); list.add(leftDown); String midUp = encode(uplat, lon); list.add(midUp); String midMid = encode(lat, lon); list.add(midMid); String midDown = encode(downLat, lon); list.add(midDown); String rightUp = encode(uplat, rightLng); list.add(rightUp); String rightMid = encode(lat, rightLng); list.add(rightMid); String rightDown = encode(downLat, rightLng); list.add(rightDown); return list; } //根据经纬度和范围，获取对应的二进制 private BitSet getBits(double l, double floor, double ceiling) { BitSet buffer = new BitSet(numbits); for (int i = 0; i &lt; numbits; i++) { double mid = (floor + ceiling) / 2; if (l &gt;= mid) { buffer.set(i); floor = mid; } else { ceiling = mid; } } return buffer; } //将经纬度合并后的二进制进行指定的32位编码 private String base32(long i) { char[] buf = new char[65]; int charPos = 64; boolean negative = (i &lt; 0); if (!negative) { i = -i; } while (i &lt;= -32) { buf[charPos--] = digits[(int) (-(i % 32))]; i /= 32; } buf[charPos] = digits[(int) (-i)]; if (negative) { buf[--charPos] = '-'; } return new String(buf, charPos, (65 - charPos)); } private void setMinLatLng() { minLat = MAXLAT - MINLAT; for (int i = 0; i &lt; numbits; i++) { minLat /= 2.0; } minLng = MAXLNG - MINLNG; for (int i = 0; i &lt; numbits; i++) { minLng /= 2.0; } } //根据二进制和范围解码 private double decode(BitSet bs, double floor, double ceiling) { double mid = 0; for (int i = 0; i &lt; bs.length(); i++) { mid = (floor + ceiling) / 2; if (bs.get(i)) floor = mid; else ceiling = mid; } return mid; } //对编码后的字符串解码 public double[] decode(String geohash) { StringBuilder buffer = new StringBuilder(); for (char c : geohash.toCharArray()) { int i = lookup.get(c) + 32; buffer.append(Integer.toString(i, 2).substring(1)); } BitSet lonset = new BitSet(); BitSet latset = new BitSet(); //偶数位，经度 int j = 0; for (int i = 0; i &lt; numbits * 2; i += 2) { boolean isSet = false; if (i &lt; buffer.length()) isSet = buffer.charAt(i) == '1'; lonset.set(j++, isSet); } //奇数位，纬度 j = 0; for (int i = 1; i &lt; numbits * 2; i += 2) { boolean isSet = false; if (i &lt; buffer.length()) isSet = buffer.charAt(i) == '1'; latset.set(j++, isSet); } double lon = decode(lonset, -180, 180); double lat = decode(latset, -90, 90); return new double[]{lat, lon}; } public static void main(String[] args) { GeoHash geoHash = new GeoHash(); // 北京站 String encode = geoHash.encode(39.910508, 116.433589); System.out.println(encode); // 天安门 System.out.println(geoHash.encode(39.913884, 116.403874)); // 首都机场 System.out.println(geoHash.encode(40.086109, 116.606819)); BitSet latbits = geoHash.getBits(39.913884, -90, 90); BitSet lonbits = geoHash.getBits(116.403874, -180, 180); // for (int i=0; i&lt; latbits.length(); i++) { // System.out.println(latbits.get(i)); // } for (int i = 0; i &lt; numbits; i++) { // System.out.print(latbits.get(i)); System.out.print(latbits.get(i) ? '1' : '0'); System.out.print(&quot; &quot;); } System.out.println(); StringBuilder buffer = new StringBuilder(); for (int i = 0; i &lt; numbits; i++) { buffer.append((lonbits.get(i)) ? '1' : '0'); buffer.append((latbits.get(i)) ? '1' : '0'); } System.out.println(buffer.toString()); System.out.println(geoHash.encode(39.913884, 116.403874)); } } 写在最后 如果嫌GeoHash算法麻烦，但是还想用它，没关系。 Redis知道你懒Redis官网GeoHash用法 ","link":"https://mask0407.github.io/suanfa/"},{"title":"Spark入门( 九)——机器学习 Spark MLlib","content":" 机器学习是什么？ 机器学习 Spark MLlib Spark MLlib案例 快速入门 基本统计 Correlation(相关性) Hypothesis testing(假设检验) Summarizer(总结器) 未完待续。。。 (Spark MLlib) 机器学习是什么？ 机器学习 数据挖掘有着50多年的发展历史。机器学习就是其子领域之一，特点是利用大型计算机集群来从海量数据中分析和提取知识 机器学习与计算统计学密切相关。它与数学优化紧密关联，为其提供方法、理论和应用领域。机器学习在各种传统设计和编程不能胜任的计算机任务中有广泛应用。典型的应用如垃圾邮件过滤、光学字符识别(OCR)、搜索引擎和计算机视觉。机器学习有时和数据挖掘联用，但更偏向探索性数据分析，亦称为无监督学习。 与学习系统的可用输入自然属性不同，机器学习系统可分为3种。学习算法发现输入数据的内在结构。它可以有目标(隐含模式),也可以是发现特征的一种途径。 无监督学习 学习系统的输入数据中并不包含对应的标签(或期望的输出)，它需要自行从输入中找到输入数据的内在结构 监督学习 系统已知各输入对应的期望输出系统的目标是学习如何将输入映射到输出 强化学习 系统与环境进行交互，它有已定义的目标，但没有人类显式地告知其是否正在接近目标 Spark MLlib MLlib是Spark的机器学习（ML）库。其目标是使实用的机器学习可扩展且容易。在较高级别，它提供了以下工具： ML算法：常见的学习算法，例如分类，回归，聚类和协同过滤 特征化：特征提取，变换，降维和选择 管道：用于构建，评估和调整ML管道的工具 持久性：保存和加载算法，模型和管道 实用程序：线性代数，统计信息，数据处理等。 Spark MLlib案例 基于DataFrame的API是主要API 快速入门 pom.xml &lt;!-- https://mvnrepository.com/artifact/org.apache.spark/spark-mllib --&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-mllib_2.11&lt;/artifactId&gt; &lt;version&gt;2.2.1&lt;/version&gt; &lt;/dependency&gt; def main(args: Array[String]): Unit = { // 屏蔽日志 Logger.getLogger(&quot;org.apache.spark&quot;).setLevel(Level.WARN) Logger.getLogger(&quot;org.apache.jetty.server&quot;).setLevel(Level.OFF) val spark = SparkSession .builder() .master(&quot;local[*]&quot;) .appName(Demo01.getClass.getName) .getOrCreate() import spark.implicits._ val data = Seq( /** * 稀疏向量表示方式 * 4, Seq((0, 1.0), (3, -2.0)) * 表示向量长度为4 有两个非0位置：0和3位置，0和3的值分别为1.0、-2.0 * 该向量可表示为(1.0, 0, 0, -2.0) */ Vectors.sparse(4, Seq((0, 1.0), (3, -2.0))), // 密集向量表示方式 Vectors.dense(4.0, 5.0, 0.0, 3.0), Vectors.dense(6.0, 7.0, 0.0, 8.0), Vectors.sparse(4, Seq((0, 9.0), (3, 1.0))) ) val df = data.map(Tuple1.apply).toDF(&quot;features&quot;) df.show() // 计算df features列的相关性 val Row(coeff1: Matrix) = Correlation.corr(df, &quot;features&quot;).head println(s&quot;Pearson correlation matrix:\\n $coeff1&quot;) val Row(coeff2: Matrix) = Correlation.corr(df, &quot;features&quot;, &quot;spearman&quot;).head println(s&quot;Spearman correlation matrix:\\n $coeff2&quot;) spark.stop() } 基本统计 Correlation(相关性) Correlation 使用指定的方法为向量的输入数据集计算相关矩阵。输出将是一个DataFrame，其中包含向量列的相关矩阵。 皮尔森系数公式： 当两个变量的线性关系增强时，相关系数趋于1或-1。正相关时趋于1，负相关时趋于-1。当两个变量独立时相关系统为0，但反之不成立。当Y和X服从联合正态分布时，其相互独立和不相关是等价的 val data = Seq( /** * 稀疏向量表示方式 * 4, Seq((0, 1.0), (3, -2.0)) * 表示向量长度为4 有两个非0位置：0和3位置，0和3的值分别为1.0、-2.0 * 该向量可表示为(1.0, 0, 0, -2.0) */ Vectors.sparse(4, Seq((0, 1.0), (3, -2.0))), // 密集向量表示方式 Vectors.dense(4.0, 5.0, 0.0, 3.0), Vectors.dense(6.0, 7.0, 0.0, 8.0), Vectors.sparse(4, Seq((0, 9.0), (3, 1.0))) ) val df = data.map(Tuple1.apply).toDF(&quot;features&quot;) df.show() // 计算features的相关性, method系数默认为Pearson val Row(coeff1: Matrix) = Correlation.corr(df, &quot;features&quot;).head println(s&quot;Pearson correlation matrix:\\n $coeff1&quot;) // 计算features的相关性, method系数为Spearson val Row(coeff2: Matrix) = Correlation.corr(df, &quot;features&quot;, &quot;spearman&quot;).head println(s&quot;Spearman correlation matrix:\\n $coeff2&quot;) Hypothesis testing(假设检验) 假设检验是一种强大的统计工具，可用来确定结果是否具有统计学意义，以及该结果是否偶然发生。spark.ml当前支持Pearson的卡方（数学处理错误）测试独立性。 ChiSquareTest针对标签上的每个功能进行Pearson的独立性测试。对于每个要素，（要素，标签）对将转换为列联矩阵，针对该列矩阵计算卡方统计量。所有标签和特征值必须是分类的。 import org.apache.spark.ml.linalg.{Vector, Vectors} import org.apache.spark.ml.stat.ChiSquareTest val data = Seq( (0.0, Vectors.dense(0.5, 10.0)), (0.0, Vectors.dense(1.5, 20.0)), (1.0, Vectors.dense(1.5, 30.0)), (0.0, Vectors.dense(3.5, 30.0)), (0.0, Vectors.dense(3.5, 40.0)), (1.0, Vectors.dense(3.5, 40.0)) ) val df = data.toDF(&quot;label&quot;, &quot;features&quot;) df.show() val chi = ChiSquareTest.test(df, &quot;features&quot;, &quot;label&quot;).head println(s&quot;pValues = ${chi.getAs[Vector](0)}&quot;) println(s&quot;degreesOfFreedom ${chi.getSeq[Int](1).mkString(&quot;[&quot;, &quot;,&quot;, &quot;]&quot;)}&quot;) println(s&quot;statistics ${chi.getAs[Vector](2)}&quot;) Summarizer(总结器) import spark.implicits._ import org.apache.spark.ml.stat.Summarizer._ val data = Seq( (Vectors.dense(2.0, 3.0, 5.0), 1.0), (Vectors.dense(4.0, 6.0, 7.0), 2.0) ) val df = data.toDF(&quot;features&quot;, &quot;weight&quot;) val (meanVal, varianceVal) = df.select(metrics(&quot;mean&quot;, &quot;variance&quot;) .summary($&quot;features&quot;, $&quot;weight&quot;).as(&quot;summary&quot;)) .select(&quot;summary.mean&quot;, &quot;summary.variance&quot;) .as[(Vector, Vector)].first() println(s&quot;with weight: mean = ${meanVal}, variance = ${varianceVal}&quot;) val (meanVal2, varianceVal2) = df.select(mean($&quot;features&quot;), variance($&quot;features&quot;)) .as[(Vector, Vector)].first() println(s&quot;without weight: mean = ${meanVal2}, sum = ${varianceVal2}&quot;) 未完待续。。。 ","link":"https://mask0407.github.io/spark08/"},{"title":"Spark入门( 八)——Spark流计算新玩法-Structured Streaming","content":" 简介 快速入门案例 程序流程结构 基本概念 故障容错 Structured Streaming API Input Sources File Source Socket source(debug) Kafka source Output Sink File sink(Append Mode Only) KafkaSink((Append|Update|Complete)) Foreach sink(Append|Update|Complate) UserRowWriter Window on Event Time 处理延迟 Data 和 Watermarking Watermarking保障机制： Spark清除window聚合状态条件 Update Mode Append Mode Join 操作 Stream-static Joins Stream-stream Joins inner join outer join (Structured Streaming) 简介 Structured Streaming 构建在SparkSQL之上的流处理引擎。可以使用户继续使用DataSet/dataFrame操 作流数据。并且提供了多种计算模型可供选择，默认情况下，使用的依然是Spark的marco batch这种计 算模型能够到100ms左右的end-to-end的精准一次的容错计算。除此之外也提供了基于EventTime语义 的窗口计算（DStream 基于Processor Time不同）。同时在spark-2.3版本又提出新的计算模型 Continuous Processing可以达到1ms左右的精准一次的容错计算。 快速入门案例 pom &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-sql_2.11&lt;/artifactId&gt; &lt;version&gt;2.4.3&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-core_2.11&lt;/artifactId&gt; &lt;version&gt;2.4.3&lt;/version&gt; &lt;/dependency&gt; WordCount def main(args: Array[String]): Unit = { Logger.getLogger(&quot;org.apache.spark&quot;).setLevel(Level.WARN) Logger.getLogger(&quot;org.apache.jetty.server&quot;).setLevel(Level.OFF) //1.构建SparkSession val spark = SparkSession.builder() .appName(&quot;wordcount&quot;) .master(&quot;local[*]&quot;) .getOrCreate() import spark.implicits._ //2.创建输入流-readStream var lines = spark.readStream .format(&quot;socket&quot;) .option(&quot;host&quot;, &quot;localhost&quot;) .option(&quot;port&quot;, 9999) .load() //3.对dataframe实现转换 var wordCounts = lines.as[String] .flatMap(_.split(&quot;\\\\s+&quot;)) .groupBy(&quot;value&quot;) .count() //4.构建query 输出 val query = wordCounts.writeStream .format(&quot;console&quot;) .outputMode(OutputMode.Update()) //有状态持续计算 Complete| Update| Append .start() //5.等待流结束 query.awaitTermination() } 有状态持续计算 Complete| Update| Append 之间的区别 Complete: 每一个trigger到来时，就输出整个完整的dataframe Update: 只输出那些被修改的Row。 每一次window sliding，就去跟原来的结果比较，有变化就输出 Append: 只输出新添加的（原来没有的）Row（）（如果是groupby，要有watermark才可以） 每当一个watermark时间结束了，这个临时的结果再回转换成正式的结果并导出。 nc -l 999 输入 aa bb cc aa cc aa aa aa 输出结果(由于使用了Update 第二次输入没有bb，所有Batch: 2没有bb输出) ------------------------------------------- Batch: 1 ------------------------------------------- +-----+-----+ |value|count| +-----+-----+ | cc| 1| | bb| 1| | aa| 2| +-----+-----+ ------------------------------------------- Batch: 2 ------------------------------------------- +-----+-----+ |value|count| +-----+-----+ | cc| 2| | aa| 5| +-----+-----+ 程序流程结构 1.构建SparkSession 对象 2.借助于SparkSession#readStream加载动态的Dataframe 3.使用DataFrame API或者是SQL语句 实现对动态数据计算 4.通过DataFrame#writeStream方法构建StreamQuery对象 5.调用StreamQuery#awaitTermination等待关闭指令 基本概念 Structure Stream的核心思想是通过将实时数据流看成是一个持续插入table.因此用户就可以使用SQL查 询DynamicTable|UnboundedTable。底层Spark通过StreamQuery实现对数据持续计算。 当对Input执行转换的时候系统产生一张结果表 ResultTable ,当有新的数据产生的时候，系统会往 Input Table 插入一行数据，这会最终导致系统更新 ResultTable ,每一次的更新系统将更新的数 据写到外围系统-Sink. Output 定义如何将Result写出到外围系统，目前Spark支持三种输出模式：(上面已经简单介绍过了) Complete Mode - 整个ResultTable的数据会被写到外围系统。 Update Mode - 只会将ResultTable中被更新的行，写到外围系统（ spark-2.1.1 +支持） Append Mode - 只有新数据插入ResultTable的时候，才会将结果输出。注意：这种模式只适用 于被插入结果表的数据都是只读的情况下，才可以将输出模式定义为Append（查询当中不应该出 现聚合算子，当然也有特例，例如流中声明watermarker） 由于Structure Streaming计算的特点，Spark会在内存当中存储程序计算中间状态用于生产结果表的数 据，Spark并不会存储 Input Table 的数据，一旦处理完成之后，读取的数据会被丢弃。整个聚合的 过程无需用户干预（对比Storm，Storm状态管理需要将数据写到外围系统）。 故障容错 Structured Streaming通过checkpoint和write ahead log去记录每一次批处理的数据源的偏移量（区 间），可以保证在失败的时候可以重复的读取数据源。其次Structure Streaming也提供了Sink的幂等写 的特性（在编程中一个幂等 操作的特点是其任意多次执行所产生的影响均与一次执行的影响相同）， 因此Structure Streaming实现end-to-end exactly-once语义的故障恢复。 Structured Streaming API 自Spark-2.0版本以后Dataframe/Dataset才可以处理有界数据和无界数据。Structured Streaming也是用 SparkSession方式去创建Dataset/DataFrame ,同时所有Dataset/DataFrame 的操作保持和Spark SQL 中Dataset/DataFrame 一致。 Input Sources File Source 目前支持支持text, csv, json, orc, parquet等格式的文件，当这些数据被放入到采样目录，系统会以流的 形式读取采样目录下的文件. //1.创建SparkSession val spark = SparkSession .builder() .master(&quot;local[*]&quot;) .appName(&quot;printline&quot;) .getOrCreate() import spark.implicits._ var df = spark.readStream .format(&quot;text&quot;) //json/csv/parquet/orc 等 .load(&quot;file:///Users/mashikang/IdeaProjects/structured_stream/src/main/resources&quot;) var userDF = df.as[String] .map(line =&gt; line.split(&quot;\\\\s+&quot;)) .map(tokens =&gt; (tokens(0).toInt, tokens(1), tokens(2).toBoolean, tokens(3).toInt)) .toDF(&quot;id&quot;, &quot;name&quot;, &quot;sex&quot;, &quot;age&quot;) val query = userDF.writeStream.format(&quot;console&quot;) .outputMode(OutputMode.Append()) .start() query.awaitTermination() 文件 1 zhangsan true 20 2 lisi true 28 3 wangwu false 24 4 zhaoliu true 28 Socket source(debug) //1.构建SparkSession val spark = SparkSession.builder() .appName(&quot;wordcount&quot;) .master(&quot;local[*]&quot;) .getOrCreate() import spark.implicits._ //2.创建输入流-readStream var lines = spark.readStream .format(&quot;socket&quot;) .option(&quot;host&quot;, &quot;localhost&quot;) .option(&quot;port&quot;, 9999) .load() //3.对dataframe实现转换 var wordCounts = lines.as[String] .flatMap(_.split(&quot;\\\\s+&quot;)) .groupBy(&quot;value&quot;) .count() //4.构建query 输出 val query = wordCounts.writeStream .format(&quot;console&quot;) .outputMode(OutputMode.Update()) //有状态持续计算 Complete| Update .start() //5.等待流结束 query.awaitTermination() Kafka source pom.xml &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-sql-kafka-0-10_2.11&lt;/artifactId&gt; &lt;version&gt;2.4.3&lt;/version&gt; &lt;/dependency&gt; //1.创建SparkSession val spark=SparkSession .builder() .master(&quot;local[*]&quot;) .appName(&quot;printline&quot;) .getOrCreate() import spark.implicits._ var df=spark.readStream .format(&quot;kafka&quot;) .option(&quot;kafka.bootstrap.servers&quot;, &quot;localhost:9092&quot;) .option(&quot;subscribe&quot;, &quot;topic01&quot;) .load() .selectExpr(&quot;CAST(key AS STRING)&quot;,&quot;CAST(value AS STRING)&quot;) val wordCounts=df.select(&quot;value&quot;).as[String] .flatMap(_.split(&quot;\\\\s+&quot;)) .coalesce(1) .map((_,1)) .toDF(&quot;word&quot;,&quot;count&quot;) .groupBy(&quot;word&quot;) .sum(&quot;count&quot;) val query = wordCounts.writeStream. format(&quot;console&quot;) .outputMode(OutputMode.Update()) .start() query.awaitTermination() Output Sink File sink(Append Mode Only) //1.创建SparkSession val spark = SparkSession.builder() .master(&quot;local[*]&quot;) .appName(&quot;printline&quot;) .getOrCreate() import spark.implicits._ var df = spark.readStream .format(&quot;kafka&quot;) .option(&quot;kafka.bootstrap.servers&quot;, &quot;localhost:9092&quot;) .option(&quot;subscribe&quot;, &quot;topic01&quot;) .load() .selectExpr(&quot;CAST(key AS STRING)&quot;, &quot;CAST(value AS STRING)&quot;) val wordCounts = df.select(&quot;value&quot;).as[String] .flatMap(_.split(&quot;\\\\s+&quot;)) .coalesce(1) .map((_, 1)) .toDF(&quot;word&quot;, &quot;count&quot;) val query = wordCounts.writeStream .format(&quot;json&quot;) .option(&quot;checkpointLocation&quot;, &quot;file:///Users/mashikang/IdeaProjects/structured_stream/src/main/resources/checkpoints&quot;) .outputMode(OutputMode.Append()) .start(&quot;file:////Users/mashikang/IdeaProjects/structured_stream/src/main/resource/json&quot;) query.awaitTermination() KafkaSink((Append|Update|Complete)) //1.创建SparkSession val spark = SparkSession.builder() .master(&quot;local[*]&quot;) .appName(&quot;printline&quot;) .getOrCreate() import spark.implicits._ import org.apache.spark.sql.functions._ var df = spark.readStream .format(&quot;kafka&quot;) .option(&quot;kafka.bootstrap.servers&quot;, &quot;localhost:9092&quot;) .option(&quot;subscribe&quot;, &quot;topic01&quot;) .load() .selectExpr(&quot;CAST(key AS STRING)&quot;, &quot;CAST(value AS STRING)&quot;) val wordCounts = df.select(&quot;value&quot;).as[String] .flatMap(_.split(&quot;\\\\s+&quot;)) .coalesce(1) .map((_, 1)) .toDF(&quot;word&quot;, &quot;count&quot;) .groupBy(&quot;word&quot;) .agg(sum(&quot;count&quot;) as &quot;count&quot;) .selectExpr(&quot;word&quot;, &quot;CAST(count AS STRING)&quot;) .withColumnRenamed(&quot;word&quot;, &quot;key&quot;) .withColumnRenamed(&quot;count&quot;, &quot;value&quot;) val query = wordCounts.writeStream .format(&quot;kafka&quot;) .option(&quot;kafka.bootstrap.servers&quot;, &quot;localhost:9092&quot;) .option(&quot;topic&quot;, &quot;topic02&quot;) .option(&quot;checkpointLocation&quot;, &quot;file:///Users/mashikang/IdeaProjects/structured_stream/src/main/resources/checkpoints&quot;) .outputMode(OutputMode.Update()) .start() query.awaitTermination() Foreach sink(Append|Update|Complate) UserRowWriter 这里的 open方法在，每一次微批的时候触发，其中 epochId表示计算的批次。一般如果要保证 exactly-once 语义的处理时候，需要在外围系统存储 epochId，如果存在重复计算 epochId 不 变。 class UserRowWriter extends ForeachWriter[Row] { // 存储 上一次epochid var lastEpochId: Long = -1L /** * 计算 当前是否处理当前批次，如果epochId=lastEpochId说明是重复记录，丢弃更新 false * epochId!=lastEpochId 返回true 调用 open * * @param partitionId * @param epochId * @return */ override def open(partitionId: Long, epochId: Long): Boolean = { var flag: Boolean = false if (epochId != -1L) { if (lastEpochId == epochId) { // 是重复记录 flag = false } else { flag = true lastEpochId = epochId } } else { // 第一次进来 lastEpochId = epochId flag = true } flag } override def process(value: Row): Unit = { println(&quot; ,epochId:&quot; + lastEpochId) } override def close(errorOrNull: Throwable): Unit = { if (errorOrNull != null) errorOrNull.printStackTrace() } } //1.创建SparkSession val spark = SparkSession.builder() .master(&quot;local[*]&quot;) .appName(&quot;printline&quot;) .getOrCreate() import spark.implicits._ import org.apache.spark.sql.functions._ var df = spark.readStream .format(&quot;kafka&quot;) .option(&quot;kafka.bootstrap.servers&quot;, &quot;localhost:9092&quot;) .option(&quot;subscribe&quot;, &quot;topic01&quot;) .load() .selectExpr(&quot;CAST(key AS STRING)&quot;, &quot;CAST(value AS STRING)&quot;) val wordCounts = df.select(&quot;value&quot;).as[String] .flatMap(_.split(&quot;\\\\s+&quot;)) .coalesce(1) .map((_, 1)) .toDF(&quot;word&quot;, &quot;count&quot;) .groupBy(&quot;word&quot;) .agg(sum(&quot;count&quot;) as &quot;count&quot;) .selectExpr(&quot;word&quot;, &quot;CAST(count AS STRING)&quot;) .withColumnRenamed(&quot;word&quot;, &quot;key&quot;) .withColumnRenamed(&quot;count&quot;, &quot;value&quot;) val query = wordCounts.writeStream .outputMode(OutputMode.Update()) .foreach(new UserRowWriter) .start() query.awaitTermination() Window on Event Time Structured Streaming使用聚合函数基于EventTime计算window是非常简单的类似于分组聚合。分组聚 合是按照指定的column字段对表中的数据进行分组，然后使用聚合函数对用户指定的column字段进行 聚合。 下面一张图描绘的是计算10分钟内的单词统计，每间隔5分钟滑动一个时间窗口。 按照窗口原始含义是将落入到同一个窗口的数据进行分组，因此在Structured Streaming可以使用 groupby和window表达窗口计算 Logger.getLogger(&quot;org.apache.spark&quot;).setLevel(Level.WARN) Logger.getLogger(&quot;org.apache.jetty.server&quot;).setLevel(Level.OFF) //1.创建SparkSession val spark=SparkSession .builder() .master(&quot;local[*]&quot;) .appName(&quot;printline&quot;) .getOrCreate() import spark.implicits._ //字符,时间戳 var df=spark.readStream .format(&quot;socket&quot;) .option(&quot;host&quot;, &quot;localhost&quot;) .option(&quot;port&quot;, &quot;9999&quot;) .load() import org.apache.spark.sql.functions._ var sdf=new SimpleDateFormat(&quot;mm:ss&quot;) val wordCounts=df.select(&quot;value&quot;) .as[String] .map(_.split(&quot;,&quot;)) // 这里的Timestamp导如java.sql的依赖 .map(tokens=&gt;(tokens(0),new Timestamp(tokens(1).toLong))) .toDF(&quot;word&quot;,&quot;timestamp&quot;) .groupBy( window($&quot;timestamp&quot;,&quot;10 seconds&quot;,&quot;5 seconds&quot;), $&quot;word&quot; ) .count() .map(r=&gt; (sdf.format(r.getStruct(0).getTimestamp(0)), sdf.format(r.getStruct(0).getTimestamp(1)), r.getString(1),r.getLong(2))) .toDF(&quot;start&quot;,&quot;end&quot;,&quot;word&quot;,&quot;count&quot;) val query = wordCounts.writeStream .outputMode(OutputMode.Update()) .format(&quot;console&quot;) .start() query.awaitTermination() 处理延迟 Data 和 Watermarking 默认情况下，Spark会把落入到时间窗口的数据进行聚合操作。但是需要思考的是Event-Time是基于事 件的时间戳进行窗口聚合的。那就有可能事件窗口已经触发很久了，但是有一些元素因为某种原因，导 致迟到了，这个时候Spark需要将迟到的的数据加入到已经触发的窗口进行重复计算。但是需要注意如 果在长时间的流计算过程中，如果不去限定窗口计算的时间，那么意味着Spark要在内存中一直存储窗 口的状态，这样是不切实际的，因此Spark提供一种称为watermarker的机制用于限定存储在Spark内存 中中间结果存储的时间，这样系统就可以将已经确定触发过的窗口的中间结果给删除。如果后续还有数 据在窗口endtime以后抵达该窗口，Spark把这种数据定义为late数据。其中watermarker计算方式 max event time seen by engine - late threshold如果watermarker的取值大于了时间窗口的 endtime即可认定该窗口的计算结果就可以被丢弃了。如果此时再有数据落入到已经被丢弃的时间窗 口，则该迟到的数据会被Spark放弃更新，也就是丢弃。 Watermarking=max event time seen by engine - late threshold Watermarking保障机制： 能够保证在window的EndTime &gt; 水位线的窗口的状态Spark会存储起来，这个时候如果有迟到的 数据再水位线没有淹没window之前Spark可以保障迟到的数据能正常的处理。 如果水位线已经没过了窗口的end时间，那么后续迟到数据不一定能够被处理，换句话说，迟到越 久的数据 被处理的几率越小。 如果是使用水位线计算 ，输出模式必须是Update或者Append,否则系统不会删除。 Logger.getLogger(&quot;org.apache.spark&quot;).setLevel(Level.WARN) Logger.getLogger(&quot;org.apache.jetty.server&quot;).setLevel(Level.OFF) //1.创建SparkSession val spark=SparkSession .builder() .master(&quot;local[*]&quot;) .appName(&quot;printline&quot;) .getOrCreate() import spark.implicits._ //字符,时间戳 var df=spark.readStream .format(&quot;socket&quot;) .option(&quot;host&quot;, &quot;localhost&quot;) .option(&quot;port&quot;, &quot;9999&quot;) .load() import org.apache.spark.sql.functions._ var sdf=new SimpleDateFormat(&quot;mm:ss&quot;) val wordCounts=df.select(&quot;value&quot;) .as[String] .map(_.split(&quot;,&quot;)) // 这里的Timestamp导如java.sql的依赖 .map(tokens=&gt;(tokens(0),new Timestamp(tokens(1).toLong))) .toDF(&quot;word&quot;,&quot;timestamp&quot;) // 与上面窗口的API相比，多了水位线的设置 .withWatermark(&quot;timestamp&quot;, &quot;5 seconds&quot;) .groupBy( window($&quot;timestamp&quot;,&quot;10 seconds&quot;,&quot;5 seconds&quot;), $&quot;word&quot; ) .count() .map(r=&gt; (sdf.format(r.getStruct(0).getTimestamp(0)), sdf.format(r.getStruct(0).getTimestamp(1)), r.getString(1),r.getLong(2))) .toDF(&quot;start&quot;,&quot;end&quot;,&quot;word&quot;,&quot;count&quot;) val query = wordCounts.writeStream .outputMode(OutputMode.Update()) .format(&quot;console&quot;) .start() query.awaitTermination() Spark清除window聚合状态条件 Output mode 必须是 Append 或者 Update.，如果是Update 只要窗口有数据更新即可有输出。 如果是Append，必须当水位线没过window的时候才会将Result写出。 Update Mode Append Mode 必须在分组出现聚合使用时间column/window列 withWaterMaker的时间column必须和groupBy后面时间column保持一致，例如： 错误实例df.withWatermark(&quot;time&quot;, &quot;1 min&quot;).groupBy(&quot;time2&quot;).count()。 一定要在分组聚合之前调用withWaterMaking，例如df.groupBy(&quot;time&quot;).count().withWatermark(&quot;time&quot;, &quot;1 min&quot;) 错误实例 df.withWatermark(&quot;time&quot;, &quot;1 min&quot;).groupBy(&quot;time&quot;).count()正确写法。 Join 操作 Structured Streaming 不仅仅支持对静态的 Dataset/DataFrame 做join操作，也支持对streaming Dataset/DataFrame实现join操作。 Stream-static Joins spark-2.0 支持 Stream-stream Joins Spark 2.3 支持 Stream-static Joins //1.创建SparkSession val spark=SparkSession .builder() .master(&quot;local[6]&quot;) .appName(&quot;printline&quot;) .getOrCreate() import spark.implicits._ import org.apache.spark.sql.functions._ /** * +---+------+---+ * | id| name|age| * +---+------+---+ * | 1| 张三 | 18| * | 2| lisi| 28| * | 3|wangwu| 38| * +---+------+---+ */ val userDF=spark.read .format(&quot;json&quot;) .load(&quot;/Users/mashikang/IdeaProjects/structured_stream/src/main/resources/json&quot;) .selectExpr(&quot;CAST(id AS INTEGER)&quot;,&quot;name&quot;,&quot;CAST(age AS INTEGER)&quot;) //1 apple 1 4.5 var orderItemDF= spark.readStream .format(&quot;socket&quot;) .option(&quot;host&quot;,&quot;localhost&quot;) .option(&quot;port&quot;,9999) .load() .as[String] .map(line=&gt;line.split(&quot;\\\\s+&quot;)) .map(tokens=&gt;(tokens(0).toInt,tokens(1),tokens(2).toInt,tokens(3).toDouble)) .toDF(&quot;uid&quot;,&quot;item&quot;,&quot;count&quot;,&quot;price&quot;) val jointResults = orderItemDF.join(userDF,$&quot;id&quot;===$&quot;uid&quot;,&quot;left_outer&quot;) val query = jointResults .writeStream .format(&quot;console&quot;) .outputMode(OutputMode.Append()) .start() query.awaitTermination() Stream-stream Joins 两边流都必须声明watermarker，告知引擎什么是可以清楚状态（默认取最低）。 需要在连接条件中添加eventTime column的时间约束，这样引擎就知道什么时候可以清除后续 的流的状态。 Time range join conditions Join on event-time windows inner join 方案1 Time range join conditions //1.创建SparkSession val spark = SparkSession .builder() .master(&quot;local[*]&quot;) .appName(&quot;printline&quot;) .getOrCreate() import spark.implicits._ //001 apple 1 4.5 1566529410000 val orderDF = spark .readStream .format(&quot;socket&quot;) .option(&quot;host&quot;, &quot;localhost&quot;) .option(&quot;port&quot;, 9999) .load() .map(row =&gt; row.getAs[String](&quot;value&quot;).split(&quot;\\\\s+&quot;)) .map(t =&gt; (t(0), t(1), (t(2).toInt * t(3).toDouble), new Timestamp(t(4).toLong))) .toDF(&quot;uid&quot;, &quot;item&quot;, &quot;cost&quot;, &quot;order_time&quot;) //001 zhangsan 1566529410000 val userDF = spark .readStream .format(&quot;socket&quot;) .option(&quot;host&quot;, &quot;localhost&quot;) .option(&quot;port&quot;, 8888) .load() .map(row =&gt; row.getAs[String](&quot;value&quot;).split(&quot;\\\\s+&quot;)) .map(t =&gt; (t(0), t(1), new Timestamp(t(2).toLong))) .toDF(&quot;id&quot;, &quot;name&quot;, &quot;login_time&quot;) import org.apache.spark.sql.functions._ //用户的登陆数据缓存 2 seconds 订单数据缓存4秒 val userWatermarker = userDF.withWatermark(&quot;login_time&quot;, &quot;2 seconds&quot;) val orderWaterMarker = orderDF.withWatermark(&quot;order_time&quot;, &quot;4 seconds&quot;) //连接 用户登陆以后将2秒以内的购买行为和用进行join val joinDF = userWatermarker.join(orderWaterMarker, expr( &quot;&quot;&quot; |id=uid and order_time &gt;= login_time and order_time &lt;= login_time + interval 2 seconds &quot;&quot;&quot;.stripMargin) ) val query = joinDF.writeStream .format(&quot;console&quot;) .outputMode(OutputMode.Append()).start() query.awaitTermination() 方案2Join on event-time windows //1.创建SparkSession val spark = SparkSession .builder() .master(&quot;local[*]&quot;) .appName(&quot;printline&quot;) .getOrCreate() import spark.implicits._ //001 apple 1 4.5 1566529410000 val orderDF = spark .readStream .format(&quot;socket&quot;) .option(&quot;host&quot;, &quot;localhost&quot;) .option(&quot;port&quot;, 9999) .load() .map(row =&gt; row.getAs[String](&quot;value&quot;).split(&quot;\\\\s+&quot;)) .map(t =&gt; (t(0), t(1), (t(2).toInt * t(3).toDouble), new Timestamp(t(4).toLong))) .toDF(&quot;uid&quot;, &quot;item&quot;, &quot;cost&quot;, &quot;order_time&quot;) //001 zhangsan 1566529410000 val userDF = spark .readStream .format(&quot;socket&quot;) .option(&quot;host&quot;, &quot;localhost&quot;) .option(&quot;port&quot;, 8888) .load() .map(row =&gt; row.getAs[String](&quot;value&quot;).split(&quot;\\\\s+&quot;)) .map(t =&gt; (t(0), t(1), new Timestamp(t(2).toLong))) .toDF(&quot;id&quot;, &quot;name&quot;, &quot;login_time&quot;) import org.apache.spark.sql.functions._ //用户的登陆数据缓存 2 seconds 订单数据缓存4秒 val userWatermarker = userDF.withWatermark(&quot;login_time&quot;, &quot;2 seconds&quot;) .select( window($&quot;login_time&quot;, &quot;5 seconds&quot;), $&quot;id&quot;, $&quot;name&quot;, $&quot;login_time&quot;) .withColumnRenamed(&quot;window&quot;, &quot;leftWindow&quot;) val orderWaterMarker = orderDF.withWatermark(&quot;order_time&quot;, &quot;4 seconds&quot;) .select( window($&quot;order_time&quot;, &quot;5 seconds&quot;), $&quot;uid&quot;, $&quot;item&quot;, $&quot;cost&quot;, $&quot;order_time&quot;) .withColumnRenamed(&quot;window&quot;, &quot;rightWindow&quot;) //连接用户登陆以后将2秒以内的购买行为和用进行join val joinDF = userWatermarker .join( orderWaterMarker, expr( &quot;&quot;&quot; |id=uid and leftWindow = rightWindow &quot;&quot;&quot;.stripMargin) ) val query = joinDF.writeStream .format(&quot;console&quot;) .outputMode(OutputMode.Append()).start() query.awaitTermination() outer join //1.创建SparkSession val spark = SparkSession .builder() .master(&quot;local[*]&quot;) .appName(&quot;printline&quot;) .getOrCreate() import spark.implicits._ //001 apple 1 4.5 1566529410000 val orderDF = spark .readStream .format(&quot;socket&quot;) .option(&quot;host&quot;, &quot;localhost&quot;) .option(&quot;port&quot;, 9999) .load() .map(row =&gt; row.getAs[String](&quot;value&quot;).split(&quot;\\\\s+&quot;)) .map(t =&gt; (t(0), t(1), (t(2).toInt * t(3).toDouble), new Timestamp(t(4).toLong))) .toDF(&quot;uid&quot;, &quot;item&quot;, &quot;cost&quot;, &quot;order_time&quot;) //001 zhangsan 1566529410000 val userDF = spark .readStream .format(&quot;socket&quot;) .option(&quot;host&quot;, &quot;localhost&quot;) .option(&quot;port&quot;, 8888) .load() .map(row =&gt; row.getAs[String](&quot;value&quot;).split(&quot;\\\\s+&quot;)) .map(t =&gt; (t(0), t(1), new Timestamp(t(2).toLong))) .toDF(&quot;id&quot;, &quot;name&quot;, &quot;login_time&quot;) import org.apache.spark.sql.functions._ //系统分别会对 user 和 order 缓存 最近 1 seconds 和 2 seconds 数据, // 一旦时间过去，系统就无 法保证数据状态继续保留 val loginWatermarker=userDF.withWatermark(&quot;login_time&quot;,&quot;1 second&quot;) val orderWatermarker=orderDF.withWatermark(&quot;order_time&quot;,&quot;2 seconds&quot;) //计算订单的时间 &amp; 用户 登陆之后的0~1 seconds 关联 数据 并且进行join val joinDF = loginWatermarker .join( orderWatermarker, expr(&quot;uid=id and order_time &gt;= login_time and order_time &lt;= login_time + interval 1 seconds&quot;), &quot;leftOuter&quot; ) val query = joinDF.writeStream .format(&quot;console&quot;) .outputMode(OutputMode.Append()).start() query.awaitTermination() ","link":"https://mask0407.github.io/spark07/"},{"title":"Spark入门(七)——最全的Saprk SQL算子介绍与使用(下)","content":" SQL语法查询 单行查询 模糊查询 排序查询 limit查询 分组查询 having过滤 case-when 行转列 pivot Cube计算 Join表连接 子查询 开窗函数 开窗函数SQL解读 ROW_NUM RANK() DENSE_RANK() /密集排名 自定义函数 单行函数 聚合函数（untyped） Load/Save Paquet JSON ORC(存储压缩格式，比较节省空间) CSV JDBC DataFrame转为RDD (Spark SQL 查询) SQL语法查询 单行查询 // 单行查询 var userDF = List((1, &quot;张三&quot;, true, 18, 15000, 1)) .toDF(&quot;id&quot;, &quot;name&quot;, &quot;sex&quot;, &quot;age&quot;, &quot;salary&quot;, &quot;dept&quot;) userDF.createTempView(&quot;t_employee&quot;) val sql = &quot;select * from t_employee where name = '张三'&quot; spark.sql(sql) .show() +---+----+----+---+------+----+ | id|name| sex|age|salary|dept| +---+----+----+---+------+----+ | 1|张三|true| 18| 15000| 1| +---+----+----+---+------+----+ 模糊查询 var userDF= List((1,&quot;张三&quot;,true,18,15000,1)) .toDF(&quot;id&quot;,&quot;name&quot;,&quot;sex&quot;,&quot;age&quot;,&quot;salary&quot;,&quot;dept&quot;) userDF.createTempView(&quot;t_employee&quot;) val sql=&quot;select * from t_employee where name like '%三%'&quot; spark.sql(sql) .show() +---+----+----+---+------+----+ | id|name| sex|age|salary|dept| +---+----+----+---+------+----+ | 1|张三|true| 18| 15000| 1| +---+----+----+---+------+----+ 排序查询 var userDF = List((1, &quot;张三&quot;, true, 18, 15000, 1), (2, &quot;ls&quot;, false, 18, 12000, 1)) .toDF(&quot;id&quot;, &quot;name&quot;, &quot;sex&quot;, &quot;age&quot;, &quot;salary&quot;, &quot;dept&quot;) //构建视图 userDF.createTempView(&quot;t_employee&quot;) val sql = &quot;&quot;&quot; |select * from t_employee where salary &gt; 10000 order by salary desc &quot;&quot;&quot; .stripMargin spark.sql(sql) .show() +---+----+-----+---+------+----+ | id|name| sex|age|salary|dept| +---+----+-----+---+------+----+ | 1|张三| true| 18| 15000| 1| | 2|李四|false| 18| 12000| 1| +---+----+-----+---+------+----+ limit查询 var userDF= List( (1,&quot;张三&quot;,true,18,15000,1), (2,&quot;李四&quot;,false,18,12000,1), (3,&quot;王五&quot;,false,18,16000,2) ) .toDF(&quot;id&quot;,&quot;name&quot;,&quot;sex&quot;,&quot;age&quot;,&quot;salary&quot;,&quot;dept&quot;) //构建视图 userDF.createTempView(&quot;t_employee&quot;) val sql= &quot;&quot;&quot; |select * from t_employee where salary &gt; 10000 order by salary desc limit 2 &quot;&quot;&quot;.stripMargin spark.sql(sql) .show() +---+----+-----+---+------+----+ | id|name| sex|age|salary|dept| +---+----+-----+---+------+----+ | 3|王五|false| 18| 16000| 2| | 1|张三| true| 18| 15000| 1| +---+----+-----+---+------+----+ 分组查询 var userDF= List( (1,&quot;张三&quot;,true,18,15000,1), (2,&quot;李四&quot;,false,18,12000,1), (3,&quot;王五&quot;,false,18,16000,2) ) .toDF(&quot;id&quot;,&quot;name&quot;,&quot;sex&quot;,&quot;age&quot;,&quot;salary&quot;,&quot;dept&quot;) //构建视图 userDF.createTempView(&quot;t_employee&quot;) val sql= &quot;&quot;&quot; |select dept ,avg(salary) as avg_slalary from t_employee |group by dept order by avg_slalary desc &quot;&quot;&quot;.stripMargin spark.sql(sql) .show() +----+-----------+ |dept|avg_slalary| +----+-----------+ | 2| 16000.0| | 1| 13500.0| +----+-----------+ having过滤 var userDF= List( (1,&quot;张三&quot;,true,18,15000,1), (2,&quot;李四&quot;,false,18,12000,1), (3,&quot;王五&quot;,false,18,16000,2) ) .toDF(&quot;id&quot;,&quot;name&quot;,&quot;sex&quot;,&quot;age&quot;,&quot;salary&quot;,&quot;dept&quot;) //构建视图 userDF.createTempView(&quot;t_employee&quot;) val sql= &quot;&quot;&quot; | select dept ,avg(salary) as avg_slalary | from t_employee group by dept | having avg_slalary &gt; 13500 | order by avg_slalary desc &quot;&quot;&quot;.stripMargin spark.sql(sql) .show() +----+-----------+ |dept|avg_slalary| +----+-----------+ | 2| 16000.0| +----+-----------+ case-when var userDF= List( (1,&quot;张三&quot;,true,18,15000,1), (2,&quot;李四&quot;,false,18,12000,1), (3,&quot;王五&quot;,false,18,16000,2) ) .toDF(&quot;id&quot;,&quot;name&quot;,&quot;sex&quot;,&quot;age&quot;,&quot;salary&quot;,&quot;dept&quot;) //构建视图 userDF.createTempView(&quot;t_employee&quot;) val sql= &quot;&quot;&quot; |select id,name,case sex when true then '男' else '女' end as sex_alias |from t_employee &quot;&quot;&quot;.stripMargin spark.sql(sql) .show() +---+----+---------+ | id|name|sex_alias| +---+----+---------+ | 1|张三| 男| | 2|李四| 女| | 3|王五| 女| +---+----+---------+ 行转列 // 行转列 var scoreDF = List( (1, &quot;语文&quot;, 100), (1, &quot;数学&quot;, 100), (1, &quot;英语&quot;, 100), (2, &quot;数学&quot;, 79), (2, &quot;语文&quot;, 80), (2, &quot;英语&quot;, 100) ).toDF(&quot;id&quot;, &quot;course&quot;, &quot;score&quot;) scoreDF.createOrReplaceTempView(&quot;t_course&quot;) val sql = &quot;&quot;&quot; | select id, | max(case course when '数学' then score else 0 end) as math, | max(case course when '英语' then score else 0 end) as english, | max(case course when '语文' then score else 0 end) as chinese | from t_course group by id &quot;&quot;&quot;.stripMargin spark.sql(sql) .show() +---+----+-------+-------+ | id|math|english|chinese| +---+----+-------+-------+ | 1| 100| 100| 100| | 2| 79| 100| 80| +---+----+-------+-------+ pivot var scoreDF = List( (1, &quot;语文&quot;, 100), (1, &quot;数学&quot;, 100), (1, &quot;英语&quot;, 100), (2, &quot;数学&quot;, 79), (2, &quot;语文&quot;, 80), (2, &quot;英语&quot;, 100) ).toDF(&quot;id&quot;, &quot;course&quot;, &quot;score&quot;) scoreDF.createOrReplaceTempView(&quot;t_course&quot;) val sql = &quot;&quot;&quot; |select * |from t_course |pivot(max(score) for course in ('数学' ,'语文','英语')) | &quot;&quot;&quot;.stripMargin spark.sql(sql) .show() +---+----+----+----+ | id|数学|语文|英语| +---+----+----+----+ | 1| 100| 100| 100| | 2| 79| 80| 100| +---+----+----+----+ 在书写SQL的时候除去聚合字段和输出列明字段，其他字段作为groupby后的隐藏字段。 Cube计算 // Cube计算 val frame = List( (110, 50, 80, 80), (120, 60, 95, 75), (120, 50, 96, 70) ) .toDF(&quot;height&quot;, &quot;weight&quot;, &quot;uiq&quot;, &quot;ueq&quot;) frame.createTempView(&quot;t_user&quot;) val sql= &quot;&quot;&quot; |select height,weight,avg(uiq),avg(ueq) |from t_user |group by cube(height,weight) &quot;&quot;&quot;.stripMargin spark.sql(sql) .show() +------+------+-----------------+--------+ |height|weight| avg(uiq)|avg(ueq)| +------+------+-----------------+--------+ | 110| 50| 80.0| 80.0| | 120| null| 95.5| 72.5| | 120| 60| 95.0| 75.0| | null| 60| 95.0| 75.0| // weight 是60的所有数据 的uiq、ueq平均值 | null| null|90.33333333333333| 75.0| // 所有数据的uiq、ueq平均值 | 120| 50| 96.0| 70.0| | 110| null| 80.0| 80.0| | null| 50| 88.0| 75.0| +------+------+-----------------+--------+ Join表连接 // join val userCatagoryCostDF=List( (1,&quot;电脑配件&quot;,100), (1,&quot;母婴用品&quot;,100), (1,&quot;生活用品&quot;,100), (2,&quot;居家美食&quot;,79), (2,&quot;消费电子&quot;,80), (2,&quot;生活用品&quot;,100) ).toDF(&quot;uid&quot;,&quot;category&quot;,&quot;cost&quot;) val usersDF= List( (1,&quot;张晓三&quot;,true,18,15000), (2,&quot;李晓四&quot;,true,18,18000), (3,&quot;王晓五&quot;,false,18,10000) ).toDF(&quot;id&quot;,&quot;name&quot;,&quot;sex&quot;,&quot;age&quot;,&quot;salary&quot;) usersDF.createTempView(&quot;t_user&quot;) userCatagoryCostDF.createTempView(&quot;t_user_cost&quot;) val sql = &quot;&quot;&quot; |select u.*,o.* |from t_user u |left join t_user_cost o |on u.id=o.uid |where uid is not null &quot;&quot;&quot;.stripMargin spark.sql(sql) .show() +---+------+----+---+------+---+--------+----+ | id| name| sex|age|salary|uid|category|cost| +---+------+----+---+------+---+--------+----+ | 1|张晓三|true| 18| 15000| 1|电脑配件| 100| | 1|张晓三|true| 18| 15000| 1|母婴用品| 100| | 1|张晓三|true| 18| 15000| 1|生活用品| 100| | 2|李晓四|true| 18| 18000| 2|居家美食| 79| | 2|李晓四|true| 18| 18000| 2|消费电子| 80| | 2|李晓四|true| 18| 18000| 2|生活用品| 100| +---+------+----+---+------+---+--------+----+ 子查询 // 子查询 var df=List( (1,&quot;zs&quot;,true,1,15000), (2,&quot;ls&quot;,false,2,18000), (3,&quot;ww&quot;,false,2,14000), (4,&quot;zl&quot;,false,1,18000), (5,&quot;win7&quot;,false,1,16000) ).toDF(&quot;id&quot;,&quot;name&quot;,&quot;sex&quot;,&quot;dept&quot;,&quot;salary&quot;) df.createTempView(&quot;t_employee&quot;) val sql= &quot;&quot;&quot; |select id,name,salary,dept, |(select avg(salary) from t_employee t2 where t1.dept=t2.dept) as avg_salary |from t_employee t1 |order by dept desc &quot;&quot;&quot;.stripMargin spark.sql(sql) .show() +---+----+------+----+------------------+ | id|name|salary|dept| avg_salary| +---+----+------+----+------------------+ | 2| ls| 18000| 2| 16000.0| | 3| ww| 14000| 2| 16000.0| | 5|win7| 16000| 1|16333.333333333334| | 1| zs| 15000| 1|16333.333333333334| | 4| zl| 18000| 1|16333.333333333334| +---+----+------+----+------------------+ 在spark SQL不允许在子查询中使用非等值连接。（MySQL|Oracle支持） 开窗函数 在正常的统计分析中 ，通常使用聚合函数作为分析，聚合分析函数的特点是将n行记录合并成一行，在数据库的统计当中 还有一种统计称为开窗统计，开窗函数可以实现将一行变成多行。可以将数据库查询的每一条记录比作是一幢高楼的一 层, 开窗函数就是在每一层开一扇窗, 让每一层能看到整装楼的全貌或一部分。 // 开窗函数 var df=List( (1,&quot;zs&quot;,true,1,15000), (2,&quot;ls&quot;,false,2,18000), (3,&quot;ww&quot;,false,2,14000), (4,&quot;zl&quot;,false,1,18000), (5,&quot;win7&quot;,false,1,16000) ).toDF(&quot;id&quot;,&quot;name&quot;,&quot;sex&quot;,&quot;dept&quot;,&quot;salary&quot;) df.createTempView(&quot;t_employee&quot;) val sql= &quot;&quot;&quot; |select id,name,salary,dept, |count(id) over(partition by dept order by salary desc) as rank, |(count(id) over(partition by dept order by salary desc rows between current row and unbounded following) - 1) as low_than_me, |avg(salary) over(partition by dept rows between unbounded preceding and unbounded following) as avg_salary, |avg(salary) over() as all_avg_salary |from t_employee t1 order by dept desc &quot;&quot;&quot;.stripMargin spark.sql(sql) .show() spark.stop() +---+----+------+----+----+-----------+------------------+--------------+ | id|name|salary|dept|rank|low_than_me| avg_salary|all_avg_salary| +---+----+------+----+----+-----------+------------------+--------------+ | 2| ls| 18000| 2| 1| 1| 16000.0| 16200.0| | 3| ww| 14000| 2| 2| 0| 16000.0| 16200.0| | 4| zl| 18000| 1| 1| 2|16333.333333333334| 16200.0| | 5|win7| 16000| 1| 2| 1|16333.333333333334| 16200.0| | 1| zs| 15000| 1| 3| 0|16333.333333333334| 16200.0| +---+----+------+----+----+-----------+------------------+--------------+ 开窗函数SQL解读 select id,name,salary,dept, # 按部门分组、工资倒叙排序展示 当前部门的id总数 count(id) over(partition by dept order by salary desc) as rank, # 按部门分组、工资倒叙排序展示当前行至最后一行id总数-1 (count(id) over(partition by dept order by salary desc rows between current row and unbounded following) - 1) as low_than_me, # 按部门分组展示首行至尾行的平均工资 如：2部门平均工资16000 1部门平均工资16333.333333333334 avg(salary) over(partition by dept rows between unbounded preceding and unbounded following) as avg_salary, # 展示所有员工的平均工资 avg(salary) over() as all_avg_salary from t_employee t1 order by dept desc 总结 聚合函数(字段) over ([[partition by 字段] order by 字段 asc [rows between 起始行偏移量 and 终止偏移量]] ) 其中：偏移量的取值 preceding：用于累加前N行（分区之内）。若是从分区第一行头开始，则为 unbounded。 N为：相对当前行向前 的偏移量负数。 following:与preceding相反，累加后N行（分区之内）。若是累加到该分区结束则为unbounded。N为：相对当 前行向后的偏移量正数 current row：顾名思义，当前行，偏移量为0 ROW_NUM 统计当前记录所在的行号 // ROW_NUM var df=List( (1,&quot;zs&quot;,true,1,15000), (2,&quot;ls&quot;,false,2,18000), (3,&quot;ww&quot;,false,2,14000), (4,&quot;zl&quot;,false,1,18000), (5,&quot;win7&quot;,false,1,16000) ).toDF(&quot;id&quot;,&quot;name&quot;,&quot;sex&quot;,&quot;dept&quot;,&quot;salary&quot;) df.createTempView(&quot;t_employee&quot;) val sql= &quot;&quot;&quot; |select id,name,salary,dept, |ROW_NUMBER() over(partition by dept order by salary desc) as rank |from t_employee t1 |order by dept desc &quot;&quot;&quot;.stripMargin spark.sql(sql) .show() +---+----+------+----+----+ | id|name|salary|dept|rank| +---+----+------+----+----+ | 2| ls| 18000| 2| 1| | 3| ww| 14000| 2| 2| | 4| zl| 18000| 1| 1| | 5|win7| 16000| 1| 2| | 1| zs| 15000| 1| 3| +---+----+------+----+----+ 如果部门存在相同薪资此时ROW_NUMBER只能表示当前记录在窗口行标 RANK() // RANK var df=List( (1,&quot;zs&quot;,true,1,15000), (2,&quot;ls&quot;,false,2,18000), (3,&quot;ww&quot;,false,2,14000), (4,&quot;zl&quot;,false,1,18000), (6,&quot;zl1&quot;,true,1,18000), (5,&quot;win7&quot;,false,1,16000) ).toDF(&quot;id&quot;,&quot;name&quot;,&quot;sex&quot;,&quot;dept&quot;,&quot;salary&quot;) df.createTempView(&quot;t_employee&quot;) val sql= &quot;&quot;&quot; |select id,name,salary,dept, |RANK() over(partition by dept order by salary desc) as rank |from t_employee t1 |order by dept desc &quot;&quot;&quot;.stripMargin spark.sql(sql) .show() +---+----+------+----+----+ | id|name|salary|dept|rank| +---+----+------+----+----+ | 2| ls| 18000| 2| 1| | 3| ww| 14000| 2| 2| | 4| zl| 18000| 1| 1| | 6| zl1| 18000| 1| 1| | 5|win7| 16000| 1| 3| //因为出现两个排名为1的，所有这里是3，故而排名序号不连续 | 1| zs| 15000| 1| 4| +---+----+------+----+----+ 与ROW_NUM相比，排名特点是不连续。 DENSE_RANK() /密集排名 // DENSE_RANK/密集排名 var df = List( (1, &quot;zs&quot;, true, 1, 15000), (2, &quot;ls&quot;, false, 2, 18000), (3, &quot;ww&quot;, false, 2, 14000), (4, &quot;zl&quot;, false, 1, 18000), (6, &quot;zl1&quot;, true, 1, 18000), (5, &quot;win7&quot;, false, 1, 16000) ).toDF(&quot;id&quot;, &quot;name&quot;, &quot;sex&quot;, &quot;dept&quot;, &quot;salary&quot;) df.createTempView(&quot;t_employee&quot;) val sql = &quot;&quot;&quot; |select id,name,salary,dept, |DENSE_RANK() over(partition by dept order by salary desc) as rank |from t_employee t1 |order by dept desc &quot;&quot;&quot;.stripMargin spark.sql(sql) .show() +---+----+------+----+----+ | id|name|salary|dept|rank| +---+----+------+----+----+ | 3| ww| 14000| 2| 2| | 2| ls| 18000| 2| 1| | 4| zl| 18000| 1| 1| | 6| zl1| 18000| 1| 1| | 1| zs| 15000| 1| 3| | 5|win7| 16000| 1| 2| +---+----+------+----+----+ 自定义函数 单行函数 // 自定义单行函数 var df = List( (1, &quot;zs&quot;, true, 1, 15000), (2, &quot;ls&quot;, false, 2, 18000), (3, &quot;ww&quot;, false, 2, 14000), (4, &quot;zl&quot;, false, 1, 18000), (6, &quot;zl1&quot;, true, 1, 18000), (5, &quot;win7&quot;, false, 1, 16000) ).toDF(&quot;id&quot;, &quot;name&quot;, &quot;sex&quot;, &quot;dept&quot;, &quot;salary&quot;) df.createTempView(&quot;t_employee&quot;) spark.udf .register(&quot;convertSex&quot;, (sex: Boolean) =&gt; { sex match { case true =&gt; &quot;男&quot; case false =&gt; &quot;女&quot; } }) val sql = &quot;&quot;&quot; |select id,name,convertSex(sex) as usex |from t_employee &quot;&quot;&quot;.stripMargin spark.sql(sql) .show() +---+----+----+ | id|name|usex| +---+----+----+ | 1| zs| 男| | 2| ls| 女| | 3| ww| 女| | 4| zl| 女| | 6| zl1| 男| | 5|win7| 女| +---+----+----+ 聚合函数（untyped） 只需要写一个类继承 UserDefinedAggregateFunction 即可。 import org.apache.spark.sql.Row import org.apache.spark.sql.expressions.{MutableAggregationBuffer, UserDefinedAggregateFunction} import org.apache.spark.sql.types.{DataType, DoubleType, StructType} class CustomSum extends UserDefinedAggregateFunction { //1.输入的字段类型信息 name属性 叫什么无所谓 override def inputSchema: StructType = { new StructType().add(&quot;salary&quot;, DoubleType) } //2.中间结果变量类型 override def bufferSchema: StructType = { new StructType().add(&quot;taotalsalary&quot;, DoubleType) } //3.最终返回结果的类型 override def dataType: DataType = DoubleType //4.设置返回结果类型是否固定 override def deterministic: Boolean = true //5.初始化中间结果 override def initialize(buffer: MutableAggregationBuffer): Unit = { //第0个位置元素是0.0 buffer.update(0, 0.0) } //6.将传如的数值添加到中间结果变量中 override def update(buffer: MutableAggregationBuffer, input: Row): Unit = { val history = buffer.getAs[Double](0) val current = input.getAs[Double](0) buffer.update(0, history + current) } //7.将局部结果聚合到buffer1中 override def merge(buffer1: MutableAggregationBuffer, buffer2: Row): Unit = { val result = buffer1.getAs[Double](0) + buffer2.getAs[Double](0) buffer1.update(0, result) } //8.返回最终结果 override def evaluate(buffer: Row): Any = { buffer.getAs[Double](0) } } spark 代码 // 自定义聚合函数（untyped） var df = List( (1, &quot;zs&quot;, true, 1, 15000), (2, &quot;ls&quot;, false, 2, 18000), (3, &quot;ww&quot;, false, 2, 14000), (4, &quot;zl&quot;, false, 1, 18000), (6, &quot;zl1&quot;, true, 1, 18000), (5, &quot;win7&quot;, false, 1, 16000) ).toDF(&quot;id&quot;, &quot;name&quot;, &quot;sex&quot;, &quot;dept&quot;, &quot;salary&quot;) df.createTempView(&quot;t_employee&quot;) spark.udf .register(&quot;customSum&quot;, new CustomSum) val sql = &quot;&quot;&quot; |select dept,customSum(salary) |from t_employee |group by dept &quot;&quot;&quot;.stripMargin spark.sql(sql) .show() +----+---------------------------------+ |dept|customsum(CAST(salary AS DOUBLE))| +----+---------------------------------+ | 1| 67000.0| | 2| 32000.0| +----+---------------------------------+ Load/Save Paquet Parquet简介 Parquet是面向分析型业务的列式存储格式，由Twitter和Cloudera合作开发，2015年5月从Apache的孵化器里毕业成为Apache顶级项目 http://parquet.apache.org/ // paquet var df=List( (1,&quot;zs&quot;,true,1,15000), (2,&quot;ls&quot;,false,2,18000), (3,&quot;ww&quot;,false,2,14000), (4,&quot;zl&quot;,false,1,18000), (6,&quot;zl1&quot;,true,1,18000), (5,&quot;win7&quot;,false,1,16000) ).toDF(&quot;id&quot;,&quot;name&quot;,&quot;sex&quot;,&quot;dept&quot;,&quot;salary&quot;) df.write .format(&quot;parquet&quot;) .save(&quot;file:////Users/mashikang/IdeaProjects/spark_sql/src/main/resources/parquet&quot;) spark.read .parquet(&quot;file:////Users/mashikang/IdeaProjects/spark_sql/src/main/resources/parquet&quot;) .show() +---+----+-----+----+------+ | id|name| sex|dept|salary| +---+----+-----+----+------+ | 5|win7|false| 1| 16000| | 6| zl1| true| 1| 18000| | 4| zl|false| 1| 18000| | 3| ww|false| 2| 14000| | 1| zs| true| 1| 15000| | 2| ls|false| 2| 18000| +---+----+-----+----+------+ 存储文件样式 JSON // json var df=List( (1,&quot;zs&quot;,true,1,15000), (2,&quot;ls&quot;,false,2,18000), (3,&quot;ww&quot;,false,2,14000), (4,&quot;zl&quot;,false,1,18000), (6,&quot;zl1&quot;,true,1,18000), (5,&quot;win7&quot;,false,1,16000) ).toDF(&quot;id&quot;,&quot;name&quot;,&quot;sex&quot;,&quot;dept&quot;,&quot;salary&quot;) df.write .format(&quot;json&quot;) .save(&quot;file:///Users/mashikang/IdeaProjects/spark_sql/src/main/resources/json&quot;) spark.read .json(&quot;file:///Users/mashikang/IdeaProjects/spark_sql/src/main/resources/json&quot;) .show() +----+---+----+------+-----+ |dept| id|name|salary| sex| +----+---+----+------+-----+ | 1| 5|win7| 16000|false| | 2| 3| ww| 14000|false| | 1| 4| zl| 18000|false| | 2| 2| ls| 18000|false| | 1| 6| zl1| 18000| true| | 1| 1| zs| 15000| true| +----+---+----+------+-----+ ORC(存储压缩格式，比较节省空间) // ORC var df=List( (1,&quot;zs&quot;,true,1,15000), (2,&quot;ls&quot;,false,2,18000), (3,&quot;ww&quot;,false,2,14000), (4,&quot;zl&quot;,false,1,18000), (6,&quot;zl1&quot;,true,1,18000), (5,&quot;win7&quot;,false,1,16000) ).toDF(&quot;id&quot;,&quot;name&quot;,&quot;sex&quot;,&quot;dept&quot;,&quot;salary&quot;) df.write .format(&quot;orc&quot;) .save(&quot;file:///Users/mashikang/IdeaProjects/spark_sql/src/main/resources/orc&quot;) spark.read .orc(&quot;file:///Users/mashikang/IdeaProjects/spark_sql/src/main/resources/orc&quot;) .show() +---+----+-----+----+------+ | id|name| sex|dept|salary| +---+----+-----+----+------+ | 5|win7|false| 1| 16000| | 4| zl|false| 1| 18000| | 3| ww|false| 2| 14000| | 6| zl1| true| 1| 18000| | 1| zs| true| 1| 15000| | 2| ls|false| 2| 18000| +---+----+-----+----+------+ CSV // CSV var df=List( (1,&quot;zs&quot;,true,1,15000), (2,&quot;ls&quot;,false,2,18000), (3,&quot;ww&quot;,false,2,14000), (4,&quot;zl&quot;,false,1,18000), (6,&quot;zl1&quot;,true,1,18000), (5,&quot;win7&quot;,false,1,16000) ).toDF(&quot;id&quot;,&quot;name&quot;,&quot;sex&quot;,&quot;dept&quot;,&quot;salary&quot;) df.write .format(&quot;csv&quot;) .option(&quot;sep&quot;, &quot;,&quot;) .option(&quot;inferSchema&quot;, &quot;true&quot;) .option(&quot;header&quot;, &quot;true&quot;) .save(&quot;file:///Users/mashikang/IdeaProjects/spark_sql/src/main/resources/csv&quot;) spark.read .option(&quot;sep&quot;, &quot;,&quot;) .option(&quot;inferSchema&quot;, &quot;true&quot;) .option(&quot;header&quot;, &quot;true&quot;) .csv(&quot;file:///Users/mashikang/IdeaProjects/spark_sql/src/main/resources/csv&quot;) .show() +---+----+-----+----+------+ | id|name| sex|dept|salary| +---+----+-----+----+------+ | 5|win7|false| 1| 16000| | 4| zl|false| 1| 18000| | 3| ww|false| 2| 14000| | 2| ls|false| 2| 18000| | 6| zl1| true| 1| 18000| | 1| zs| true| 1| 15000| +---+----+-----+----+------+ JDBC // JDBC val usersDF = List( (1, &quot;张晓三&quot;, 1, 15000), (2, &quot;李晓四&quot;, 1, 18000), (3, &quot;王晓五&quot;, 1, 10000) ).toDF(&quot;id&quot;, &quot;name&quot;, &quot;dept&quot;, &quot;salary&quot;) usersDF.write .format(&quot;jdbc&quot;) .mode(SaveMode.Overwrite) .option(&quot;user&quot;, &quot;root&quot;) .option(&quot;password&quot;, &quot;root&quot;) .option(&quot;url&quot;, &quot;jdbc:mysql://localhost:3306/test&quot;) .option(&quot;dbtable&quot;, &quot;t_user&quot;) .save() val props = new Properties() props.put(&quot;user&quot;, &quot;root&quot;) props.put(&quot;password&quot;, &quot;root&quot;) spark.read .jdbc(&quot;jdbc:mysql://localhost:3306/test&quot;, &quot;t_user&quot;, props) .show() 或者 val usersDF = List( (1, &quot;张晓三&quot;, 1, 15000), (2, &quot;李晓四&quot;, 1, 18000), (3, &quot;王晓五&quot;, 1, 10000) ).toDF(&quot;id&quot;, &quot;name&quot;, &quot;dept&quot;, &quot;salary&quot;) usersDF.write .format(&quot;jdbc&quot;) .mode(SaveMode.Overwrite) .option(&quot;user&quot;, &quot;root&quot;) .option(&quot;password&quot;, &quot;root&quot;) .option(&quot;url&quot;, &quot;jdbc:mysql://localhost:3306/test&quot;) .option(&quot;dbtable&quot;, &quot;t_user&quot;) .save() spark.read.format(&quot;jdbc&quot;) .option(&quot;user&quot;, &quot;root&quot;) .option(&quot;password&quot;, &quot;root&quot;) .option(&quot;url&quot;, &quot;jdbc:mysql://localhost:3306/test&quot;) .option(&quot;dbtable&quot;, &quot;t_user&quot;) .load() .show() DataFrame转为RDD val usersDF = List( (1, &quot;张晓三&quot;, 1, 15000.0), (2, &quot;李晓四&quot;, 1, 18000.0), (3, &quot;王晓五&quot;, 1, 10000.0) ).toDF(&quot;id&quot;, &quot;name&quot;, &quot;dept&quot;, &quot;salary&quot;) usersDF.rdd.foreachPartition(its =&gt; { its.foreach(row =&gt; { val id = row.getAs[Int](&quot;id&quot;) val name = row.getAs[String](&quot;name&quot;) val salary = row.getAs[Double](&quot;salary&quot;) println(s&quot;$id,$name,$salary&quot;) }) }) 2,李晓四,18000.0 3,王晓五,10000.0 1,张晓三,15000.0 ","link":"https://mask0407.github.io/spark06/"},{"title":"Spark入门(六)——最全的Saprk SQL算子介绍与使用(上)","content":" Datasets &amp; DataFrames简介 快速入门 Dataset &amp; DataFrame实战 Dataset create case-class Tuple(元组) json数据 RDD Dataframe create json文件 case-class Tuple(元组) RDD转换 DataFrame Operations（Untyped）DataFrame无类型操作 printSchema 打印Dataframe的表结构（表头） show select selectExpr withColumn withColumnRenamed drop dropDuplicates orderBy|sort groupBy agg limit where pivot（行转列） na(替换当前为null的值) join cube(多维度) Dataset Oprations (Strong typed) 数据集操作-强类型 (Spark SQL) Spark SQL是构建在RDD之上的ETL（Extract Transform Load）工具。SparkSQL在RDD之上抽象出来Dataset/Dataframe 这两个类提供了类似RDD的功能，也就意味用户可以使用map、faltMap、filter等高阶算子，同时也通过了基于列的命名查询，也就是说Dataset/DataFrame提供了两套操作数据的API，这些API可以给Saprk引擎要提供更多信息，系统可可以根据这些信息对计算实现一定的优化。目前Spark SQL提供了两种交互方式 SQL 脚本 Dataset API(strong-typed类型、untyped类型操作) Datasets &amp; DataFrames简介 Dataset是一个分布式数据集，Dataset是在spark-1.6提出新的API，该API构建在RDD（strong type，使用lambda表达式）之上同时可以借助于Spark SQL对执行引擎的优点，使得使用Dateset执行一些数据的转换比直接使用RDD算子功能和性能都有所提升。因此我们可以认为Dateset就是一个加强版本的RDD。 Dataset除了可以使用JVM中数组|集合对象创建之外，也可以将任意的一个RDD转换为Dataset. Python does not have the support for the Dataset API. DataFrames 是Dataset的一种特殊情况。比如 Dataset中可以存储任意 对象类型的数据作为Dataset的元素。但是Dataframe的元素只有一种类型Row类型，这种基于Row查询和传统数据库中ResultSet操作极其相似。因为Row类型的数据表示Dataframe的一个元素，类似数据库中的一行，这些行中的元素可以通过下标或者column name访问。由于 Dateset是API的兼容或者支持度上不是多么的好，但是Dataframe在 API层面支持的Scala、Java、R、Python支持比较全面。 快速入门 引入依赖 &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-core_2.11&lt;/artifactId&gt; &lt;version&gt;2.4.3&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-sql_2.11&lt;/artifactId&gt; &lt;version&gt;2.4.3&lt;/version&gt; &lt;/dependency&gt; 创建字符统计（untyped） def main(args: Array[String]): Unit = { //1.创建SparkSeesion val spark = SparkSession .builder() .appName(&quot;wordcount&quot;) .master(&quot;local[6]&quot;) .getOrCreate() //2.导入spark定义隐式增强|转换 import spark.implicits._ //3.创建dataset val lines = Array(&quot;this is a demo&quot;, &quot;hello spark&quot;) val wordRDD = spark.sparkContext .makeRDD(lines) .flatMap(_.split(&quot;\\\\s+&quot;)) .map((_, 1)) val ds: Dataset[(String, Int)] = wordRDD.toDS() //4.对Dataset执行sql算子操作 ds.groupBy($&quot;_1&quot;) //无类型操作 .sum(&quot;_2&quot;) .as(&quot;total&quot;) .withColumnRenamed(&quot;_1&quot;, &quot;word&quot;) .withColumnRenamed(&quot;sum(_2)&quot;, &quot;total&quot;) .show() //5.关闭spark spark.stop() } 创建字符统计（strong typed） def main(args: Array[String]): Unit = { //1.创建SparkSeesion val spark = SparkSession .builder() .appName(&quot;wordcount&quot;) .master(&quot;local[6]&quot;) .getOrCreate() //2.导入spark定义隐式增强|转换 import spark.implicits._ //3.创建dataset val lines = Array(&quot;this is a demo&quot;, &quot;hello spark&quot;) val wordRDD = spark.sparkContext .makeRDD(lines) .flatMap(_.split(&quot;\\\\s+&quot;)) .map((_, 1)) val ds: Dataset[(String, Int)] = wordRDD.toDS() //4.对Dataset执行sql算子操作 ds.groupByKey(t =&gt; t._1) .agg(typed.sum[(String, Int)](tuple =&gt; tuple._2).name(&quot;total&quot;)) .show() //5.关闭spark spark.stop() } Dataset &amp; DataFrame实战 Dataset create Dataset类似于RDD，不同的是Spark SQL有一套自己的序列化规范独立于Spark RDD（Java/Kryo序列化）之上称为Encoders。不同于SparkRDD序列化，由于Dataset支持无类型操作，用户无需获取操作的类型，操作仅仅是列名，因为Spark SQL在执行算子操作的时候可以省略反序列化的步骤，继而提升程序执行 效率。 case-class // case-class case class Person(id: Int, name: String, age: Int, sex: Boolean) /** * case-class */ val person: Dataset[Person] = List( Person(1, &quot;zhangsan&quot;, 18, true), Person(2, &quot;lisi&quot;, 28, true) ) .toDS() person.select($&quot;id&quot;, $&quot;name&quot;) .show() 注： 加入隐式转换 import spark.implicits._ 该语句需要放在获取spark对象的语句之后 case class Person(id: Int, name: String, age: Int, sex: Boolean) 的定义需要放在方法的作用域之外（即Scala/Java的成员变量位置） 如果case类是java编写，则java bean需要实现 Serializable，以及增加get、set方法 结果: +---+--------+ | id| name| +---+--------+ | 1|zhangsan| | 2| lisi| +---+--------+ Tuple(元组) /** * tuple */ val person: Dataset[(Int, String, Int, Boolean)] = List( (1, &quot;zhangsan&quot;, 18, true), (2, &quot;lisi&quot;, 28, true) ) .toDS() person.select($&quot;_1&quot;, $&quot;_2&quot;).show() 结果: +---+--------+ | _1| _2| +---+--------+ | 1|zhangsan| | 2| lisi| +---+--------+ json数据 数据： {&quot;name&quot;:&quot;张三&quot;,&quot;age&quot;:18} {&quot;name&quot;:&quot;李四&quot;,&quot;age&quot;:28} {&quot;name&quot;:&quot;王五&quot;,&quot;age&quot;:38} //数值默认是long类型 case class User(name: String, age: Long) /** * json格式 */ spark.read .json(&quot;file:///Users/mashikang/IdeaProjects/spark_sql/src/main/resources/json/&quot;) .as[User] .show() 结果: +---+----+ |age|name| +---+----+ | 18|张三| | 28|李四| | 38|王五| +---+----+ RDD Tuple(元组) /** * RDD 元组 */ val userRDD = spark.sparkContext.makeRDD(List((1,&quot;张三&quot;,true,18,15000.0))) userRDD.toDS().show() 结果: +---+----+----+---+-------+ | _1| _2| _3| _4| _5| +---+----+----+---+-------+ | 1|张三|true| 18|15000.0| +---+----+----+---+-------+ case-class // case-class case class Person(id: Int, name: String, age: Int, sex: Boolean) /** * RDD case-class */ val userRDD = spark.sparkContext.makeRDD(List(Person(1,&quot;张三&quot;,18,true))) userRDD.toDS().show() 结果: +---+----+---+----+ | id|name|age| sex| +---+----+---+----+ | 1|张三| 18|true| +---+----+---+----+ Dataframe create DataFrame是一个命名列的数据集，用户可以直接操作 column 因此几乎所有DataFrame推荐操作都是无类型操作 。用户也可以把一个DataFrame看做是 Dataset[Row] 类型的数据集。 json文件 /** * json */ val dataFrame: DataFrame = spark.read .json(&quot;file:///Users/mashikang/IdeaProjects/spark_sql/src/main/resources/json/&quot;) dataFrame.printSchema() dataFrame.show() 结果: root |-- age: long (nullable = true) |-- name: string (nullable = true) +---+----+ |age|name| +---+----+ | 18|张三| | 28|李四| | 38|王五| +---+----+ case-class case class User(id:Int,name:String,sex:Boolean) /** * case-class */ var userDF=List(User(1,&quot;张三&quot;,true)) .toDF() userDF.show() 结果: +---+----+----+ | id|name| sex| +---+----+----+ | 1|张三|true| +---+----+----+ Tuple(元组) /** * Tuple */ var userDF=List((1,&quot;张三&quot;,true)) .toDF(&quot;id&quot;,&quot;name&quot;,&quot;sex&quot;) userDF.show() 结果: +---+----+----+ | id|name| sex| +---+----+----+ | 1|张三|true| +---+----+----+ RDD转换 Tuple /** * RDD Tuple */ var userDF = spark.sparkContext.parallelize(List((1, &quot;张三&quot;, true))) .toDF(&quot;id&quot;, &quot;name&quot;, &quot;sex&quot;) //可以指定列 userDF.show() 结果: +---+----+----+ | id|name| sex| +---+----+----+ | 1|张三|true| +---+----+----+ case-class /** * RDD case-class */ var userDF = spark.sparkContext.parallelize(List(User(1, &quot;张三&quot;, true))) .toDF() userDF.show() 结果: +---+----+----+ | id|name| sex| +---+----+----+ | 1|张三|true| +---+----+----+ RDD[Row]类型转为DataFrame /** * RDD[Row]转换 */ var userRDD: RDD[Row] = spark.sparkContext.parallelize(List(User(1, &quot;张三&quot;, true))) .map(u =&gt; Row(u.id, u.name, u.sex)) var schema = new StructType() .add(&quot;id&quot;, IntegerType) .add(&quot;name&quot;, StringType) .add(&quot;sex&quot;, BooleanType) var userDF = spark.createDataFrame(userRDD, schema) userDF.show() 结果: +---+----+----+ | id|name| sex| +---+----+----+ | 1|张三|true| +---+----+----+ RDD case-class /** * RDD case-class */ var userRDD: RDD[User] = spark.sparkContext .makeRDD(List(User(1, &quot;张三&quot;, true))) var userDF = spark.createDataFrame(userRDD) userDF.show() 结果: +---+----+----+ | id|name| sex| +---+----+----+ | 1|张三|true| +---+----+----+ RDD Tuple /** * RDD Tuple */ var userRDD:RDD[(Int,String,Boolean)]=spark.sparkContext .makeRDD(List((1,&quot;张三&quot;,true))) var userDF=spark.createDataFrame(userRDD) userDF.show() 结果: +---+----+----+ | _1| _2| _3| +---+----+----+ | 1|张三|true| +---+----+----+ DataFrame Operations（Untyped）DataFrame无类型操作 printSchema 打印Dataframe的表结构（表头） var df = List((1, &quot;张三&quot;, true)) .toDF(&quot;id&quot;, &quot;name&quot;, &quot;sex&quot;) df.printSchema() root |-- id: integer (nullable = false) |-- name: string (nullable = true) |-- sex: boolean (nullable = false) show var df = List( (1, &quot;zs&quot;, true, 1, 15000), (2, &quot;ls&quot;, false, 1, 15000) ) .toDF(&quot;id&quot;, &quot;name&quot;, &quot;sex&quot;, &quot;dept&quot;, &quot;salary&quot;) df.select($&quot;id&quot;, $&quot;name&quot;, $&quot;salary&quot;) .show() +---+----+------+ | id|name|salary| +---+----+------+ | 1| zs| 15000| | 2| ls| 15000| +---+----+------+ select var df = List( (1, &quot;zs&quot;, true, 1, 15000), (2, &quot;ls&quot;, false, 1, 18000) ) .toDF(&quot;id&quot;, &quot;name&quot;, &quot;sex&quot;, &quot;dept&quot;, &quot;salary&quot;) df.select($&quot;id&quot;,$&quot;name&quot;,$&quot;sex&quot;,$&quot;dept&quot;,$&quot;salary&quot; * 12 as &quot;annual_salary&quot;) .show() +---+----+-----+----+-------------+ | id|name| sex|dept|annual_salary| +---+----+-----+----+-------------+ | 1| zs| true| 1| 180000| | 2| ls|false| 1| 216000| +---+----+-----+----+-------------+ selectExpr var df = List( (1, &quot;zs&quot;, true, 1, 15000), (2, &quot;ls&quot;, false, 1, 18000) ) .toDF(&quot;id&quot;, &quot;name&quot;, &quot;sex&quot;, &quot;dept&quot;, &quot;salary&quot;) //等价 df.select($&quot;id&quot;,$&quot;name&quot;,$&quot;sex&quot;,$&quot;dept&quot;,$&quot;salary&quot; * 12 as &quot;annual_salary&quot;) df.selectExpr(&quot;id&quot;, &quot;name&quot;, &quot;sex&quot;, &quot;dept&quot;, &quot;salary * 12 as annual_salary&quot;) .show() +---+----+-----+----+-------------+ | id|name| sex|dept|annual_salary| +---+----+-----+----+-------------+ | 1| zs| true| 1| 180000| | 2| ls|false| 1| 216000| +---+----+-----+----+-------------+ withColumn var df = List( (1, &quot;zs&quot;, true, 1, 15000), (2, &quot;ls&quot;, false, 1, 18000) ) .toDF(&quot;id&quot;, &quot;name&quot;, &quot;sex&quot;, &quot;dept&quot;, &quot;salary&quot;) df.select($&quot;id&quot;, $&quot;name&quot;, $&quot;sex&quot;, $&quot;dept&quot;, $&quot;salary&quot;) .withColumn(&quot;annual_salary&quot;, $&quot;salary&quot; * 12) .show() +---+----+-----+----+------+-------------+ | id|name| sex|dept|salary|annual_salary| +---+----+-----+----+------+-------------+ | 1| zs| true| 1| 15000| 180000| | 2| ls|false| 1| 18000| 216000| +---+----+-----+----+------+-------------+ withColumnRenamed var df = List( (1, &quot;zs&quot;, true, 1, 15000), (2, &quot;ls&quot;, false, 1, 18000) ) .toDF(&quot;id&quot;, &quot;name&quot;, &quot;sex&quot;, &quot;dept&quot;, &quot;salary&quot;) df.select($&quot;id&quot;, $&quot;name&quot;, $&quot;sex&quot;, $&quot;dept&quot;, $&quot;salary&quot;) .withColumn(&quot;annula_salary&quot;, $&quot;salary&quot; * 12) .withColumnRenamed(&quot;dept&quot;, &quot;department&quot;) .withColumnRenamed(&quot;name&quot;, &quot;username&quot;) .show() +---+--------+-----+----------+------+-------------+ | id|username| sex|department|salary|annula_salary| +---+--------+-----+----------+------+-------------+ | 1| zs| true| 1| 15000| 180000| | 2| ls|false| 1| 18000| 216000| +---+--------+-----+----------+------+-------------+ drop var df = List( (1, &quot;zs&quot;, true, 1, 15000), (2, &quot;ls&quot;, false, 1, 18000) ) .toDF(&quot;id&quot;, &quot;name&quot;, &quot;sex&quot;, &quot;dept&quot;, &quot;salary&quot;) df.select($&quot;id&quot;,$&quot;name&quot;,$&quot;sex&quot;,$&quot;dept&quot;,$&quot;salary&quot;) .withColumn(&quot;annula_salary&quot;,$&quot;salary&quot; * 12) .withColumnRenamed(&quot;dept&quot;,&quot;department&quot;) .withColumnRenamed(&quot;name&quot;,&quot;username&quot;) .drop(&quot;sex&quot;) .show() +---+--------+----------+------+-------------+ | id|username|department|salary|annula_salary| +---+--------+----------+------+-------------+ | 1| zs| 1| 15000| 180000| | 2| ls| 1| 18000| 216000| +---+--------+----------+------+-------------+ dropDuplicates var df = List( (1, &quot;zs&quot;, true, 1, 15000), (2, &quot;ls&quot;, false, 1, 18000), (3, &quot;ww&quot;, false, 1, 19000), (4, &quot;zl&quot;, false, 1, 18000) ) .toDF(&quot;id&quot;, &quot;name&quot;, &quot;sex&quot;, &quot;dept&quot;, &quot;salary&quot;) df.select($&quot;id&quot;, $&quot;name&quot;, $&quot;sex&quot;, $&quot;dept&quot;, $&quot;salary&quot;) .dropDuplicates(&quot;sex&quot;, &quot;salary&quot;) .show() +---+----+-----+----+------+ | id|name| sex|dept|salary| +---+----+-----+----+------+ | 3| ww|false| 1| 19000| | 1| zs| true| 1| 15000| | 2| ls|false| 1| 18000| +---+----+-----+----+------+ orderBy|sort var df = List( (1, &quot;zs&quot;, true, 1, 15000), (2, &quot;ls&quot;, false, 2, 18000), (3, &quot;ww&quot;, false, 2, 14000), (4, &quot;zl&quot;, false, 1, 18000), (5, &quot;zl&quot;, false, 1, 16000) ) .toDF(&quot;id&quot;, &quot;name&quot;, &quot;sex&quot;, &quot;dept&quot;, &quot;salary&quot;) df.select($&quot;id&quot;, $&quot;name&quot;, $&quot;sex&quot;, $&quot;dept&quot;, $&quot;salary&quot;) .orderBy($&quot;salary&quot; desc, $&quot;id&quot; asc) //.sort($&quot;salary&quot; desc,$&quot;id&quot; asc) .show() +---+----+-----+----+------+ | id|name| sex|dept|salary| +---+----+-----+----+------+ | 2| ls|false| 2| 18000| | 4| zl|false| 1| 18000| | 5| zl|false| 1| 16000| | 1| zs| true| 1| 15000| | 3| ww|false| 2| 14000| +---+----+-----+----+------+ groupBy var df = List( (1, &quot;zs&quot;, true, 1, 15000), (2, &quot;ls&quot;, false, 2, 18000), (3, &quot;ww&quot;, false, 2, 14000), (4, &quot;zl&quot;, false, 1, 18000), (5, &quot;zl&quot;, false, 1, 16000) ) .toDF(&quot;id&quot;, &quot;name&quot;, &quot;sex&quot;, &quot;dept&quot;, &quot;salary&quot;) df.select($&quot;id&quot;, $&quot;name&quot;, $&quot;sex&quot;, $&quot;dept&quot;, $&quot;salary&quot;) .groupBy($&quot;dept&quot;) .max(&quot;salary&quot;) .show() +----+-----------+ |dept|max(salary)| +----+-----------+ | 1| 18000| | 2| 18000| +----+-----------+ 类似的算子还有 max、min、avg|mean、sum、count agg var df = List( (1, &quot;zs&quot;, true, 1, 15000), (2, &quot;ls&quot;, false, 2, 18000), (3, &quot;ww&quot;, false, 2, 14000), (4, &quot;zl&quot;, false, 1, 18000), (5, &quot;zl&quot;, false, 1, 16000) ) .toDF(&quot;id&quot;, &quot;name&quot;, &quot;sex&quot;, &quot;dept&quot;, &quot;salary&quot;) import org.apache.spark.sql.functions._ df.select($&quot;id&quot;, $&quot;name&quot;, $&quot;sex&quot;, $&quot;dept&quot;, $&quot;salary&quot;) .groupBy($&quot;dept&quot;) .agg(max(&quot;salary&quot;) as &quot;max_salary&quot;, avg(&quot;salary&quot;) as &quot;avg_salary&quot;) .show() +----+----------+------------------+ |dept|max_salary| avg_salary| +----+----------+------------------+ | 1| 18000|16333.333333333334| | 2| 18000| 16000.0| +----+----------+------------------+ agg 还可以传递表达式 var df = List( (1, &quot;zs&quot;, true, 1, 15000), (2, &quot;ls&quot;, false, 2, 18000), (3, &quot;ww&quot;, false, 2, 14000), (4, &quot;zl&quot;, false, 1, 18000), (5, &quot;zl&quot;, false, 1, 16000) ) .toDF(&quot;id&quot;, &quot;name&quot;, &quot;sex&quot;, &quot;dept&quot;, &quot;salary&quot;) import org.apache.spark.sql.functions._ df.select($&quot;id&quot;, $&quot;name&quot;, $&quot;sex&quot;, $&quot;dept&quot;, $&quot;salary&quot;) .groupBy($&quot;dept&quot;) .agg(Map(&quot;salary&quot;-&gt;&quot;max&quot;,&quot;id&quot;-&gt;&quot;count&quot;)) .show() +----+-----------+---------+ |dept|max(salary)|count(id)| +----+-----------+---------+ | 1| 18000| 3| | 2| 18000| 2| +----+-----------+---------+ limit var df = List( (1, &quot;zs&quot;, true, 1, 15000), (2, &quot;ls&quot;, false, 2, 18000), (3, &quot;ww&quot;, false, 2, 14000), (4, &quot;zl&quot;, false, 1, 18000), (5, &quot;zl&quot;, false, 1, 16000) ) .toDF(&quot;id&quot;, &quot;name&quot;, &quot;sex&quot;, &quot;dept&quot;, &quot;salary&quot;) df.select($&quot;id&quot;, $&quot;name&quot;, $&quot;sex&quot;, $&quot;dept&quot;, $&quot;salary&quot;) .orderBy($&quot;id&quot; desc) .limit(4) .show() +---+----+-----+----+------+ | id|name| sex|dept|salary| +---+----+-----+----+------+ | 5| zl|false| 1| 16000| | 4| zl|false| 1| 18000| | 3| ww|false| 2| 14000| | 2| ls|false| 2| 18000| +---+----+-----+----+------+ where var df = List( (1, &quot;zs&quot;, true, 1, 15000), (2, &quot;ls&quot;, false, 2, 18000), (3, &quot;ww&quot;, false, 2, 14000), (4, &quot;zl&quot;, false, 1, 18000), (5, &quot;win7&quot;, false, 1, 16000) ) .toDF(&quot;id&quot;, &quot;name&quot;, &quot;sex&quot;, &quot;dept&quot;, &quot;salary&quot;) df.select($&quot;id&quot;, $&quot;name&quot;, $&quot;sex&quot;, $&quot;dept&quot;, $&quot;salary&quot;) //where(&quot;(name like '%s%' and salary &gt; 15000) or name = 'win7'&quot;) .where(($&quot;name&quot; like &quot;%s%&quot; and $&quot;salary&quot; &gt; 15000) or $&quot;name&quot; === &quot;win7&quot;) .show() +---+----+-----+----+------+ | id|name| sex|dept|salary| +---+----+-----+----+------+ | 2| ls|false| 2| 18000| | 5|win7|false| 1| 16000| +---+----+-----+----+------+ pivot（行转列） var scoreDF = List( (1, &quot;math&quot;, 85), (1, &quot;chinese&quot;, 80), (1, &quot;english&quot;, 90), (2, &quot;math&quot;, 90), (2, &quot;chinese&quot;, 80) ) .toDF(&quot;id&quot;, &quot;course&quot;, &quot;score&quot;) import org.apache.spark.sql.functions._ //select id,max(case course when 'math' then score else 0 end )as math ,max(case course when 'chinese' then score else 0 end) as chinese from t_course group by id; scoreDF.selectExpr(&quot;id&quot;, &quot;case course when 'math' then score else 0 end as math&quot;, &quot;case course when 'chinese' then score else 0 end as chinese&quot;, &quot;case course when 'english' then score else 0 end as english&quot;) .groupBy(&quot;id&quot;) .agg(max($&quot;math&quot;), max($&quot;chinese&quot;), max($&quot;english&quot;)) .show() +---+---------+------------+------------+ | id|max(math)|max(chinese)|max(english)| +---+---------+------------+------------+ | 1| 85| 80| 90| | 2| 90| 80| 0| +---+---------+------------+------------+ 简易写法 var scoreRDD = List( (1, &quot;math&quot;, 85), (1, &quot;chinese&quot;, 80), (1, &quot;english&quot;, 90), (2, &quot;math&quot;, 90), (2, &quot;chinese&quot;, 80) ) scoreRDD.toDF(&quot;id&quot;, &quot;course&quot;, &quot;score&quot;) .groupBy(&quot;id&quot;) // 行转列 可选值 .pivot(&quot;course&quot;, scoreRDD.map(t =&gt; t._2).distinct) .max(&quot;score&quot;) .show() +---+----+-------+-------+ | id|math|chinese|english| +---+----+-------+-------+ | 1| 85| 80| 90| | 2| 90| 80| null| +---+----+-------+-------+ na(替换当前为null的值) var scoreRDD = List( (1, &quot;math&quot;, 85), (1, &quot;chinese&quot;, 80), (1, &quot;english&quot;, 90), (2, &quot;math&quot;, 90), (2, &quot;chinese&quot;, 80), (3, &quot;math&quot;, 100) ) scoreRDD.toDF(&quot;id&quot;, &quot;course&quot;, &quot;score&quot;) .groupBy(&quot;id&quot;) // 行转列 可选值 .pivot(&quot;course&quot;, scoreRDD.map(t =&gt; t._2).distinct) .max(&quot;score&quot;) .na.fill(Map(&quot;english&quot; -&gt; -1, &quot;chinese&quot; -&gt; 0)) .show() +---+----+-------+-------+ | id|math|chinese|english| +---+----+-------+-------+ | 1| 85| 80| 90| | 3| 100| 0| -1| | 2| 90| 80| -1| +---+----+-------+-------+ join case class UserCost(id: Int, category: String, totalCost: Double) case class User(id: Int, name: String, sex: Boolean, age: Int, salary: Double) var userCostDF = spark.sparkContext .parallelize(List( UserCost(1, &quot;电脑配件&quot;, 100), UserCost(1, &quot;母婴用品&quot;, 100), UserCost(1, &quot;生活用品&quot;, 100), UserCost(2, &quot;居家美食&quot;, 79), UserCost(2, &quot;消费电子&quot;, 80), UserCost(2, &quot;生活用品&quot;, 100) )) .toDF() .withColumnRenamed(&quot;id&quot;, &quot;uid&quot;) val categories = userCostDF .select(&quot;category&quot;) .as[(String)] .rdd .distinct .collect() var userDF = spark.sparkContext .parallelize(List( User(1, &quot;张晓三&quot;, true, 18, 15000), User(2, &quot;李晓四&quot;, true, 18, 18000), User(3, &quot;王晓五&quot;, false, 18, 10000) )) .toDF() userDF.join(userCostDF, $&quot;id&quot; === $&quot;uid&quot;, &quot;left_outer&quot;) .drop(&quot;uid&quot;) .groupBy(&quot;id&quot;, &quot;name&quot;) .pivot($&quot;category&quot;, categories) .sum(&quot;totalCost&quot;) .na.fill(0.0) .show() +---+------+--------+--------+--------+--------+--------+ | id| name| 电脑配件| 生活用品| 母婴用品| 居家美食| 消费电子| +---+------+--------+--------+--------+--------+--------+ | 1|张晓三| 100.0| 100.0| 100.0| 0.0| 0.0| | 3|王晓五| 0.0| 0.0| 0.0| 0.0| 0.0| | 2|李晓四| 0.0| 100.0| 0.0| 79.0| 80.0| +---+------+--------+--------+--------+--------+--------+ cube(多维度) import org.apache.spark.sql.functions._ List( (110, 50, 80, 80), (120, 60, 95, 75), (120, 50, 96, 70) ) .toDF(&quot;height&quot;, &quot;weight&quot;, &quot;IQ&quot;, &quot;EQ&quot;) .cube($&quot;height&quot;, $&quot;weight&quot;) .agg(avg(&quot;IQ&quot;), avg(&quot;EQ&quot;)) .show() +------+------+-----------------+-------+ |height|weight| avg(IQ)|avg(EQ)| +------+------+-----------------+-------+ | 110| 50| 80.0| 80.0| | 120| null| 95.5| 72.5| | 120| 60| 95.0| 75.0| | null| 60| 95.0| 75.0| | null| null|90.33333333333333| 75.0| | 120| 50| 96.0| 70.0| | 110| null| 80.0| 80.0| | null| 50| 88.0| 75.0| +------+------+-----------------+-------+ Dataset Oprations (Strong typed) 数据集操作-强类型 由于强类型操作都是基于类型操作，Spark SQL的操作都是推荐使用Dataframe基于列操作，因此一般情况下不推荐使用。 val lines = Array(&quot;this is a demo&quot;, &quot;hello spark&quot;) val wordRDD = spark.sparkContext.makeRDD(lines) .flatMap(_.split(&quot;\\\\s+&quot;)) .map((_, 1)) import org.apache.spark.sql.expressions.scalalang.typed val ds: Dataset[(String, Int)] = wordRDD.toDS() ds.groupByKey(t =&gt; t._1) .agg(typed.sum[(String, Int)](tuple =&gt; tuple._2).name(&quot;total&quot;)) .filter(tuple =&gt; tuple._1.contains(&quot;o&quot;)) .show() +-----+-----+ |value|total| +-----+-----+ |hello| 1.0| | demo| 1.0| +-----+-----+ ","link":"https://mask0407.github.io/spark05/"},{"title":"Spark入门(五)——Spark Streaming","content":" Spark Streaming(流处理) 什么是流处理？ 快速入门 概念介绍 初始化 StreamingContext Discretized Streams (DStreams) InputStream &amp; Receivers Basic Sources Queue of RDDs as a Stream(测试) Advance Source Kafka Spark Stream 算子 UpdateStateByKey 重故障中|重启中状态恢复 窗口函数 - window Output Operations (Spark Streaming（流处理）) Spark Streaming(流处理) 什么是流处理？ 一般流式计算会与批量计算相比较。在流式计算模型中，输入是持续的，可以认为在时间上是无界的，也就意味着，永远拿不到全量数据去做计算。同时，计算结果是持续输出的，也即计算结果在时间上也是无界的。流式计算一般对实时性要求较高，同时一般是先定义目标计算，然后数据到来之后将计算逻辑应用于数据。同时为了提高计算效率，往往尽可能采用增量计算代替全量计算。批量处理模型中，一般先有全量数据集，然后定义计算逻辑，并将计算应用于全量数据。特点是全量计算，并且计算结果一次性全量输出。 Spark Streaming是核心Spark API的扩展，可实现实时数据流的可扩展，高吞吐量，容错流处理。数据可以从许多来源（如Kafka，Flume，Kinesis或TCP套接字）中获取，并且可以使用以高级函数（如map，reduce，join和window）表示的复杂算法进行处理。最后，处理后的数据可以推送到文件系统，数据库和实时dashboards。 在内部，它的工作原理如下。 Spark Streaming接收实时输入数据流并将数据分成批处理，然后由Spark引擎处理以批量生成最终结果流。 Spark Streaming提供称为离散流或DStream的高级抽象，表示连续的数据流。DStream可以从来自Kafka，Flume和Kinesis等源的输入数据流创建，也可以通过在其他DStream上应用高级操作来创建。在内部DStream表示为一系列RDD。 备注:Spark Streaming 因为底层使用批处理模拟流处理，因此在实时性上大打折扣，这就导致了Spark Streaming在流处理领域有者着先天的劣势。虽然Spark Streaming 在实时性上不如一些专业的流处理引擎(Storm/Flink)但是Spark Stream在使用吸取RDD设计经验，提供了比较友好的API算子，使得使用RDD做批处理的程序员可以平滑的过渡到流处理。 针对于Spark Streaming的微观的批处理问题，目前大数据处理领域又诞生了新秀Flink，该大数据处理引擎，在API易用性上和实时性上都有一定的兼顾，但是与spark最大的差异是Flink底层的处理引擎是流处理引擎，因此Flink天生就是流处理，但是Spark因为底层是批处理，导致了Spark Streaming在实时性上就没法和其他的专业流处理框架对比了。 快速入门 pom.xml &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-core_2.11&lt;/artifactId&gt; &lt;version&gt;2.4.3&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-streaming_2.11&lt;/artifactId&gt; &lt;version&gt;2.4.3&lt;/version&gt; &lt;/dependency&gt; SparkStreamWordCounts val conf = new SparkConf().setMaster(&quot;local[2]&quot;).setAppName(&quot;NetworkWordCount&quot;) val ssc = new StreamingContext(conf, Seconds(5)) ssc.sparkContext.setLogLevel(&quot;FATAL&quot;)//关闭日志打印 ssc.socketTextStream(&quot;CentOS&quot;,9999) .flatMap(_.split(&quot; &quot;)) .map((_,1)) .reduceByKey(_+_) .print() ssc.start() ssc.awaitTermination()//等待系统发送指定关闭流计算 需要注意：[root@CentOS ~]# yum install -y nc启动nc服务[root@CentOS ~]# nc -lk 9999,注意在调用改程序的时候，需要设置local[n],n&gt;1 概念介绍 通过上述案例的运行，现在我们来一起探讨一些流处理的概念。在处理流计算的时候，除去spark-core依赖以外我们还需要引入spark-streaming模块。要从Spark Streaming核心API中不存在的Kafka，Flume和Kinesis等源中提取数据，您必须将相应的工件spark-streaming-xxx_2.11添加到依赖项中。xxx例如Kafka &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-streaming-kafka-0-10_2.11&lt;/artifactId&gt; &lt;version&gt;2.4.3&lt;/version&gt; &lt;/dependency&gt; 初始化 StreamingContext 要初始化Spark Streaming程序，必须创建一个StreamingContext对象，它是所有Spark Streaming功能的主要入口点。 import org.apache.spark._ import org.apache.spark.streaming._ val conf = new SparkConf().setAppName(appName).setMaster(master) val ssc = new StreamingContext(conf, Seconds(1)) appName参数是应用程序在集群UI上显示的名称。 master是Spark，YARN群集URL，或者是在本地模式下运行的特殊local [*]字符串。实际上，在群集上运行时，您不希望在程序中对master进行硬编码，而是使用spark-submit启动指定–master配置。但是，对于本地测试和单元测试，您可以传递local [*]以在进程中运行Spark Streaming（系统会自动检测本地系统的核的数目）。 请注意ssc会在内部创建一个SparkContext（所有Spark功能的起点），如果需要获取SparkContext对象用户可以调用ssc.sparkContext访问。例如用户使用SparkContext关闭日志。 val conf = new SparkConf() .setMaster(&quot;local[5]&quot;) .setAppName(&quot;wordCount&quot;) val ssc = new StreamingContext(conf,Seconds(1)) //关闭其他日志 ssc.sparkContext.setLogLevel(&quot;FATAL&quot;) 必须根据应用程序的延迟要求和可用的群集资源设置批处理间隔。要使群集上运行的Spark Streaming应用程序保持稳定，系统应该能够以接收数据的速度处理数据。换句话说，批处理数据应该在生成时尽快处理。通过监视流式Web UI中的处理时间可以找到是否适用于应用程序，其中批处理时间应小于批处理间隔。 val conf = new SparkConf() .setMaster(&quot;local[5]&quot;) .setAppName(&quot;wordCount&quot;) val sc = new SparkContext(conf) val ssc = new StreamingContext(sc,Seconds(1)) 当用户创建完StreamingContext对象之后，用户需要完成以下步骤 定义数据源，用于创建输入的 DStreams. 定义流计算算子，通过定义这些算子实现对DStream数据转换和输出 调用streamingContext.start()启动数据. 等待计算结束 (人工结束或者是错误) 调用 streamingContext.awaitTermination(). 如果是人工结束，程序应当调用 streamingContext.stop()结束流计算. 重要因素需要谨记 一旦流计算启动，无法再往计算流程中添加任何计算算子 一旦SparkContext对象被stop后，无法重启。 一个JVM系统中只能实例化一个StreamingContext对象。 SparkContext被stop()后，内部创建的SparkContext也会被stop.如果仅仅是想Stop StreamingContext, 可以设置stop() 中的可选参数 stopSparkContext=false即可. ssc.stop(stopSparkContext = false) 一个SparkContext 可以重复使用并且创建多个StreamingContexts, 前提是上一个启动的StreamingContext 被停止了(但是并没有关闭 SparkContext对象) 。 Discretized Streams (DStreams) Discretized Stream或DStream是Spark Streaming提供的基本抽象。它表示连续的数据流，可以是从源接收的输入数据流，也可以是通过转换输入流生成的已处理数据流。在内部，DStream由一系列连续的RDD表示，这是Spark对不可变分布式数据集的抽象。DStream中的每个RDD都包含来自特定时间间隔的数据，如下图所示。 应用于DStream的任何操作都转换为底层RDD上的操作。例如，在先前Quick Start示例中，flatMap操作应用于行DStream中的每个RDD以生成单词DStream的RDD。如下图所示。 这些底层RDD转换由Spark引擎计算。 DStream操作隐藏了大部分细节，并为开发人员提供了更高级别的API以方便使用。 InputStream &amp; Receivers Input DStream 表示流计算的输入，Spark中默认提供了两类的InputStream： Baisc Source ：例如 filesystem、scoket Advance Source：例如：Kafka、Flume等外围系统的数据。 除filesystem以外，其他的Input DStream默认都会占用一个Core(计算资源)，在测试或者生产环境下，分配给计算应用的Core数目必须大于Receivers个数。（本质上除filesystem源以外，其他的输入都是Receiver抽象类的实现。）了例如socketTextStream底层封装了SocketReceiver Basic Sources 因为在快速入门案例中已经使用了socketTextStream，后续我们只测试一下filesystem对于从与HDFS API兼容的任何文件系统（即HDFS，S3，NFS等）上的文件读取数据，可以通过StreamingContext.fileStream [KeyClass，ValueClass，InputFormatClass]创建DStream。文件流不需要运行Receiver，因此不需要为接收文件数据分配任何core。对于简单的文本文件，最简单的方法是StreamingContext.textFileStream（dataDirectory） val conf = new SparkConf().setMaster(&quot;local[2]&quot;).setAppName(&quot;FileSystemWordCount&quot;) val ssc = new StreamingContext(conf, Seconds(5)) ssc.sparkContext.setLogLevel(&quot;FATAL&quot;)//关闭日志打印 val lines = ssc.textFileStream(&quot;hdfs://CentOS:9000/demo/words&quot;) lines.flatMap(_.split(&quot; &quot;)) .map((_,1)) .reduceByKey(_+_) .print() ssc.start() ssc.awaitTermination() 在HDFS上创建目录 [root@CentOS ~]# hdfs dfs -mkdir -p /demo/words [root@CentOS ~]# hdfs dfs -put install.log /demo/words Queue of RDDs as a Stream(测试) 为了使用测试数据测试Spark Streaming应用程序，还可以使用streamingContext.queueStream（queueOfRDDs）基于RDD队列创建DStream。推入队列的每个RDD将被视为DStream中的一批数据，并像流一样处理。 val conf = new SparkConf().setMaster(&quot;local[2]&quot;).setAppName(&quot;FileSystemWordCount&quot;) val ssc = new StreamingContext(conf, Seconds(1)) ssc.sparkContext.setLogLevel(&quot;FATAL&quot;)//关闭日志打印 val queue=new mutable.Queue[RDD[String]](); val lines = ssc.queueStream(queue) lines.flatMap(_.split(&quot; &quot;)) .map((_,1)) .reduceByKey(_+_) .print() ssc.start() for(i &lt;- 1 to 30){ queue += ssc.sparkContext.makeRDD(List(&quot;this is a demo&quot;,&quot;hello how are you&quot;)) Thread.sleep(1000) } ssc.stop() Advance Source Kafka pom.xml &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-streaming-kafka-0-10_2.11&lt;/artifactId&gt; &lt;version&gt;2.4.3&lt;/version&gt; &lt;/dependency&gt; Kafka对接Spark Streaming val conf = new SparkConf().setMaster(&quot;local[2]&quot;).setAppName(&quot;FileSystemWordCount&quot;) val ssc = new StreamingContext(conf, Seconds(1)) ssc.sparkContext.setLogLevel(&quot;FATAL&quot;)//关闭日志打印 val kafkaParams = Map[String, Object]( &quot;bootstrap.servers&quot; -&gt; &quot;CentOS:9092&quot;, &quot;key.deserializer&quot; -&gt; classOf[StringDeserializer], &quot;value.deserializer&quot; -&gt; classOf[StringDeserializer], &quot;group.id&quot; -&gt; &quot;group1&quot;, &quot;enable.auto.commit&quot; -&gt; (false: java.lang.Boolean) ) KafkaUtils.createDirectStream(ssc, LocationStrategies.PreferConsistent,//设置加载数据位置策略， Subscribe[String,String](Array(&quot;topic01&quot;),kafkaParams)) .map(record =&gt; record.value()) .flatMap(_.split(&quot; &quot;)) .map((_,1)) .reduceByKey(_+_) .print() ssc.start() ssc.awaitTermination() Spark Stream 算子 transform(func) 该算子可以将DStream的数据转变成RDD，用户操作流数据就像操作RDD感觉是一样的。 val conf = new SparkConf().setMaster(&quot;local[2]&quot;).setAppName(&quot;KafkaStreamWordCount&quot;) val ssc = new StreamingContext(conf, Seconds(1)) val kafkaParams = Map[String, Object]( &quot;bootstrap.servers&quot; -&gt; &quot;CentOS:9092&quot;, &quot;key.deserializer&quot; -&gt; classOf[StringDeserializer], &quot;value.deserializer&quot; -&gt; classOf[StringDeserializer], &quot;group.id&quot; -&gt; &quot;group1&quot;, &quot;enable.auto.commit&quot; -&gt; (false: java.lang.Boolean) ) val cacheRDD = ssc.sparkContext.makeRDD(List(&quot;001 zhangsan&quot;, &quot;002 lisi&quot;, &quot;003 zhaoliu&quot;)) .map(item =&gt; (item.split(&quot;\\\\s+&quot;)(0), item.split(&quot;\\\\s+&quot;)(1))) .distinct() .cache() //001 apple KafkaUtils.createDirectStream(ssc, LocationStrategies.PreferConsistent, Subscribe[String,String](Array(&quot;topic01&quot;),kafkaParams)) .map(record =&gt; record.value()) .map(value=&gt;{ val tokens = value.split(&quot;\\\\s+&quot;) (tokens(0),tokens(1)) }) .transform(rdd=&gt;{rdd.rightOuterJoin(cacheRDD)}) .filter(t=&gt; {t._2._1!=None}) .print() ssc.sparkContext.setLogLevel(&quot;FATAL&quot;)//关闭日志打印 ssc.start() ssc.awaitTermination() UpdateStateByKey val conf = new SparkConf().setMaster(&quot;local[2]&quot;).setAppName(&quot;KafkaStreamWordCount&quot;) val ssc = new StreamingContext(conf, Seconds(1)) ssc.checkpoint(&quot;hdfs://CentOS:9000/checkpoints&quot;)//在JVM启动参数中添加-D HADOOP_USER_NAME=root def updateFun(newValues: Seq[Int], runningCount: Option[Int]): Option[Int] = { var total= newValues.sum+runningCount.getOrElse(0) Some(total) } ssc.socketTextStream(&quot;CentOS&quot;,9999) .flatMap(_.split(&quot;\\\\s+&quot;)) .map((_,1)) .updateStateByKey(updateFun) .print() ssc.sparkContext.setLogLevel(&quot;FATAL&quot;)//关闭日志打印 ssc.start() ssc.awaitTermination() 因为UpdateStateByKey 算子每一次的输出都是全量输出，在做状态更新的时候代价较高，因此推荐大家使用mapWithState mapWithState val conf = new SparkConf().setMaster(&quot;local[2]&quot;).setAppName(&quot;KafkaStreamWordCount&quot;) val ssc = new StreamingContext(conf, Seconds(1)) ssc.checkpoint(&quot;hdfs://CentOS:9000/checkpoints&quot;) def updateFun(newValues: Seq[Int], runningCount: Option[Int]): Option[Int] = { var total= newValues.sum+runningCount.getOrElse(0) Some(total) } ssc.socketTextStream(&quot;CentOS&quot;,9999) .flatMap(_.split(&quot;\\\\s+&quot;)) .map((_,1)) .mapWithState(StateSpec.function((k:String,v:Option[Int],state:State[Int])=&gt;{ var total=0 if(state.exists()){ total=state.getOption().getOrElse(0) } total += v.getOrElse(1) state.update(total)//更新状态 (k,total) })) .print() ssc.sparkContext.setLogLevel(&quot;FATAL&quot;)//关闭日志打印 ssc.start() ssc.awaitTermination() 重故障中|重启中状态恢复 var checkpointPath=&quot;hdfs://CentOS:9000/checkpoints&quot;//设置检查点目录，一般将检查点目录存储在HDFS上 var ssc=StreamingContext.getOrCreate(checkpointPath,()=&gt;{//第一次启时候初始化，一旦书写完成后，无法进行修改！ println(&quot;=======init=======&quot;) val conf = new SparkConf().setMaster(&quot;local[2]&quot;).setAppName(&quot;KafkaStreamWordCount&quot;) val ssc = new StreamingContext(conf, Seconds(5)) ssc.checkpoint(checkpointPath) def updateFun(newValues: Seq[Int], runningCount: Option[Int]): Option[Int] = { var total= newValues.sum+runningCount.getOrElse(0) Some(total) } ssc.socketTextStream(&quot;CentOS&quot;,9999) .flatMap(_.split(&quot;\\\\s+&quot;)) .map((_,1)) .mapWithState(StateSpec.function((k:String,v:Option[Int],state:State[Int])=&gt;{ var total=0 if(state.exists()){ total=state.getOption().getOrElse(0) } total += v.getOrElse(1) state.update(total)//更新状态 (k,total) })) .checkpoint(Seconds(5))//设置状态持久化的频率，该频率不能高于 微批 拆分频率 ts&gt;=5s .print() ssc }) ssc.sparkContext.setLogLevel(&quot;FATAL&quot;)//关闭日志打印 ssc.start() ssc.awaitTermination() 窗口函数 - window Spark Streaming还提供窗口计算，允许您在滑动数据窗口上应用转换。下图说明了此滑动窗口。 以上描述了窗口长度是3个时间单位的微批,窗口的滑动间隔是2个时间单位的微批，注意：Spark的流处理中要求窗口的长度以及滑动间隔必须是微批的整数倍。 val conf = new SparkConf().setMaster(&quot;local[2]&quot;).setAppName(&quot;KafkaStreamWordCount&quot;) val ssc = new StreamingContext(conf, Seconds(1)) ssc.socketTextStream(&quot;CentOS&quot;,9999) .flatMap(_.split(&quot;\\\\s+&quot;)) .map((_,1)) .reduceByKeyAndWindow((v1:Int,v2:Int)=&gt; v1+v2,Seconds(5),Seconds(5)) .print() ssc.sparkContext.setLogLevel(&quot;FATAL&quot;)//关闭日志打印 ssc.start() ssc.awaitTermination() Output Operations foreachRDD(func) val conf = new SparkConf().setMaster(&quot;local[2]&quot;).setAppName(&quot;KafkaStreamWordCount&quot;) val ssc = new StreamingContext(conf, Seconds(1)) ssc.socketTextStream(&quot;CentOS&quot;,9999) .flatMap(_.split(&quot;\\\\s+&quot;)) .map((_,1)) .window(Seconds(5),Seconds(5)) .reduceByKey((v1:Int,v2:Int)=&gt; v1+v2) .foreachRDD(rdd=&gt;{ rdd.foreachPartition(items=&gt;{ var jedisPool=new JedisPool(&quot;CentOS&quot;,6379) val jedis = jedisPool.getResource val pipeline = jedis.pipelined()//jedis批处理 val map = items.map(t=&gt;(t._1,t._2+&quot;&quot;)).toMap.asJava pipeline.hmset(&quot;wordcount&quot;,map) pipeline.sync() jedis.close() }) }) ssc.sparkContext.setLogLevel(&quot;FATAL&quot;)//关闭日志打印 ssc.start() ssc.awaitTermination() ","link":"https://mask0407.github.io/spark04/"},{"title":"Spark入门(四)——Spark RDD算子使用方法","content":"Spark RDD算子 RDD算子实战 转换算子 map(function) 传入的集合元素进行RDD[T]转换 def map(f: T =&gt; U): org.apache.spark.rdd.RDD[U] scala&gt; sc.parallelize(List(1,2,3,4,5),3).map(item =&gt; item*2+&quot; &quot; ) res1: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[2] at map at &lt;console&gt;:25 scala&gt; sc.parallelize(List(1,2,3,4,5),3).map(item =&gt; item*2+&quot; &quot; ).collect res2: Array[String] = Array(&quot;2 &quot;, &quot;4 &quot;, &quot;6 &quot;, &quot;8&quot;, &quot;10 &quot;) filter(func) 将满足条件结果记录 def filter(f: T=&gt; Boolean): org.apache.spark.rdd.RDD[T] scala&gt; sc.parallelize(List(1,2,3,4,5),3).filter(item=&gt; item%2==0).collect res3: Array[Int] = Array(2, 4) flatMap(func) 将一个元素转换成元素的数组，然后对数组展开。def flatMap[U](f: T=&gt; TraversableOnce[U]): org.apache.spark.rdd.RDD[U] scala&gt; sc.parallelize(List(&quot;ni hao&quot;,&quot;hello spark&quot;),3).flatMap(line=&gt;line.split(&quot;\\\\s+&quot;)).collect res4: Array[String] = Array(ni, hao, hello, spark) mapPartitions(func) 与map类似，但在RDD的每个分区（块）上单独运行，因此当在类型T的RDD上运行时，func必须是Iterator &lt;T&gt; =&gt; Iterator &lt;U&gt;类型 def mapPartitions[U](f: Iterator[Int] =&gt; Iterator[U],preservesPartitioning: Boolean): org.apache.spark.rdd.RDD[U] scala&gt; sc.parallelize(List(1,2,3,4,5),3).mapPartitions(items=&gt; for(i&lt;-items;if(i%2==0)) yield i*2 ).collect() res7: Array[Int] = Array(4, 8) mapPartitionsWithIndex(func) 与mapPartitions类似，但也为func提供了表示分区索引的整数值，因此当在类型T的RDD上运行时，func必须是类型（Int，Iterator &lt;T&gt;）=&gt; Iterator &lt;U&gt;。 def mapPartitionsWithIndex[U](f: (Int, Iterator[T]) =&gt; Iterator[U],preservesPartitioning: Boolean): org.apache.spark.rdd.RDD[U] scala&gt; sc.parallelize(List(1,2,3,4,5),3).mapPartitionsWithIndex((p,items)=&gt; for(i&lt;-items) yield (p,i)).collect res11: Array[(Int, Int)] = Array((0,1), (1,2), (1,3), (2,4), (2,5)) sample(withReplacement, fraction, seed) 对数据进行一定比例的采样，使用withReplacement参数控制是否允许重复采样。 def sample(withReplacement: Boolean,fraction: Double,seed: Long): org.apache.spark.rdd.RDD[T] scala&gt; sc.parallelize(List(1,2,3,4,5,6,7),3).sample(false,0.7,1L).collect res13: Array[Int] = Array(1, 4, 6, 7) union(otherDataset) 返回一个新数据集，其中包含源数据集和参数中元素的并集。 def union(other: org.apache.spark.rdd.RDD[T]): org.apache.spark.rdd.RDD[T] scala&gt; var rdd1=sc.parallelize(Array((&quot;张三&quot;,1000),(&quot;李四&quot;,100),(&quot;赵六&quot;,300))) scala&gt; var rdd2=sc.parallelize(Array((&quot;张三&quot;,1000),(&quot;王五&quot;,100),(&quot;温七&quot;,300))) scala&gt; rdd1.union(rdd2).collect res16: Array[(String, Int)] = Array((张三,1000), (李四,100), (赵六,300), (张三,1000), (王五,100), (温七,300)) intersection(otherDataset) 返回包含源数据集和参数中元素交集的新RDD。 def intersection(other: org.apache.spark.rdd.RDD[T],numPartitions: Int): org.apache.spark.rdd.RDD[T] scala&gt; var rdd1=sc.parallelize(Array((&quot;张三&quot;,1000),(&quot;李四&quot;,100),(&quot;赵六&quot;,300))) scala&gt; var rdd2=sc.parallelize(Array((&quot;张三&quot;,1000),(&quot;王五&quot;,100),(&quot;温七&quot;,300))) scala&gt; rdd1.intersection(rdd2).collect res17: Array[(String, Int)] = Array((张三,1000)) distinct([numPartitions])) 返回包含源数据集的不同元素的新数据集。 scala&gt; sc.parallelize(List(1,2,3,3,5,7,2),3).distinct.collect res19: Array[Int] = Array(3, 1, 7, 5, 2) groupByKey([numPartitions]) 在（K，V）对的数据集上调用时，返回（K，Iterable &lt;V&gt;）对的数据集。 注意：如果要对每个键执行聚合（例如总和或平均值）进行分组，则使用reduceByKey或aggregateByKey将产生更好的性能。 注意：默认情况下，输出中的并行级别取决于父RDD的分区数。您可以传递可选的numPartitions参数来设置不同数量的任务。 scala&gt; sc.parallelize(List(&quot;ni hao&quot;,&quot;hello spark&quot;),3).flatMap(line=&gt;line.split(&quot;\\\\s+&quot;)).map(word=&gt;(word,1)).groupByKey(3).map(tuple=&gt;(tuple._1,tuple._2.sum)).collect reduceByKey(func, [numPartitions]) 当调用（K，V）对的数据集时，返回（K，V）对的数据集，其中使用给定的reduce函数func聚合每个键的值，该函数必须是类型（V，V）=&gt; V. scala&gt; sc.parallelize(List(&quot;ni hao&quot;,&quot;hello spark&quot;),3).flatMap(line=&gt;line.split(&quot;\\\\s+&quot;)).map(word=&gt;(word,1)).reduceByKey((v1,v2)=&gt;v1+v2).collect() res33: Array[(String, Int)] = Array((hao,1), (hello,1), (spark,1), (ni,1)) scala&gt; sc.parallelize(List(&quot;ni hao&quot;,&quot;hello spark&quot;),3).flatMap(line=&gt;line.split(&quot;\\\\s+&quot;)).map(word=&gt;(word,1)).reduceByKey(_+_).collect() res34: Array[(String, Int)] = Array((hao,1), (hello,1), (spark,1), (ni,1)) aggregateByKey(zeroValue)(seqOp, combOp, [numPartitions]) 当调用（K，V）对的数据集时，返回（K，U）对的数据集，其中使用给定的组合函数和中性“零”值聚合每个键的值。允许与输入值类型不同的聚合值类型，同时避免不必要的分配。 scala&gt; sc.parallelize(List(&quot;ni hao&quot;,&quot;hello spark&quot;),3).flatMap(line=&gt;line.split(&quot;\\\\s+&quot;)).map(word=&gt;(word,1)).aggregateByKey(0L)((z,v)=&gt;z+v,(u1,u2)=&gt;u1+u2).collect res35: Array[(String, Long)] = Array((hao,1), (hello,1), (spark,1), (ni,1)) sortByKey([ascending], [numPartitions]) 当调用K实现Ordered的（K，V）对数据集时，返回按键升序或降序排序的（K，V）对数据集，如布尔升序参数中所指定。 scala&gt; sc.parallelize(List(&quot;ni hao&quot;,&quot;hello spark&quot;),3).flatMap(line=&gt;line.split(&quot;\\\\s+&quot;)).map(word=&gt;(word,1)).aggregateByKey(0L)((z,v)=&gt;z+v,(u1,u2)=&gt;u1+u2).sortByKey(false).collect() res37: Array[(String, Long)] = Array((spark,1), (ni,1), (hello,1), (hao,1)) sortBy(func,[ascending], [numPartitions])** 对（K，V）数据集调用sortBy时，用户可以通过指定func指定排序规则，T =&gt; U 要求U必须实现Ordered接口 scala&gt; sc.parallelize(List(&quot;ni hao&quot;,&quot;hello spark&quot;),3).flatMap(line=&gt;line.split(&quot;\\\\s+&quot;)).map(word=&gt;(word,1)).aggregateByKey(0L)((z,v)=&gt;z+v,(u1,u2)=&gt;u1+u2).sortBy(_._2,true,2).collect res42: Array[(String, Long)] = Array((hao,1), (hello,1), (spark,1), (ni,1)) join 当调用类型（K，V）和（K，W）的数据集时，返回（K，（V，W））对的数据集以及每个键的所有元素对。通过leftOuterJoin，rightOuterJoin和fullOuterJoin支持外连接。 scala&gt; var rdd1=sc.parallelize(Array((&quot;001&quot;,&quot;张三&quot;),(&quot;002&quot;,&quot;李四&quot;),(&quot;003&quot;,&quot;王五&quot;))) scala&gt; var rdd2=sc.parallelize(Array((&quot;001&quot;,(&quot;apple&quot;,18.0)),(&quot;001&quot;,(&quot;orange&quot;,18.0)))) scala&gt; rdd1.join(rdd2).collect res43: Array[(String, (String, (String, Double)))] = Array((001,(张三,(apple,18.0))), (001,(张三,(orange,18.0)))) cogroup 当调用类型（K，V）和（K，W）的数据集时，返回（K，（Iterable ，Iterable ））元组的数据集。此操作也称为groupWith。 scala&gt; var rdd1=sc.parallelize(Array((&quot;001&quot;,&quot;张三&quot;),(&quot;002&quot;,&quot;李四&quot;),(&quot;003&quot;,&quot;王五&quot;))) scala&gt; var rdd2=sc.parallelize(Array((&quot;001&quot;,&quot;apple&quot;),(&quot;001&quot;,&quot;orange&quot;),(&quot;002&quot;,&quot;book&quot;))) scala&gt; rdd1.cogroup(rdd2).collect() res46: Array[(String, (Iterable[String], Iterable[String]))] = Array((001,(CompactBuffer(张三),CompactBuffer(apple, orange))), (002,(CompactBuffer(李四),CompactBuffer(book))), (003,(CompactBuffer(王五),CompactBuffer()))) cartesian 当调用类型为T和U的数据集时，返回（T，U）对的数据集（所有元素对）。 scala&gt; var rdd1=sc.parallelize(List(&quot;a&quot;,&quot;b&quot;,&quot;c&quot;)) scala&gt; var rdd2=sc.parallelize(List(1,2,3,4)) scala&gt; rdd1.cartesian(rdd2).collect() res47: Array[(String, Int)] = Array((a,1), (a,2), (a,3), (a,4), (b,1), (b,2), (b,3), (b,4), (c,1), (c,2), (c,3), (c,4)) coalesce(numPartitions) 将RDD中的分区数减少为numPartitions。过滤大型数据集后，可以使用概算子减少分区数。 scala&gt; sc.parallelize(List(&quot;ni hao&quot;,&quot;hello spark&quot;),3).coalesce(1).partitions.length res50: Int = 1 scala&gt; sc.parallelize(List(&quot;ni hao&quot;,&quot;hello spark&quot;),3).coalesce(1).getNumPartitions res51: Int = 1 repartition 随机重新调整RDD中的数据以创建更多或更少的分区。 scala&gt; sc.parallelize(List(&quot;a&quot;,&quot;b&quot;,&quot;c&quot;),3).mapPartitionsWithIndex((index,values)=&gt;for(i&lt;-values) yield (index,i) ).collect res52: Array[(Int, String)] = Array((0,a), (1,b), (2,c)) scala&gt; sc.parallelize(List(&quot;a&quot;,&quot;b&quot;,&quot;c&quot;),3).repartition(2).mapPartitionsWithIndex((index,values)=&gt;for(i&lt;-values) yield (index,i) ).collect res53: Array[(Int, String)] = Array((0,a), (0,c), (1,b)) 动作算子 collect 用在测试环境下，通常使用collect算子将远程计算的结果拿到Drvier端，注意一般数据量比较小，用于测试。 scala&gt; var rdd1=sc.parallelize(List(1,2,3,4,5),3).collect().foreach(println) saveAsTextFile 将计算结果存储在文件系统中，一般存储在HDFS上 scala&gt; sc.parallelize(List(&quot;ni hao&quot;,&quot;hello spark&quot;),3).flatMap(_.split(&quot;\\\\s+&quot;)).map((_,1)).reduceByKey(_+_).sortBy(_._2,false,3).saveAsTextFile(&quot;hdfs:///wordcounts&quot;) foreach 迭代遍历所有的RDD中的元素，通常是将foreach传递的数据写到外围系统中，比如说可以将数据写入到Hbase中。 scala&gt; sc.parallelize(List(&quot;ni hao&quot;,&quot;hello spark&quot;),3).flatMap(.split(&quot;\\s+&quot;)).map((,1)).reduceByKey(+).sortBy(_._2,false,3).foreach(println) (hao,1) (hello,1) (spark,1) (ni,1) 注意如果使用以上代码写数据到外围系统，会因为不断创建和关闭连接影响写入效率，一般推荐使用foreachPartition val lineRDD: RDD[String] = sc.textFile(&quot;file:///E:/demo/words/t_word.txt&quot;) lineRDD.flatMap(line=&gt;line.split(&quot; &quot;)) .map(word=&gt;(word,1)) .groupByKey() .map(tuple=&gt;(tuple._1,tuple._2.sum)) .sortBy(tuple=&gt;tuple._2,false,3) .foreachPartition(items=&gt;{ //创建连接 items.foreach(t=&gt;println(&quot;存储到数据库&quot;+t)) //关闭连接 }) 共享变量 变量广播 通常情况下，当一个RDD的很多操作都需要使用driver中定义的变量时，每次操作，driver都要把变量发送给worker节点一次，如果这个变量中的数据很大的话，会产生很高的传输负载，导致执行效率降低。使用广播变量可以使程序高效地将一个很大的只读数据发送给多个worker节点，而且对每个worker节点只需要传输一次，每次操作时executor可以直接获取本地保存的数据副本，不需要多次传输。 val conf = new SparkConf().setAppName(&quot;demo&quot;).setMaster(&quot;local[2]&quot;) val sc = new SparkContext(conf) val userList = List( &quot;001,张三,28,0&quot;, &quot;002,李四,18,1&quot;, &quot;003,王五,38,0&quot;, &quot;004,zhaoliu,38,-1&quot; ) val genderMap = Map(&quot;0&quot; -&gt; &quot;女&quot;, &quot;1&quot; -&gt; &quot;男&quot;) val bcMap = sc.broadcast(genderMap) sc.parallelize(userList,3) .map(info=&gt;{ val prefix = info.substring(0, info.lastIndexOf(&quot;,&quot;)) val gender = info.substring(info.lastIndexOf(&quot;,&quot;) + 1) val genderMapValue = bcMap.value val newGender = genderMapValue.getOrElse(gender, &quot;未知&quot;) prefix + &quot;,&quot; + newGender }).collect().foreach(println) sc.stop() 累加器 Spark提供的Accumulator，主要用于多个节点对一个变量进行共享性的操作。Accumulator只提供了累加的功能。但是确给我们提供了多个task对一个变量并行操作的功能。但是task只能对Accumulator进行累加操作，不能读取它的值。只有Driver程序可以读取Accumulator的值。 scala&gt; var count=sc.longAccumulator(&quot;count&quot;) scala&gt; sc.parallelize(List(1,2,3,4,5,6),3).foreach(item=&gt; count.add(item)) scala&gt; count.value res1: Long = 21 ","link":"https://mask0407.github.io/spark03/"},{"title":"Spark入门(三)——SparkRDD剖析(面试点)","content":" RDD简介 如下案例： RDD容错 RDD 宽窄依赖 Sage划分(重点) 小结 RDD缓存机制 Check Point 机制 (Spark RDD剖析) RDD简介 Spark计算中一个重要的概念就是可以跨越多个节点的可伸缩分布式数据集 RDD（resilient distributed dataset） Spark的内存计算的核心就是RDD的并行计算。RDD可以理解是一个弹性的，分布式、不可变的、带有分区的数据集合，所谓的Spark的批处理，实际上就是正对RDD的集合操作，RDD有以下特点： 任意一个RDD都包含分区数（决定程序某个阶段计算并行度） RDD所谓的分布式计算是在分区内部计算的 因为RDD是只读的，RDD之间的变换存着依赖关系（宽依赖、窄依赖） 针对于k-v类型的RDD，一般可以指定分区策略（一般系统提供） 针对于存储在HDFS上的文件，系统可以计算最优位置，计算每个切片。（了解） 如下案例： 通过上述的代码中不难发现，Spark的整个任务的计算无外乎围绕RDD的三种类型操作RDD创建、RDD转换、RDD Action.通常习惯性的将flatMap/map/reduceByKey称为RDD的转换算子，collect触发任务执行，因此被人们称为动作算子。在Spark中所有的Transform算子都是lazy执行的，只有在Action算子的时候，Spark才会真正的运行任务，也就是说只有遇到Action算子的时候，SparkContext才会对任务做DAG状态拆分，系统才会计算每个状态下任务的TaskSet，继而TaskSchedule才会将任务提交给Executors执行。现将以上字符统计计算流程描述如下： textFile(&quot;路径&quot;，分区数)-&gt; flatMap -&gt; map -&gt; reduceByKey -&gt; sortBy在这些转换中其中flatMap/map、reduceByKey、sotBy都是转换算子，所有的转换算子都是Lazy执行的。程序在遇到collect（Action 算子）系统会触发job执行。Spark底层会按照RDD的依赖关系将整个计算拆分成若干个阶段，我们通常将RDD的依赖关系称为RDD的血统-lineage。血统的依赖通常包含：宽依赖、窄依赖。 RDD容错 在理解DAGSchedule如何做状态划分的前提是需要大家了解一个专业术语lineage通常被人们称为RDD的血统。在了解什么是RDD的血统之前，先来看看程序猿进化过程。 上图中描述了一个程序猿起源变化的过程，我们可以近似的理解类似于RDD的转换也是一样的，Spark的计算本质就是对RDD做各种转换，因为RDD是一个不可变只读的集合，因此每次的转换都需要上一次的RDD作为本次转换的输入，因此RDD的lineage描述的是RDD间的相互依赖关系。为了保证RDD中数据的健壮性，RDD数据集通过所谓的血统关系(Lineage)记住了它是如何从其它RDD中演变过来的。Spark将RDD之间的关系归类为宽依赖和窄依赖。Spark会根据Lineage存储的RDD的依赖关系对RDD计算做故障容错，目前Saprk的容错策略更具RDD依赖关系重新计算、对RDD做Cache、对RDD做Checkpoint手段完成RDD计算的故障容错。 RDD 宽窄依赖 RDD在Lineage依赖方面分为两种Narrow Dependencies与Wide Dependencies用来解决数据容错的高效性。Narrow Dependencies是指父RDD的每一个分区最多被一个子RDD的分区所用，表现为一个父RDD的分区对应于一个子RDD的分区或多个父RDD的分区对应于子RDD的一个分区，也就是说一个父RDD的一个分区不可能对应一个子RDD的多个分区。Wide Dependencies父RDD的一个分区对应一个子RDD的多个分区。 对于Wide Dependencies这种计算的输入和输出在不同的节点上，一般需要夸节点做Shuffle，因此如果是RDD在做宽依赖恢复的时候需要多个节点重新计算成本较高。相对于Narrow Dependencies RDD间的计算是在同一个Task当中实现的是线程内部的的计算，因此在RDD分区数据丢失的的时候，也非常容易恢复。 Sage划分(重点) Spark任务阶段的划分是按照RDD的lineage关系逆向生成的这么一个过程，Spark任务提交的流程大致如下图所示： 这里可以分析一下DAGScheduel中对State拆分的逻辑代码片段如下所示： DAGScheduler.scala 第719行 def runJob[T, U]( rdd: RDD[T], func: (TaskContext, Iterator[T]) =&gt; U, partitions: Seq[Int], callSite: CallSite, resultHandler: (Int, U) =&gt; Unit, properties: Properties): Unit = { val start = System.nanoTime val waiter = submitJob(rdd, func, partitions, callSite, resultHandler, properties) //... } DAGScheduler - 675行 def submitJob[T, U]( rdd: RDD[T], func: (TaskContext, Iterator[T]) =&gt; U, partitions: Seq[Int], callSite: CallSite, resultHandler: (Int, U) =&gt; Unit, properties: Properties): JobWaiter[U] = { //eventProcessLoop 实现的是一个队列，系统底层会调用 doOnReceive -&gt; case JobSubmitted -&gt; dagScheduler.handleJobSubmitted(951行) eventProcessLoop.post(JobSubmitted( jobId, rdd, func2, partitions.toArray, callSite, waiter, SerializationUtils.clone(properties))) waiter } DAGScheduler - 951行 private[scheduler] def handleJobSubmitted(jobId: Int, finalRDD: RDD[_], func: (TaskContext, Iterator[_]) =&gt; _, partitions: Array[Int], callSite: CallSite, listener: JobListener, properties: Properties) { var finalStage: ResultStage = null try { //... finalStage = createResultStage(finalRDD, func, partitions, jobId, callSite) } catch { //... } submitStage(finalStage) } DAGScheduler - 1060行 private def submitStage(stage: Stage) { val jobId = activeJobForStage(stage) if (jobId.isDefined) { logDebug(&quot;submitStage(&quot; + stage + &quot;)&quot;) if (!waitingStages(stage) &amp;&amp; !runningStages(stage) &amp;&amp; !failedStages(stage)) { //计算当前State的父Stage val missing = getMissingParentStages(stage).sortBy(_.id) logDebug(&quot;missing: &quot; + missing) if (missing.isEmpty) { logInfo(&quot;Submitting &quot; + stage + &quot; (&quot; + stage.rdd + &quot;), which has no missing parents&quot;) //如果当前的State没有父Stage，就提交当前Stage中的Task submitMissingTasks(stage, jobId.get) } else { for (parent &lt;- missing) { //递归查找当前父Stage的父Stage submitStage(parent) } waitingStages += stage } } } else { abortStage(stage, &quot;No active job for stage &quot; + stage.id, None) } } DAGScheduler - 549行 (获取当前State的父State) private def getMissingParentStages(stage: Stage): List[Stage] = { val missing = new HashSet[Stage] val visited = new HashSet[RDD[_]] // We are manually maintaining a stack here to prevent StackOverflowError // caused by recursively visiting val waitingForVisit = new ArrayStack[RDD[_]]//栈 def visit(rdd: RDD[_]) { if (!visited(rdd)) { visited += rdd val rddHasUncachedPartitions = getCacheLocs(rdd).contains(Nil) if (rddHasUncachedPartitions) { for (dep &lt;- rdd.dependencies) { dep match { //如果是宽依赖ShuffleDependency，就添加一个Stage case shufDep: ShuffleDependency，[_, _, _] =&gt; val mapStage = getOrCreateShuffleMapStage(shufDep, stage.firstJobId) if (!mapStage.isAvailable) { missing += mapStage } //如果是窄依赖NarrowDependency，将当前的父RDD添加到栈中 case narrowDep: NarrowDependency[_] =&gt; waitingForVisit.push(narrowDep.rdd) } } } } } waitingForVisit.push(stage.rdd) while (waitingForVisit.nonEmpty) {//循环遍历栈，计算 stage visit(waitingForVisit.pop()) } missing.toList } DAGScheduler - 1083行 (提交当前Stage的TaskSet) private def submitMissingTasks(stage: Stage, jobId: Int) { logDebug(&quot;submitMissingTasks(&quot; + stage + &quot;)&quot;) // First figure out the indexes of partition ids to compute. val partitionsToCompute: Seq[Int] = stage.findMissingPartitions() // Use the scheduling pool, job group, description, etc. from an ActiveJob associated // with this Stage val properties = jobIdToActiveJob(jobId).properties runningStages += stage // SparkListenerStageSubmitted should be posted before testing whether tasks are // serializable. If tasks are not serializable, a SparkListenerStageCompleted event // will be posted, which should always come after a corresponding SparkListenerStageSubmitted // event. stage match { case s: ShuffleMapStage =&gt; outputCommitCoordinator.stageStart(stage = s.id, maxPartitionId = s.numPartitions - 1) case s: ResultStage =&gt; outputCommitCoordinator.stageStart( stage = s.id, maxPartitionId = s.rdd.partitions.length - 1) } val taskIdToLocations: Map[Int, Seq[TaskLocation]] = try { stage match { case s: ShuffleMapStage =&gt; partitionsToCompute.map { id =&gt; (id, getPreferredLocs(stage.rdd, id))}.toMap case s: ResultStage =&gt; partitionsToCompute.map { id =&gt; val p = s.partitions(id) (id, getPreferredLocs(stage.rdd, p)) }.toMap } } catch { case NonFatal(e) =&gt; stage.makeNewStageAttempt(partitionsToCompute.size) listenerBus.post(SparkListenerStageSubmitted(stage.latestInfo, properties)) abortStage(stage, s&quot;Task creation failed: $e\\n${Utils.exceptionString(e)}&quot;, Some(e)) runningStages -= stage return } stage.makeNewStageAttempt(partitionsToCompute.size, taskIdToLocations.values.toSeq) // If there are tasks to execute, record the submission time of the stage. Otherwise, // post the even without the submission time, which indicates that this stage was // skipped. if (partitionsToCompute.nonEmpty) { stage.latestInfo.submissionTime = Some(clock.getTimeMillis()) } listenerBus.post(SparkListenerStageSubmitted(stage.latestInfo, properties)) // TODO: Maybe we can keep the taskBinary in Stage to avoid serializing it multiple times. // Broadcasted binary for the task, used to dispatch tasks to executors. Note that we broadcast // the serialized copy of the RDD and for each task we will deserialize it, which means each // task gets a different copy of the RDD. This provides stronger isolation between tasks that // might modify state of objects referenced in their closures. This is necessary in Hadoop // where the JobConf/Configuration object is not thread-safe. var taskBinary: Broadcast[Array[Byte]] = null var partitions: Array[Partition] = null try { // For ShuffleMapTask, serialize and broadcast (rdd, shuffleDep). // For ResultTask, serialize and broadcast (rdd, func). var taskBinaryBytes: Array[Byte] = null // taskBinaryBytes and partitions are both effected by the checkpoint status. We need // this synchronization in case another concurrent job is checkpointing this RDD, so we get a // consistent view of both variables. RDDCheckpointData.synchronized { taskBinaryBytes = stage match { case stage: ShuffleMapStage =&gt; JavaUtils.bufferToArray( closureSerializer.serialize((stage.rdd, stage.shuffleDep): AnyRef)) case stage: ResultStage =&gt; JavaUtils.bufferToArray(closureSerializer.serialize((stage.rdd, stage.func): AnyRef)) } partitions = stage.rdd.partitions } taskBinary = sc.broadcast(taskBinaryBytes) } catch { // In the case of a failure during serialization, abort the stage. case e: NotSerializableException =&gt; abortStage(stage, &quot;Task not serializable: &quot; + e.toString, Some(e)) runningStages -= stage // Abort execution return case e: Throwable =&gt; abortStage(stage, s&quot;Task serialization failed: $e\\n${Utils.exceptionString(e)}&quot;, Some(e)) runningStages -= stage // Abort execution return } val tasks: Seq[Task[_]] = try { val serializedTaskMetrics = closureSerializer.serialize(stage.latestInfo.taskMetrics).array() stage match { case stage: ShuffleMapStage =&gt; stage.pendingPartitions.clear() partitionsToCompute.map { id =&gt; val locs = taskIdToLocations(id) val part = partitions(id) stage.pendingPartitions += id new ShuffleMapTask(stage.id, stage.latestInfo.attemptNumber, taskBinary, part, locs, properties, serializedTaskMetrics, Option(jobId), Option(sc.applicationId), sc.applicationAttemptId, stage.rdd.isBarrier()) } case stage: ResultStage =&gt; partitionsToCompute.map { id =&gt; val p: Int = stage.partitions(id) val part = partitions(p) val locs = taskIdToLocations(id) new ResultTask(stage.id, stage.latestInfo.attemptNumber, taskBinary, part, locs, id, properties, serializedTaskMetrics, Option(jobId), Option(sc.applicationId), sc.applicationAttemptId, stage.rdd.isBarrier()) } } } catch { case NonFatal(e) =&gt; abortStage(stage, s&quot;Task creation failed: $e\\n${Utils.exceptionString(e)}&quot;, Some(e)) runningStages -= stage return } if (tasks.size &gt; 0) { logInfo(s&quot;Submitting ${tasks.size} missing tasks from $stage (${stage.rdd}) (first 15 &quot; + s&quot;tasks are for partitions ${tasks.take(15).map(_.partitionId)})&quot;) taskScheduler.submitTasks(new TaskSet( tasks.toArray, stage.id, stage.latestInfo.attemptNumber, jobId, properties)) } else { // Because we posted SparkListenerStageSubmitted earlier, we should mark // the stage as completed here in case there are no tasks to run markStageAsFinished(stage, None) stage match { case stage: ShuffleMapStage =&gt; logDebug(s&quot;Stage ${stage} is actually done; &quot; + s&quot;(available: ${stage.isAvailable},&quot; + s&quot;available outputs: ${stage.numAvailableOutputs},&quot; + s&quot;partitions: ${stage.numPartitions})&quot;) markMapStageJobsAsFinished(stage) case stage : ResultStage =&gt; logDebug(s&quot;Stage ${stage} is actually done; (partitions: ${stage.numPartitions})&quot;) } submitWaitingChildStages(stage) } } 小结 通过以上源码分析，可以得出Spark所谓宽窄依赖事实上指的是ShuffleDependency或者是NarrowDependency如果是ShuffleDependency系统会生成一个ShuffeMapStage,如果是NarrowDependency则忽略，归为当前Stage。当系统回推到起始RDD的时候因为发现当前RDD或者ShuffleMapStage没有父Stage的时候，当前系统会将当前State下的Task封装成ShuffleMapTask(如果是ResultStage就是ResultTask),当前Task的数目等于当前state分区的分区数。然后将Task封装成TaskSet通过调用taskScheduler.submitTasks将任务提交给集群。 RDD缓存机制 缓存是一种RDD计算容错的一种手段，程序在RDD数据丢失的时候，可以通过缓存快速计算当前RDD的值，而不需要反推出所有的RDD重新计算，因此Spark在需要对某个RDD多次使用的时候，为了提高程序的执行效率用户可以考虑使用RDD的cache。如下测试： val conf = new SparkConf() .setAppName(&quot;word-count&quot;) .setMaster(&quot;local[2]&quot;) val sc = new SparkContext(conf) val value: RDD[String] = sc.textFile(&quot;file:///D:/demo/words/&quot;) .cache() value.count() var begin=System.currentTimeMillis() value.count() var end=System.currentTimeMillis() println(&quot;耗时：&quot;+ (end-begin))//耗时：253 //失效缓存 value.unpersist() begin=System.currentTimeMillis() value.count() end=System.currentTimeMillis() println(&quot;不使用缓存耗时：&quot;+ (end-begin))//2029 sc.stop() 除了调用cache之外，Spark提供了更细粒度的RDD缓存方案，用户可以根据集群的内存状态选择合适的缓存策略。用户可以使用persist方法指定缓存级别。缓存级别有如下可选项： val NONE = new StorageLevel(false, false, false, false) val DISK_ONLY = new StorageLevel(true, false, false, false) val DISK_ONLY_2 = new StorageLevel(true, false, false, false, 2) val MEMORY_ONLY = new StorageLevel(false, true, false, true) val MEMORY_ONLY_2 = new StorageLevel(false, true, false, true, 2) val MEMORY_ONLY_SER = new StorageLevel(false, true, false, false) val MEMORY_ONLY_SER_2 = new StorageLevel(false, true, false, false, 2) val MEMORY_AND_DISK = new StorageLevel(true, true, false, true) val MEMORY_AND_DISK_2 = new StorageLevel(true, true, false, true, 2) val MEMORY_AND_DISK_SER = new StorageLevel(true, true, false, false) val MEMORY_AND_DISK_SER_2 = new StorageLevel(true, true, false, false, 2) val OFF_HEAP = new StorageLevel(true, true, true, false, 1) xxRDD.persist(StorageLevel.MEMORY_AND_DISK_SER_2) 其中： MEMORY_ONLY：表示数据完全不经过序列化存储在内存中，效率高，但是有可能导致内存溢出. MEMORY_ONLY_SER和MEMORY_ONLY一样，只不过需要对RDD的数据做序列化，牺牲CPU节省内存，同样会导致内存溢出可能。 其中_2表示缓存结果有备份，如果大家不确定该使用哪种级别，一般推荐MEMORY_AND_DISK_SER_2 Check Point 机制 除了使用缓存机制可以有效的保证RDD的故障恢复，但是如果缓存失效还是会在导致系统重新计算RDD的结果，所以对于一些RDD的lineage较长的场景，计算比较耗时，用户可以尝试使用checkpoint机制存储RDD的计算结果，该种机制和缓存最大的不同在于，使用checkpoint之后被checkpoint的RDD数据直接持久化在文件系统中，一般推荐将结果写在hdfs中，这种checpoint并不会自动清空。注意checkpoint在计算的过程中先是对RDD做mark，在任务执行结束后，再对mark的RDD实行checkpoint，也就是要重新计算被Mark之后的rdd的依赖和结果，因此为了避免Mark RDD重复计算，推荐使用策略 val conf = new SparkConf().setMaster(&quot;yarn&quot;).setAppName(&quot;wordcount&quot;) val sc = new SparkContext(conf) sc.setCheckpointDir(&quot;hdfs:///checkpoints&quot;) val lineRDD: RDD[String] = sc.textFile(&quot;hdfs:///words/t_word.txt&quot;) val cacheRdd = lineRDD.flatMap(line =&gt; line.split(&quot; &quot;)) .map(word =&gt; (word, 1)) .groupByKey() .map(tuple =&gt; (tuple._1, tuple._2.sum)) .sortBy(tuple =&gt; tuple._2, false, 1) .cache() cacheRdd.checkpoint() cacheRdd.collect().foreach(tuple=&gt;println(tuple._1+&quot;-&gt;&quot;+tuple._2)) cacheRdd.unpersist() //3.关闭sc sc.stop() ","link":"https://mask0407.github.io/spark02/"},{"title":"Spark入门(二)——Spark环境搭建与开发环境","content":" Standalone单节点模式 Spark On Yarn Spark 开发环境构建 SparkRDDWordCount(本地) 集群(yarn) 集群(standalone) (Spark Standalone与Spark On Yarn环境搭建) Standalone单节点模式 Hadoop环境(有Hadoop环境的可以直接进入下面Spark环境安装) 设置CentOS进程数和文件数(重启) [root@CentOS ~]# vi /etc/security/limits.conf * soft nofile 204800 * hard nofile 204800 * soft nproc 204800 * hard nproc 204800 [root@CentOS ~]# reboot 配置主机名(重启) [root@CentOS ~]# vi /etc/sysconfig/network NETWORKING=yes HOSTNAME=CentOS [root@CentOS ~]# reboot 设置IP映射 [root@CentOS ~]# vi /etc/hosts 127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4 ::1 localhost localhost.localdomain localhost6 localhost6.localdomain6 192.168.111.132 CentOS 防火墙服务 [root@CentOS ~]# service iptables stop iptables: Setting chains to policy ACCEPT: filter [ OK ] iptables: Flushing firewall rules: [ OK ] iptables: Unloading modules: [ OK ] [root@CentOS ~]# chkconfig iptables off 安装JDK1.8+ [root@CentOS ~]# rpm -ivh jdk-8u191-linux-x64.rpm warning: jdk-8u191-linux-x64.rpm: Header V3 RSA/SHA256 Signature, key ID ec551f03: NOKEY Preparing... ########################################### [100%] 1:jdk1.8 ########################################### [100%] Unpacking JAR files... tools.jar... plugin.jar... javaws.jar... deploy.jar... rt.jar... jsse.jar... charsets.jar... localedata.jar... [root@CentOS ~]# vi .bashrc JAVA_HOME=/usr/java/latest PATH=$PATH:$JAVA_HOME/bin CLASSPATH=. export JAVA_HOME export PATH export CLASSPATH [root@CentOS ~]# source ~/.bashrc SSH配置免密 [root@CentOS ~]# ssh-keygen -t rsa Generating public/private rsa key pair. Enter file in which to save the key (/root/.ssh/id_rsa): Created directory '/root/.ssh'. Enter passphrase (empty for no passphrase): Enter same passphrase again: Your identification has been saved in /root/.ssh/id_rsa. Your public key has been saved in /root/.ssh/id_rsa.pub. The key fingerprint is: a5:2d:f5:c3:22:83:cf:13:25:59:fb:c1:f4:63:06:d4 root@CentOS The key's randomart image is: +--[ RSA 2048]----+ | ..+. | | o + oE | | o = o = | | . B + + . | | . S o = | | o = . . | | + | | . | | | +-----------------+ [root@CentOS ~]# ssh-copy-id CentOS The authenticity of host 'centos (192.168.111.132)' can't be established. RSA key fingerprint is fa:1b:c0:23:86:ff:08:5e:83:ba:65:4c:e6:f2:1f:3b. Are you sure you want to continue connecting (yes/no)? yes Warning: Permanently added 'centos,192.168.111.132' (RSA) to the list of known hosts. root@centos's password:`需要输入密码` Now try logging into the machine, with &quot;ssh 'CentOS'&quot;, and check in: .ssh/authorized_keys to make sure we haven't added extra keys that you weren't expecting. 配置HDFS 将hadoop-2.9.2.tar.gz解压到系统的/usr目录下然后配置[core|hdfs]-site.xml配置文件。 [root@CentOS ~]# vi /usr/hadoop-2.9.2/etc/hadoop/core-site.xml &lt;!--nn访问入口--&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://CentOS:9000&lt;/value&gt; &lt;/property&gt; &lt;!--hdfs工作基础目录--&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/usr/hadoop-2.9.2/hadoop-${user.name}&lt;/value&gt; &lt;/property&gt; [root@CentOS ~]# vi /usr/hadoop-2.9.2/etc/hadoop/hdfs-site.xml &lt;!--block副本因子--&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;1&lt;/value&gt; &lt;/property&gt; &lt;!--配置Sencondary namenode所在物理主机--&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt; &lt;value&gt;CentOS:50090&lt;/value&gt; &lt;/property&gt; &lt;!--设置datanode最大文件操作数--&gt; &lt;property&gt; &lt;name&gt;dfs.datanode.max.xcievers&lt;/name&gt; &lt;value&gt;4096&lt;/value&gt; &lt;/property&gt; &lt;!--设置datanode并行处理能力--&gt; &lt;property&gt; &lt;name&gt;dfs.datanode.handler.count&lt;/name&gt; &lt;value&gt;6&lt;/value&gt; &lt;/property&gt; [root@CentOS ~]# vi /usr/hadoop-2.9.2/etc/hadoop/slaves CentOS 配置hadoop环境变量 [root@CentOS ~]# vi .bashrc HADOOP_HOME=/usr/hadoop-2.9.2 JAVA_HOME=/usr/java/latest PATH=$PATH:$JAVA_HOME/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin CLASSPATH=. export JAVA_HOME export PATH export CLASSPATH export HADOOP_HOME [root@CentOS ~]# source .bashrc 启动Hadoop服务 [root@CentOS ~]# hdfs namenode -format # 创建初始化所需的fsimage文件 [root@CentOS ~]# start-dfs.sh Spark环境 下载spark-2.4.3-bin-without-hadoop.tgz解压到/usr目录,并且将Spark目录修改名字为spark-2.4.3然后修改spark-env.sh和spark-default.conf文件. 下载地址：http://mirrors.tuna.tsinghua.edu.cn/apache/spark/spark-2.4.3/spark-2.4.3-bin-without-hadoop.tgz 解压Spark安装包，并且修改解压文件名 [root@CentOS ~]# tar -zxf spark-2.4.3-bin-without-hadoop.tgz -C /usr/ [root@CentOS ~]# mv /usr/spark-2.4.3-bin-without-hadoop/ /usr/spark-2.4.3 [root@CentOS ~]# vi .bashrc SPARK_HOME=/usr/spark-2.4.3 HADOOP_HOME=/usr/hadoop-2.9.2 JAVA_HOME=/usr/java/latest PATH=$PATH:$JAVA_HOME/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$SPARK_HOME/bin:$SPARK_HOME/sbin CLASSPATH=. export JAVA_HOME export PATH export CLASSPATH export HADOOP_HOME export SPARK_HOME [root@CentOS ~]# source .bashrc 配置Spark服务 [root@CentOS spark-2.4.3]# cd /usr/spark-2.4.3/conf/ [root@CentOS conf]# mv spark-env.sh.template spark-env.sh [root@CentOS conf]# mv slaves.template slaves [root@CentOS conf]# vi slaves CentOS [root@CentOS conf]# vi spark-env.sh SPARK_MASTER_HOST=CentOS SPARK_MASTER_PORT=7077 SPARK_WORKER_CORES=4 SPARK_WORKER_MEMORY=2g LD_LIBRARY_PATH=/usr/hadoop-2.9.2/lib/native SPARK_DIST_CLASSPATH=$(hadoop classpath) export SPARK_MASTER_HOST export SPARK_MASTER_PORT export SPARK_WORKER_CORES export SPARK_WORKER_MEMORY export LD_LIBRARY_PATH export SPARK_DIST_CLASSPATH 启动Spark进程 [root@CentOS ~]# cd /usr/spark-2.4.3/ [root@CentOS spark-2.4.3]# ./sbin/start-all.sh starting org.apache.spark.deploy.master.Master, logging to /usr/spark-2.4.3/logs/spark-root-org.apache.spark.deploy.master.Master-1-CentOS.out CentOS: starting org.apache.spark.deploy.worker.Worker, logging to /usr/spark-2.4.3/logs/spark-root-org.apache.spark.deploy.worker.Worker-1-CentOS.out 测试Spark [root@CentOS spark-2.4.3]# ./bin/spark-shell --master spark://CentOS:7077 --deploy-mode client --executor-cores 2 executor-cores：在standalone模式表示程序每个Worker节点分配资源数。不能超过单台自大core个数，如果不清每台能够分配的最大core的个数，可以使用--total-executor-cores,该种分配会尽最大可能分配。 scala&gt; sc.textFile(&quot;hdfs:///words/t_words&quot;,5) .flatMap(_.split(&quot; &quot;)) .map((_,1)) .reduceByKey(_+_) .sortBy(_._1,true,3) .saveAsTextFile(&quot;hdfs:///results&quot;) 上述任务的任务拆分图： Spark On Yarn Hadoop环境(配置过Yarn的可以直接跳过Hadoop安装) 设置CentOS进程数和文件数(重启) [root@CentOS ~]# vi /etc/security/limits.conf * soft nofile 204800 * hard nofile 204800 * soft nproc 204800 * hard nproc 204800 [root@CentOS ~]# reboot 配置主机名(重启) [root@CentOS ~]# vi /etc/sysconfig/network NETWORKING=yes HOSTNAME=CentOS [root@CentOS ~]# reboot 设置IP映射 [root@CentOS ~]# vi /etc/hosts 127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4 ::1 localhost localhost.localdomain localhost6 localhost6.localdomain6 192.168.111.132 CentOS 防火墙服务 [root@CentOS ~]# service iptables stop iptables: Setting chains to policy ACCEPT: filter [ OK ] iptables: Flushing firewall rules: [ OK ] iptables: Unloading modules: [ OK ] [root@CentOS ~]# chkconfig iptables off 安装JDK1.8+ [root@CentOS ~]# rpm -ivh jdk-8u191-linux-x64.rpm warning: jdk-8u191-linux-x64.rpm: Header V3 RSA/SHA256 Signature, key ID ec551f03: NOKEY Preparing... ########################################### [100%] 1:jdk1.8 ########################################### [100%] Unpacking JAR files... tools.jar... plugin.jar... javaws.jar... deploy.jar... rt.jar... jsse.jar... charsets.jar... localedata.jar... [root@CentOS ~]# vi .bashrc JAVA_HOME=/usr/java/latest PATH=$PATH:$JAVA_HOME/bin CLASSPATH=. export JAVA_HOME export PATH export CLASSPATH [root@CentOS ~]# source ~/.bashrc SSH配置免密 [root@CentOS ~]# ssh-keygen -t rsa Generating public/private rsa key pair. Enter file in which to save the key (/root/.ssh/id_rsa): Created directory '/root/.ssh'. Enter passphrase (empty for no passphrase): Enter same passphrase again: Your identification has been saved in /root/.ssh/id_rsa. Your public key has been saved in /root/.ssh/id_rsa.pub. The key fingerprint is: a5:2d:f5:c3:22:83:cf:13:25:59:fb:c1:f4:63:06:d4 root@CentOS The key's randomart image is: +--[ RSA 2048]----+ | ..+. | | o + oE | | o = o = | | . B + + . | | . S o = | | o = . . | | + | | . | | | +-----------------+ [root@CentOS ~]# ssh-copy-id CentOS The authenticity of host 'centos (192.168.111.132)' can't be established. RSA key fingerprint is fa:1b:c0:23:86:ff:08:5e:83:ba:65:4c:e6:f2:1f:3b. Are you sure you want to continue connecting (yes/no)? yes Warning: Permanently added 'centos,192.168.111.132' (RSA) to the list of known hosts. root@centos's password:`需要输入密码` Now try logging into the machine, with &quot;ssh 'CentOS'&quot;, and check in: .ssh/authorized_keys to make sure we haven't added extra keys that you weren't expecting. 配置HDFS 将hadoop-2.9.2.tar.gz解压到系统的/usr目录下然后配置[core|hdfs]-site.xml配置文件。 [root@CentOS ~]# vi /usr/hadoop-2.9.2/etc/hadoop/core-site.xml &lt;!--nn访问入口--&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://CentOS:9000&lt;/value&gt; &lt;/property&gt; &lt;!--hdfs工作基础目录--&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/usr/hadoop-2.9.2/hadoop-${user.name}&lt;/value&gt; &lt;/property&gt; [root@CentOS ~]# vi /usr/hadoop-2.9.2/etc/hadoop/hdfs-site.xml &lt;!--block副本因子--&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;1&lt;/value&gt; &lt;/property&gt; &lt;!--配置Sencondary namenode所在物理主机--&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt; &lt;value&gt;CentOS:50090&lt;/value&gt; &lt;/property&gt; &lt;!--设置datanode最大文件操作数--&gt; &lt;property&gt; &lt;name&gt;dfs.datanode.max.xcievers&lt;/name&gt; &lt;value&gt;4096&lt;/value&gt; &lt;/property&gt; &lt;!--设置datanode并行处理能力--&gt; &lt;property&gt; &lt;name&gt;dfs.datanode.handler.count&lt;/name&gt; &lt;value&gt;6&lt;/value&gt; &lt;/property&gt; [root@CentOS ~]# vi /usr/hadoop-2.9.2/etc/hadoop/slaves CentOS [root@CentOS ~]# vi /usr/hadoop-2.9.2/etc/hadoop/yarn-site.xml &lt;!--配置MapReduce计算框架的核心实现Shuffle-洗牌--&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt; &lt;!--配置资源管理器所在的目标主机--&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt; &lt;value&gt;CentOS&lt;/value&gt; &lt;/property&gt; &lt;!--关闭物理内存检查--&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.pmem-check-enabled&lt;/name&gt; &lt;value&gt;false&lt;/value&gt; &lt;/property&gt; &lt;!--关闭虚拟内存检查--&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.vmem-check-enabled&lt;/name&gt; &lt;value&gt;false&lt;/value&gt; &lt;/property&gt; [root@CentOS ~]# vi /usr/hadoop-2.9.2/etc/hadoop/mapred-site.xml &lt;!--MapRedcue框架资源管理器的实现--&gt; &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt; 配置hadoop环境变量 [root@CentOS ~]# vi .bashrc HADOOP_HOME=/usr/hadoop-2.9.2 JAVA_HOME=/usr/java/latest PATH=$PATH:$JAVA_HOME/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin CLASSPATH=. export JAVA_HOME export PATH export CLASSPATH export HADOOP_HOME [root@CentOS ~]# source .bashrc 启动Hadoop服务 [root@CentOS ~]# hdfs namenode -format # 创建初始化所需的fsimage文件 [root@CentOS ~]# start-all.sh Spark环境 下载spark-2.4.3-bin-without-hadoop.tgz解压到/usr目录,并且将Spark目录修改名字为spark-2.4.3然后修改spark-env.sh和spark-default.conf文件. 下载地址：http://mirrors.tuna.tsinghua.edu.cn/apache/spark/spark-2.4.3/spark-2.4.3-bin-without-hadoop.tgz 解压Spark安装包，并且修改解压文件名 [root@CentOS ~]# tar -zxf spark-2.4.3-bin-without-hadoop.tgz -C /usr/ [root@CentOS ~]# mv /usr/spark-2.4.3-bin-without-hadoop/ /usr/spark-2.4.3 [root@CentOS ~]# vi .bashrc SPARK_HOME=/usr/spark-2.4.3 HADOOP_HOME=/usr/hadoop-2.9.2 JAVA_HOME=/usr/java/latest PATH=$PATH:$JAVA_HOME/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$SPARK_HOME/bin:$SPARK_HOME/sbin CLASSPATH=. export JAVA_HOME export PATH export CLASSPATH export HADOOP_HOME export SPARK_HOME [root@CentOS ~]# source .bashrc 配置Spark服务 [root@CentOS spark-2.4.3]# cd /usr/spark-2.4.3/conf/ [root@CentOS conf]# mv spark-env.sh.template spark-env.sh [root@CentOS conf]# vi spark-env.sh HADOOP_CONF_DIR=/usr/hadoop-2.9.2/etc/hadoop YARN_CONF_DIR=/usr/hadoop-2.9.2/etc/hadoop SPARK_EXECUTOR_CORES=4 SPARK_EXECUTOR_MEMORY=2G SPARK_DRIVER_MEMORY=1G LD_LIBRARY_PATH=/usr/hadoop-2.9.2/lib/native SPARK_DIST_CLASSPATH=$(hadoop classpath):$SPARK_DIST_CLASSPATH SPARK_HISTORY_OPTS=&quot;-Dspark.history.fs.logDirectory=hdfs:///spark-logs&quot; export HADOOP_CONF_DIR export YARN_CONF_DIR export SPARK_EXECUTOR_CORES export SPARK_DRIVER_MEMORY export SPARK_EXECUTOR_MEMORY export LD_LIBRARY_PATH export SPARK_DIST_CLASSPATH export SPARK_HISTORY_OPTS [root@CentOS conf]# mv spark-defaults.conf.template spark-defaults.conf [root@CentOS conf]# vi spark-defaults.conf spark.eventLog.enabled=true spark.eventLog.dir=hdfs:///spark-logs 在HDFS上创建spark-logs目录，用于作为Sparkhistory服务器存储数据的地方。 [root@CentOS ~]# hdfs dfs -mkdir /spark-logs 启动Spark历史服务器(可选) [root@CentOS spark-2.4.3]# ./sbin/start-history-server.sh starting org.apache.spark.deploy.history.HistoryServer, logging to /usr/spark-2.4.3/logs/spark-root-org.apache.spark.deploy.history.HistoryServer-1-CentOS.out [root@CentOS spark-2.4.3]# jps 5728 NodeManager 5090 NameNode 5235 DataNode 10531 Jps 5623 ResourceManager 5416 SecondaryNameNode 10459 HistoryServer 该进程启动一个内嵌的web ui端口是18080，用户可以访问改页面查看任务执行计划、历史。 测试Spark ./bin/spark-shell --master yarn --deploy-mode client --num-executors 2 --executor-cores 3 --num-executors：在Yarn模式下，表示向NodeManager申请的资源数进程， --executor-cores表示每个进程所能运行线程数。 真个任务计算资源= num-executors * executor-core scala&gt; sc.textFile(&quot;hdfs:///words/t_words&quot;,5) .flatMap(_.split(&quot; &quot;)) .map((_,1)) .reduceByKey(_+_) .sortBy(_._1,true,3) .saveAsTextFile(&quot;hdfs:///results&quot;) 上述任务的任务划分图 本地仿真 在该种模式下，无需安装yarn、无需启动Stanalone，一切都是模拟。 [root@CentOS spark-2.4.3]# ./bin/spark-shell --master local[5] Setting default log level to &quot;WARN&quot;. To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel). Spark context Web UI available at http://CentOS:4040 Spark context available as 'sc' (master = local[5], app id = local-1561742649329). Spark session available as 'spark'. Welcome to ____ __ / __/__ ___ _____/ /__ _\\ \\/ _ \\/ _ `/ __/ '_/ /___/ .__/\\_,_/_/ /_/\\_\\ version 2.4.3 /_/ Using Scala version 2.11.12 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_191) Type in expressions to have them evaluated. Type :help for more information. scala&gt; sc.textFile(&quot;hdfs:///words/t_words&quot;).flatMap(_.split(&quot; &quot;)).map((_,1)).reduceByKey(_+_).sortBy(_._1,false,3).saveAsTextFile(&quot;hdfs:///results1/&quot;) scala&gt; Spark 开发环境构建 引入开发所需依赖 &lt;dependencies&gt; &lt;!-- https://mvnrepository.com/artifact/org.apache.spark/spark-core --&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-core_2.11&lt;/artifactId&gt; &lt;version&gt;2.4.3&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;plugins&gt; &lt;!--scala编译插件--&gt; &lt;plugin&gt; &lt;groupId&gt;net.alchim31.maven&lt;/groupId&gt; &lt;artifactId&gt;scala-maven-plugin&lt;/artifactId&gt; &lt;version&gt;4.0.1&lt;/version&gt; &lt;executions&gt; &lt;execution&gt; &lt;id&gt;scala-compile-first&lt;/id&gt; &lt;phase&gt;process-resources&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;add-source&lt;/goal&gt; &lt;goal&gt;compile&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; SparkRDDWordCount(本地) //1.创建SparkContext val conf = new SparkConf().setMaster(&quot;local[10]&quot;).setAppName(&quot;wordcount&quot;) val sc = new SparkContext(conf) val lineRDD: RDD[String] = sc.textFile(&quot;file:///Users/mashikang/demo/words/t_word.txt&quot;) lineRDD.flatMap(line=&gt;line.split(&quot; &quot;)) .map(word=&gt;(word,1)) .groupByKey() .map(tuple=&gt;(tuple._1,tuple._2.sum)) .sortBy(tuple=&gt;tuple._2,false,1) .collect() .foreach(tuple=&gt;println(tuple._1+&quot;-&gt;&quot;+tuple._2)) //3.关闭sc sc.stop() 集群(yarn) //1.创建SparkContext val conf = new SparkConf().setMaster(&quot;yarn&quot;).setAppName(&quot;wordcount&quot;) val sc = new SparkContext(conf) val lineRDD: RDD[String] = sc.textFile(&quot;hdfs:///words/t_words&quot;) lineRDD.flatMap(line=&gt;line.split(&quot; &quot;)) .map(word=&gt;(word,1)) .groupByKey() .map(tuple=&gt;(tuple._1,tuple._2.sum)) .sortBy(tuple=&gt;tuple._2,false,1) .collect() .foreach(tuple=&gt;println(tuple._1+&quot;-&gt;&quot;+tuple._2)) //3.关闭sc sc.stop() 发布： [root@CentOS spark-2.4.3]# ./bin/spark-submit --master yarn --deploy-mode client --class com.mask.demo02.SparkRDDWordCount --num-executors 3 --executor-cores 4 /root/sparkrdd-1.0-SNAPSHOT.jar 集群(standalone) //1.创建SparkContext val conf = new SparkConf().setMaster(&quot;spark://CentOS:7077&quot;).setAppName(&quot;wordcount&quot;) val sc = new SparkContext(conf) val lineRDD: RDD[String] = sc.textFile(&quot;hdfs:///words/t_words&quot;) lineRDD.flatMap(line=&gt;line.split(&quot; &quot;)) .map(word=&gt;(word,1)) .groupByKey() .map(tuple=&gt;(tuple._1,tuple._2.sum)) .sortBy(tuple=&gt;tuple._2,false,1) .collect() .foreach(tuple=&gt;println(tuple._1+&quot;-&gt;&quot;+tuple._2)) //3.关闭sc sc.stop() 发布： [root@CentOS spark-2.4.3]# ./bin/spark-submit --master spark://CentOS:7077 --deploy-mode client --class com.mask.demo02.SparkRDDWordCount --num-executors 3 --total-executor-cores 4 /root/sparkrdd-1.0-SNAPSHOT.jar ","link":"https://mask0407.github.io/spark01/"},{"title":"Spark入门(一)——Spark的“前世今生”","content":" Spark简介 计算流程 (Spark的诞生) Spark简介 Spark 是一个用来实现快速而通用的集群计算的平台。 在速度方面，Spark 扩展了广泛使用的 MapReduce 计算模型，而且高效地支持更多计算模式，包括交互式查询和流处理。在处理大规模数据集时，速度是非常重要的。速度快就意味着我们可以进行交互式的数据操作，否则我们每次操作就需要等待数分钟甚至数小时。Spark 的一个主要特点就是能够在内存中进行计算，因而更快。不过即使是必须在磁盘上进行的复杂计算，Spark 依然比 MapReduce 更加高效。 总的来说，Spark 适用于各种各样原先需要多种不同的分布式平台的场景，包括批处理、迭代算法、交互式查询、流处理。通过在一个统一的框架下支持这些不同的计算，Spark使我们可以简单而低耗地把各种处理流程整合在一起。而这样的组合，在实际的数据分析过程中是很有意义的。不仅如此，Spark 的这种特性还大大减轻了原先需要对各种平台别管理的负担。Spark 所提供的接口非常丰富。除了提供基于 Python、Java、Scala 和 SQL 的简单易用的API 以及内建的丰富的程序库以外，Spark 还能和其他大数据工具密切配合使用。例如，Spark 可以运行在 Hadoop 集群上，访问包括 Cassandra 在内的任意 Hadoop 数据源。 总结 Spark是一个快如闪电的统一分析引擎（计算框架）用于大规模数据集的处理。Spark在做数据的批处理计算，计算性能大约是Hadoop MapReduce的10~100倍，因为Spark使用比较先进的基于DAG 任务调度，可以将一个任务拆分成若干个阶段，然后将这些阶段分批次交给集群计算节点处理。 MapReduce VS Spark MapReduce作为第一代大数据处理框架，在设计初期只是为了满足基于海量数据级的海量数据计算的迫切需求。自2006年剥离自Nutch（Java搜索引擎）工程，主要解决的是早期人们对大数据的初级认知所面临的问题。 整个MapReduce的计算实现的是基于磁盘的IO计算，随着大数据技术的不断普及，人们开始重新定义大数据的处理方式，不仅仅满足于能在合理的时间范围内完成对大数据的计算，还对计算的实效性提出了更苛刻的要求，因为人们开始探索使用Map Reduce计算框架完成一些复杂的高阶算法，往往这些算法通常不能通过1次性的Map Reduce迭代计算完成。由于Map Reduce计算模型总是把结果存储到磁盘中，每次迭代都需要将数据磁盘加载到内存，这就为后续的迭代带来了更多延长。 2009年Spark在加州伯克利AMP实验室诞生，2010首次开源后该项目就受到很多开发人员的喜爱，2013年6月份开始在Apache孵化，2014年2月份正式成为Apache的顶级项目。Spark发展如此之快是因为Spark在计算层方面明显优于Hadoop的Map Reduce这磁盘迭代计算，因为Spark可以使用内存对数据做计算，而且计算的中间结果也可以缓存在内存中，这就为后续的迭代计算节省了时间，大幅度的提升了针对于海量数据的计算效率。 Spark也给出了在使用MapReduce和Spark做线性回归计算（算法实现需要n次迭代）上，Spark的速率几乎是MapReduce计算10~100倍这种计算速度。 不仅如此Spark在设计理念中也提出了One stack ruled them all战略，并且提供了基于Spark批处理至上的计算服务分支例如:实现基于Spark的交互查询、近实时流处理、机器学习、Grahx 图形关系存储等。 从图中不难看出Apache Spark处于计算层，Spark项目在战略上启到了承上启下的作用，并没有废弃原有以hadoop为主体的大数据解决方案。因为Spark向下可以计算来自于HDFS、HBase、Cassandra和亚马逊S3文件服务器的数据，也就意味着使用Spark作为计算层，用户原有的存储层架构无需改动。 计算流程 因为Spark计算是在MapReduce计算之后诞生，吸取了MapReduce设计经验，极大地规避了MapReduce计算过程中的诟病，先来回顾一下MapReduce计算的流程。 总结一下几点缺点： 1）MapReduce虽然基于矢量编程思想，但是计算状态过于简单，只是简单的将任务分为Map state和Reduce State，没有考虑到迭代计算场景。 2）在Map任务计算的中间结果存储到本地磁盘，IO调用过多，数据读写效率差。 3）MapReduce是先提交任务，然后在计算过程中申请资源。并且计算方式过于笨重。每个并行度都是由一个JVM进程来实现计算。 通过简单的罗列不难发现MapReduce计算的诟病和问题，因此Spark在计算层面上借鉴了MapReduce计算设计的经验，提出了DGASchedule和TaskSchedual概念，打破了在MapReduce任务中一个job只用Map State和Reduce State的两个阶段，并不适合一些迭代计算次数比较多的场景。因此Spark 提出了一个比较先进的设计理念，任务状态拆分，Spark在任务计算初期首先通过DGASchedule计算任务的State，将每个阶段的Sate封装成一个TaskSet，然后由TaskSchedual将TaskSet提交集群进行计算。可以尝试将Spark计算的流程使用一下的流程图描述如下： 相比较于MapReduce计算，Spark计算有以下优点： 1）智能DAG任务拆分，将一个复杂计算拆分成若干个State，满足迭代计算场景 2）Spark提供了计算的缓存和容错策略，将计算结果存储在内存或者磁盘，加速每个state的运行，提升运行效率 3）Spark在计算初期，就已经申请好计算资源。任务并行度是通过在Executor进程中启动线程实现，相比较于MapReduce计算更加轻快。 目前Spark提供了Cluster Manager的实现由Yarn、Standalone、Messso、kubernates等实现。其中企业常用的有Yarn和Standalone方式的管理。 Application就是你写的代码。 Driver节点上的Driver程序运行的是main函数 Worker节点就是工作节点，上面运行Executor进程。 Executor进程负责运行Task。 ","link":"https://mask0407.github.io/spark00/"},{"title":"Thread.sleep(0)到底是个什么骚操作？","content":"今天看到一段有意思的代码：Thread.sleep(0)，下面就来分析一下这串看似无用的代码到底有没有用。 Thread.sleep(0)到底是个什么骚操作，这睡0毫秒和不睡有什么区别吗？ 操作系统资源的分配策略 想要去了解多线程，就先要去了解操作系统中资源的不同分配策略 在操作系统中，CPU资源的分配策略有多种，下面就拿两种最为典型的策略举例： 时间片算法 基于时间片算法分配资源的代表就是Unix系统。在时间片算法中，操作系统会维护一个队列，将所有的进程放入队列中。然后操作系统会按照队列中的进程顺序为他们分配属于自己的运行时间，也就是说，操作系统给A进程分配n毫秒时间，那么A进程在本次排队时只可以运行n毫秒。当A进程运行完n毫秒后，操作系统不管A是否运行完毕都会把运行权限强行剥夺，交给队列里面的下一个进程去执行，然后A进程继续去队列尾部进行下一次排队。(这里，如果A在n毫秒之前提前结束或者被阻塞，操作系统会立即收回A的运行权限) 举个例子： 你去食堂买饭，这种模式就是食堂会强制你去排队，而且没人只有十分钟点餐时间，一旦过了这个时间你就要重新去排队。 抢占式 基于抢占式分配资源的代表是Winodws系统。抢占式的操作系统都是“儒家学派”的，默认每个进程都是“君子”。意思就是：一个进程一旦拿到CPU后，除非它主动放弃CPU权限，不然别的进程是拿不到CPU权限的。然后操作系统在选取执行权限的时候也不是随机选的，它会根据优先级和饥饿时间来判定谁更需要CPU权限的。每次一个进程是释放CPU后，就会进行一次优先级评定。也就是说，如果运气好，每次都会被选中。 举个例子： 你去食堂买饭，食堂阿姨看你小伙子挺帅就先给你盛饭。然后给你盛完饭再选一次，又发现你这小伙子越看越顺眼，就再给你一碗。 关于Sleep 拿抢占式来说。去食堂吃饭，阿姨看小伙子挺不错，三番五次的给你盛饭，但是你已经吃饱了，这时候你就会告诉阿姨我吃饱了，接下来半小时不要再给我盛饭了。对应到代码里就是，A最近30min不想再参与资源选举了，这时它就会sleep(30 * 60 * 1000)，然后操作系统就会进行再次选举，并且未来半小时A不参与选举。此时当A醒来之后，或许CPU执行权在另一个进程手里。 再说Thread.sleep(0) 假如A执行了一段时间后，突然想起来其他小伙伴可能也需要CPU执行权限，不能光自己一个人霸占这个CPU，但是又没办法主动申请再次选举，就只能退而求其次执行Thread.sleep(0)，在A有执行权限的情况下执行，执行后告诉操作系统，我要休息0毫秒，你接下来0毫秒内的选举就不要选我啦。但是操作系统下次再选举的时候还是会把A算进去，因为0毫秒已经过了。 总结 Thread.sleep(0)不光有用，而且有奇效，对于想做老好人的进程可以调用一次，让操作系统再次进行选举。 ","link":"https://mask0407.github.io/java00/"},{"title":"Redis新版本发布，Redis还是单线程？","content":" Redis简介 Redis单线程时代 “单线程”的Redis为什么会这么快？ Redis的瓶颈 6.0版本后的Redis线程问题 redis的多线程不是你理解的多线程 redis的多线程是默认关闭的 (Redis从单线程到多线程的转变) Redis简介 Redis 是一个开源（BSD许可）的，内存中的数据结构存储系统，它可以用作数据库、缓存和消息中间件。 它支持多种类型的数据结构，如 字符串（strings）， 散列（hashes）， 列表（lists）， 集合（sets）， 有序集合（sorted sets） 与范围查询， bitmaps， hyperloglogs 和 地理空间（geospatial） 索引半径查询。 Redis 内置了 复制（replication），LUA脚本（Lua scripting）， LRU驱动事件（LRU eviction），事务（transactions） 和不同级别的 磁盘持久化（persistence）， 并通过 Redis哨兵（Sentinel）和自动 分区（Cluster）提供高可用性（high availability）。 Redis单线程时代 Redis在起初严格意义上也不算纯粹的单线程 单线程指的是网络请求模块使用了一个线程（所以不需考虑并发安全性），即一个线程处理所有网络请求，其他模块仍用了多个线程。比如持久化时，Redis就会开启一个子线程去操作。 “单线程”的Redis为什么会这么快？ Redis被广泛应用于缓存，一切皆源于Redis基于内存计算，它的读取速度要比寻常的RMDB(关系型数据库)快上很多。 Redis每秒可执行大约110000次的设置(SET)操作，每秒大约可执行81000次的读取/获取(GET)操作。 Redis操作具有原子性，能保证并发情况下数据的安全性。 Rdis采用请求上的单线程，避免了不必要的上下文切换和线程间的资源竞争。 Redis采用了如非阻塞IO模型、IO的多路复用等多种IO模型。 Redis的瓶颈 先来看Redis的特性，Redis是基于内存进行操作的NoSql数据库。 首先CPU不会成为Redis的瓶颈，这也是为什么单线程的Redis依然恐怖如斯的写照 Redis多用于高并发下做缓存，所以Redis的瓶颈最有可能是机器内存的大小或者网络带宽 6.0版本后的Redis线程问题 Redis的作者在 2019-12-19 发布了Redis 6.0 RC1，但是即便是这种大佬也没有逃过“真香定律”，Redis居然开始走上了多线程的潮流路线。也就是说从6.0开始Redis就是多线程的了，这也就意味着以后面试又多一个坑！ 作者在自己博客中对新特性的介绍 原文地址：Redis 6.0 RC1 作者博客原文 Redis 6 被称为是 Redis 有史以来最大的一个版本，就在2020年五一期间，6.0稳定版本发布了。下面单就线程方面先了解一下新版本特性： redis的多线程不是你理解的多线程 但跟 Memcached 这种从 IO 处理到数据访问多线程的实现模式有些差异。Redis 的多线程部分只是用来处理网络数据的读写和协议解析，执行命令仍然是单线程。之所以这么设计是不想因为多线程而变得复杂，需要去控制 key、lua（一种轻量级脚本语言）、事务，LPUSH/LPOP（redis语法：将一个或多个值插入到列表头部（左边）、移出并获取列表的第一个元素(左边)） 等等的并发问题 主线程负责接收连接请求，读事件到来(收到请求)则放到一个全局等待读处理队列 主线程处理完读事件之后，通过 RR(Round Robin) 将这些连接分配给这些 IO 线程，然后主线程忙等待(spinlock 的效果)状态 IO 线程将请求数据读取并解析完成(这里只是读数据和解析并不执行) 主线程执行所有命令并清空整个请求等待读处理队列(执行部分串行) redis的多线程是默认关闭的 Redis6.0的多线程默认是禁用的，只使用主线程。如需开启需要修改redis.conf配置文件：io-threads-do-reads yes 线程数设置通过修改redis.conf配置文件：io-threads属性指定线程数量。关于线程数的设置，官方有一个建议：4核的机器建议设置为2或3个线程，8核的建议设置为6个线程，线程数一定要小于机器核数。还需要注意的是，线程数并不是越大越好，官方认为超过了8个基本就没什么意义了。 性能方面，Redis作者在RedisConf 2019的分享中提到，开启多线程后的Redis性能可以提升一倍左右。 ","link":"https://mask0407.github.io/redis00/"},{"title":"使用Docker搭建Greenplum","content":"准备工作 Docker环境 Greenplum安装包 greenplum-db-6.4.0-rhel7-x86_64.rpm 密码:sv4p 安装 Dockerfile文件 FROM lyasper/gphost COPY greenplum-db-6.4.0-rhel7-x86_64.rpm /home/gpadmin/greenplum-db.rpm RUN rpm -i /home/gpadmin/greenplum-db.rpm --nodeps --force RUN chown -R gpadmin /usr/local/greenplum-db* RUN rm -f /home/gpadmin/greenplum-db.rpm docker-compose.yaml文件 version: '3' services: mdw: hostname: mdw image: &quot;mygreenplum&quot; ports: - &quot;2222:22&quot; - &quot;5432:5432&quot; sdw1: hostname: sdw1 image: &quot;mygreenplum&quot; sdw2: hostname: sdw2 image: &quot;mygreenplum&quot; etl: hostname: etl image: &quot;mygreenplum&quot; 执行Dockerfile docker build . -t mygreenplum 执行docker-compose docker-compose up -d # 在docker-compose.yaml所在目录执行 集群配置 目前Docker容器已经启动，但是greenplum的配置还没有更改 登陆至greenplum主节点 ssh -p 2222 gpadmin@127.0.0.1 # 密码 changeme 先刷新下配置文件 source /usr/local/greenplum-db/greenplum_path.sh 初始化配置 ./artifact/prepare.sh -s 2 -n 2 # -s 表示 segment 机器（容器）的个数 # -n 表示每个容器里 primary segment 的个数 初始化集群 source env.sh gpinitsystem -a -c gpinitsystem_config 至此安装成功 ","link":"https://mask0407.github.io/docker00/"},{"title":"关于Springboot、SpringCloud以及SpringCloud-Alibaba Nacos依赖问题","content":" SpringBoot和SpringCloud大版本对应关系 Alibaba组件版本关系 SpringBoot、Cloud、Alibaba 毕业版本依赖关系(推荐使用) 依赖管理 RELEASE 版本 Alibaba组件依赖坐标示例 (依赖关系) 由于目前阿里的SpringCloud组件以及孵化成功，导致写项目导入pom依赖坐标时比较纠结，网上对版本的兼容介绍也变得五花八门。 SpringBoot和SpringCloud大版本对应关系 Spring Boot Spring Cloud 1.2.x Angel版本 1.3.x Brixton版本 1.4.x stripes Camden版本 1.5.x Dalston版本、Edgware版本 2.0.x Finchley版本 2.1.x Greenwich.SR2 详细介绍，建议火狐浏览器 Alibaba组件版本关系 Spring Cloud Alibaba Version Sentinel Version Nacos Version RocketMQ Version Dubbo Version Seata Version (毕业版本) 2.2.0.RELEASE 1.7.1 1.1.4 4.4.0 2.7.4.1 1.0.0 (毕业版本) 2.1.1.RELEASE or 2.0.1.RELEASE or 1.5.1.RELEASE 1.7.0 1.1.4 4.4.0 2.7.3 0.9.0 (毕业版本) 2.1.0.RELEASE or 2.0.0.RELEASE or 1.5.0.RELEASE 1.6.3 1.1.1 4.4.0 2.7.3 0.7.1 (孵化器版本) 0.9.0.RELEASE or 0.2.2.RELEASE or 0.1.2.RELEASE 1.5.2 1.0.0 4.4.0 2.7.1 0.4.2 (孵化器版本) 0.2.1.RELEASE or 0.1.1.RELEASE 1.4.0 0.6.2 4.3.1 ❌ ❌ (孵化器版本) 0.2.0.RELEASE or 0.1.0.RELEASE 1.3.0-GA 0.3.0 ❌ ❌ ❌ SpringBoot、Cloud、Alibaba 毕业版本依赖关系(推荐使用) Spring Cloud Version Spring Cloud Alibaba Version Spring Boot Version Spring Cloud Hoxton 2.2.0.RELEASE 2.2.X.RELEASE Spring Cloud Greenwich 2.1.1.RELEASE 2.1.X.RELEASE Spring Cloud Finchley 2.0.1.RELEASE 2.0.X.RELEASE Spring Cloud Edgware 1.5.1.RELEASE 1.5.X.RELEASE 依赖管理 Spring Cloud Alibaba BOM 包含了它所使用的所有依赖的版本。 RELEASE 版本 Spring Cloud Hoxton 如果需要使用 Spring Cloud Hoxton 版本，请在 dependencyManagement 中添加如下内容 &lt;dependency&gt; &lt;groupId&gt;com.alibaba.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-alibaba-dependencies&lt;/artifactId&gt; &lt;version&gt;2.2.0.RELEASE&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;scope&gt;import&lt;/scope&gt; &lt;/dependency&gt; ```xml - Spring Cloud Greenwich 如果需要使用 Spring Cloud Greenwich 版本，请在 dependencyManagement 中添加如下内容 ```xml &lt;dependency&gt; &lt;groupId&gt;com.alibaba.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-alibaba-dependencies&lt;/artifactId&gt; &lt;version&gt;2.1.1.RELEASE&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;scope&gt;import&lt;/scope&gt; &lt;/dependency&gt; Spring Cloud Finchley 如果需要使用 Spring Cloud Finchley 版本，请在 dependencyManagement 中添加如下内容 &lt;dependency&gt; &lt;groupId&gt;com.alibaba.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-alibaba-dependencies&lt;/artifactId&gt; &lt;version&gt;2.0.1.RELEASE&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;scope&gt;import&lt;/scope&gt; &lt;/dependency&gt; Spring Cloud Edgware 如果需要使用 Spring Cloud Edgware 版本，请在 dependencyManagement 中添加如下内容 &lt;dependency&gt; &lt;groupId&gt;com.alibaba.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-alibaba-dependencies&lt;/artifactId&gt; &lt;version&gt;1.5.1.RELEASE&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;scope&gt;import&lt;/scope&gt; &lt;/dependency&gt; Alibaba组件依赖坐标示例 以Nacos为例 &lt;dependency&gt; &lt;groupId&gt;com.alibaba.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-alibaba-nacos-config&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!-- https://mvnrepository.com/artifact/com.alibaba.cloud/spring-cloud-starter-alibaba-nacos-discovery --&gt; &lt;dependency&gt; &lt;groupId&gt;com.alibaba.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-alibaba-nacos-discovery&lt;/artifactId&gt; &lt;/dependency&gt; 摘自 版本说明 更多孵化器版本对应可点击链接去官网查看 ","link":"https://mask0407.github.io/springcloud07/"},{"title":"Feign开启HyStrix后如何配置线程隔离及熔断策略","content":" Feign集成Hystrix默认是关闭Hystrix的，只有在配置文件中设置feign.hystrix.enabled=true才会开启Hystrix。 开启Hystrix后feign之间的方法调用就会默认启动新的线程执行和主程序不在一个线程中，因此如果上下文中存在ThreadLocal变量，在该方法中就失效了。因此一般可以通过设置CommandProperties注解属性，设置线程就可以了。 在和Feign整合后，用户无法配置Feign的ComandProperties，但是可以通过配置Bean的形式配置。 @Configuration public class FeignSupportConfig { @Bean public SetterFactory setterFactory(){ SetterFactory setterFactory =new SetterFactory() { @Override public HystrixCommand.Setter create(Target&lt;?&gt; target, Method method) { String groupKey = target.name(); String commandKey = Feign.configKey(target.type(), method); HystrixCommandProperties.Setter setter = HystrixCommandProperties.Setter() //设置统计指标60秒为一个时间窗口 .withMetricsRollingStatisticalWindowInMilliseconds(1000 * 60) //超过80%失败率 .withCircuitBreakerErrorThresholdPercentage(80) //操作5个开启短路器 .withCircuitBreakerRequestVolumeThreshold(5) //设置线程隔离 .withExecutionIsolationStrategy(HystrixCommandProperties.ExecutionIsolationStrategy.SEMAPHORE) //设置断路器的开启时间为60秒 .withCircuitBreakerSleepWindowInMilliseconds(1000 * 60); return HystrixCommand.Setter .withGroupKey(HystrixCommandGroupKey.Factory.asKey(groupKey)) .andCommandKey(HystrixCommandKey.Factory.asKey(commandKey)) .andCommandPropertiesDefaults(setter); } }; return setterFactory; } } 然后在@FeignClient中引入该配置 @FeignClient(name = &quot;USER-SERVICE&quot;, configuration = FeignSupportConfig.class) //name为服务名 至此，方法调用将会和主线程ID一直。 ","link":"https://mask0407.github.io/springcloud06/"},{"title":"SpringCloud之配置中心-Config","content":"Config-Server Spring Cloud Config 是 Spring Cloud 团队创建的一个全新项目，用来为分布式系统中的基础设施和微服务应用提供集中化的外部配置支持， 它分为服务端与客户端两个部分。服务端称为分布式配置中心， 它是一个独立的微服务应用， 用来连接配置仓库并为客户端提供获取配置信息、 加密／解密信息等访问接口；客户端微服务架构中的各个微服务应用或基础设施， 它们通过指定的配置中心来管理应用资源与业务相关的配置内容，并在启动的时候从配置中心获取和加载配置信息。 Spring Cloud Config 实现了对服务端和客户端中环境变量和属性配置的抽象映射， 所以它除了适用于 Spring 构建的应用程序之外，也可以在任何其他语言运行的应用程序中使用。 由于 Spring Cloud Config 实现的配置中心默认采用 Git 来存储配置信息， 所以使用 Spring Cloud Config 构建的配置服务器，天然就支持对微服务应用配置信息的版本管理， 并且可以通过 Git 客户端工具来方便地管理和访问配置内容。 当然它也提供了对其他存储方式的支持， 比如 SVN 仓库、 本地化文件系统。 快速入门 配置中心服务 在码云(GITEE)新建配置文件（可选GitHub、GitLab） 在码云新建项目config-server，在新项目下新建配置文件夹config，在config下创建application-test.properties，配置文件命名规则应尽可能使用：{application}-{profile}.{properties|yml} config-server/config/application-test.properties name=mask age=18 pom.xml &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-config-server&lt;/artifactId&gt; &lt;/dependency&gt; 项目中application.properties server.port=8081 spring.cloud.config.server.git.uri=https://gitee.com/mask_0407/config-server.git spring.cloud.config.server.git.username=****** #码云账号 spring.cloud.config.server.git.password=****** #码云密码 spring.cloud.config.server.git.search-paths=/config App.java @SpringBootApplication @EnableConfigServer public class App { public static void main(String[] args) { SpringApplication.run(App.class, args); } } 启动项目后访问：http://localhost:8081/application-test.properties Config-Client pom.xml &lt;!--config client依赖--&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-config&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!--ConfigurationProperties类所需依赖，手动添加的--&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-configuration-processor&lt;/artifactId&gt; &lt;optional&gt;true&lt;/optional&gt; &lt;/dependency&gt; 码云中application-test.properties server.port = 8082 name=mask age=18 bootstrap.properties server.port = 8082 spring.application.name = application #对应application-test.properties 中的application spring.cloud.config.profile = test #对应application-test.properties 中的test spring.cloud.config.uri=http://localhost:8081 # config-server 地址 # 开启所有的健康检查 management.endpoints.web.exposure.include=* AppClient @SpringBootApplication @EnableDiscoveryClient @RestController public class AppClient { @Value(&quot;${server.port}&quot;) private String port; public static void main(String[] args) { SpringApplication.run(AppClient.class, args); } @RequestMapping(&quot;print&quot;) public String print() { return port; } } 启动项目访问http://localhost:8082/print ","link":"https://mask0407.github.io/springcloud05/"},{"title":"SpringCloud之OpenFeign","content":"Feign提供声明式的远程调用，借用动态代理实现远程调用，使编写Web服务客户端变得更容易。 Spring Cloud增加了对Spring MVC注释的支持，并使用了Spring Web中默认使用的相同HttpMessageConverters。 Spring Cloud集成了Ribbon和Eureka，在使用Feign时提供负载均衡的http客户端。 快速入门 Eureka-server配置 server.port=8088 eureka.instance.hostname=localhost eureka.client.register-with-eureka=false eureka.client.fetch-registry=false spring.security.user.name=mask spring.security.user.password=111111 Eureka-Server @SpringBootApplication @EnableEurekaServer public class EurekaApp{ public static void main(String[] args) { SpringApplication.run(EurekaApp.class,args); } } 服务提供者A spring.application.name=USER-SERVICE eureka.instance.instance-id=001 eureka.instance.prefer-ip-address=true eureka.instance.lease-expiration-duration-in-seconds=20 eureka.instance.lease-renewal-interval-in-seconds=10 eureka.client.register-with-eureka=true eureka.client.healthcheck.enabled=true eureka.client.fetch-registry=false eureka.client.service-url.defaultZone=http://mask:111111@localhost:8088/eureka/ @SpringBootApplication @EnableEurekaClient @RestController @RequestMapping(&quot;main&quot;) public class EurekaApp { public static void main(String[] args) { SpringApplication.run(EurekaApp.class,args); } @RequestMapping(value = &quot;index&quot;, method = {RequestMethod.GET}) public String index() { return &quot;服务A&quot;; } } 服务提供者B server.port=8081 spring.application.name=USER-SERVICE eureka.instance.instance-id=002 eureka.instance.prefer-ip-address=true eureka.instance.lease-expiration-duration-in-seconds=20 eureka.instance.lease-renewal-interval-in-seconds=10 eureka.client.register-with-eureka=true eureka.client.healthcheck.enabled=true eureka.client.fetch-registry=false eureka.client.service-url.defaultZone=http://mask:111111@localhost:8088/eureka/ @SpringBootApplication @EnableEurekaClient @RestController @RequestMapping(&quot;main&quot;) public class EurekaApp { public static void main(String[] args) { SpringApplication.run(EurekaApp.class,args); } @RequestMapping(value = &quot;index&quot;, method = {RequestMethod.GET}) public String index() { return &quot;服务B&quot;; } } Eureka-Consumer 新增依赖 &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-openfeign&lt;/artifactId&gt; &lt;/dependency&gt; 创建接口 UserServiceController @FeignClient(name = &quot;USER-SERVICE&quot;) //name为服务名 public interface UserServiceController { @GetMapping(&quot;main/index&quot;) //该方法映射的路径 String index(); } server.port=8089 ribbon.NFLoadBalancerRuleClassName=com.netflix.loadbalancer.RandomRule eureka.client.fetch-registry=true eureka.client.register-with-eureka=false eureka.client.service-url.defaultZone=http://mask:111111@localhost:8088/eureka/ @SpringBootApplication @RestController @EnableFeignClients public class EurekaSpringBootConsumer { public static void main(String[] args) { SpringApplication.run(EurekaSpringBootConsumer.class,args); } @Autowired private UserServiceController userServiceController; @GetMapping(&quot;/rpc/test&quot;) public String index() { String index = userServiceController.index(); System.out.println(index); return &quot;OK&quot;; } } 访问http://localhost:8089/rpc/test 底层会根据接口方法的配置信息参数发送请求给远程的服务器这种实现比单纯的使用Ribbon更加的优雅。实际上Feign底层就是对Ribbon组件的封装。 注：在Fegin里不能传递自定义类型至GET查询参数 Feigen 熔断 默认Feign没有开启熔断策略，需要用户在配置文件中指定 EurekaConsumer中新增配置 feign.hystrix.enabled=true 修改UserServiceController @FeignClient(name = &quot;USER-SERVICE&quot;, fallback = UserServiceFailBack.class) //name为服务名 public interface UserServiceController { @GetMapping(&quot;main/index&quot;) //该方法映射的路径 String index(); } 新增UserServiceFailBack类 @Component public class UserServiceFailBack implements UserServiceController{ @Override public String index() { return &quot;熔断&quot;; } } 修改服务B使其抛出异常 @RestController @RequestMapping(&quot;main&quot;) public class TestController { @RequestMapping(value = &quot;index&quot;, method = {RequestMethod.GET}) public String index() { int i = 1/0; return &quot;服务B&quot;; } } 启动服务多次访问http://localhost:8089/rpc/test如下图可看出服务B抛出异常后执行熔断后代码 Feign超时设置 feign.client.config.USER-SERVICE.connect-timeout=500 feign.client.config.USER-SERVICE.read-timeout=500 Hystrix Dashboard pom引入 &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-netflix-hystrix-dashboard&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-netflix-hystrix&lt;/artifactId&gt; &lt;/dependency&gt; application.properties新增 management.endpoints.web.exposure.include=* 启动类新增注解 @EnableHystrixDashboard @EnableCircuitBreaker 配置HyStrix属性 在和Feign整合后，用户无法配置Feign的ComandProperties，但是可以通过配置Bean的形式配置 ","link":"https://mask0407.github.io/springcloud04/"},{"title":"SpringCloud之服务注册-Eureka","content":"Spring Cloud Eureka 是 Spring Cloud Netflix 微服务套件中的一部分， 它基于 Netflix Eureka 做了二次封装， 主要负责完成微服务架构中的服务治理功能。 Spring Cloud 通过为Eureka 增加了 Spring Boot 风格的自动化配置，我们只需通过简单引入依赖和注解配置就能让 Spring Boot构建的微服务应用轻松地与 Eureka 服务治理体系进行整合。 Eureka单机 依赖 &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-netflix-eureka-server&lt;/artifactId&gt; &lt;/dependency&gt; applicaiton.properties server.port=8088 eureka.instance.hostname=localhost eureka.client.register-with-eureka=false eureka.client.fetch-registry=false 必须配置eureka.instance.hostname否则Eureka会尝试自我注册 EurekaSpringBootApplication @SpringBootApplication @EnableEurekaServer public class EurekaSpringBootApplication { public static void main(String[] args) { SpringApplication.run(EurekaSpringBootApplication.class,args); } } 启动项目后访问 http://localhost:8088/ 配置说明 配置 说明 默认 eureka.server.enable-self-preservation Eureka自我保护机制 true eureka.server.eviction-interval-timer-in-ms Eureka 剔除故障节点时间间隔 60秒 eureka.server.renewal-percent-threshold Eureka租约计算阈值 0.85 eureka.client.register-with-eureka 是否注册到Eureka上 true eureka.client.fetch-registry 是否从Eureka上更新列表信息 true eureka.instance.hostname Eureka微服务实例的主机名 无 服务注册 依赖 &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-netflix-eureka-client&lt;/artifactId&gt; &lt;/dependency&gt; application.properties spring.application.name=USER-SERVICE eureka.instance.instance-id=001 eureka.instance.prefer-ip-address=true eureka.instance.lease-expiration-duration-in-seconds=20 eureka.instance.lease-renewal-interval-in-seconds=10 eureka.client.register-with-eureka=true eureka.client.healthcheck.enabled=true eureka.client.fetch-registry=false eureka.client.service-url.defaultZone=http://localhost:8088/eureka/ 运行项目实现服务的注册 注册两个服务后 引入依赖 Eureka中默认集成了Ribbon插件，因此只需导入spring-cloud-starter-netflix-eureka-client即可。 pom.xml &lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;2.1.5.RELEASE&lt;/version&gt; &lt;/parent&gt; &lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-dependencies&lt;/artifactId&gt; &lt;version&gt;Greenwich.SR1&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;scope&gt;import&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;/dependencyManagement&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-netflix-eureka-client&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt; application.properties eureka.client.fetch-registry=true eureka.client.register-with-eureka=false eureka.client.service-url.defaultZone=http://localhost:8088/eureka/ EurekaSpringBootConsumer @SpringBootApplication public class EurekaSpringBootConsumer { public static void main(String[] args) { SpringApplication.run(EurekaSpringBootConsumer.class,args); } @Bean @LoadBalanced public RestTemplate restTemplate(){ return new RestTemplate(); } } RestTemplateTests @SpringBootTest(classes = EurekaSpringBootConsumer.class) @RunWith(SpringRunner.class) public class RestTemplateTests { @Autowired private RestTemplate restTemplate; @Test public void testQueryUserById(){ String url=&quot;http://USER-SERVICE/manager/user/8&quot;; User user = restTemplate.getForObject(url, User.class); System.out.println(user); } } Eureka Server安全 新增依赖 &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-security&lt;/artifactId&gt; &lt;/dependency&gt; 新增配置 spring.security.user.name=mask spring.security.user.password=111111 新增配置类 WebSecurityConfig @EnableWebSecurity @Configuration public class WebSecurityConfig extends WebSecurityConfigurerAdapter { @Override protected void configure(HttpSecurity http) throws Exception { http.csrf().disable(); // 关闭csrf,否则其他主机无法登陆认证 http.authorizeRequests().anyRequest().authenticated().and().httpBasic();//设置成httpBasic，不可以设置为表单 } } 服务 注册/消费 新增配置 eureka.client.service-url.defaultZone=http://mask:111111@localhost:8088/eureka/ Eureka服务端高可用配置 application-eureka-1.properties server.port=6666 ## 指定当前注册中心的服务名称 spring.application.name=eurekaregistry ## 启用注册中心主动失效，并且每次主动失效检测间隔为5s 默认值60s eureka.server.eviction-interval-timer-in-ms= 5000 ## 设置eureka注册中心的响应更新时间 eureka.server.responseCacheUpdateIntervalMs=3000 eureka.server.responseCacheAutoExpirationInSeconds=60 ## 配置注册中心的主机名 eureka.instance.instance-id = eureka-1 eureka.instance.hostname = CentOSA ## 服务刷新时间配置，每隔这个时间会主动心跳一次 eureka.instance.lease-renewal-interval-in-seconds= 5 ## 服务提供者被认定为丢失心跳超时，失效多久后被删除 eureka.instance.lease-expiration-duration-in-seconds=15 ## 配置定时获取|抓取注册中心的数据时间 eureka.client.registry-fetch-interval-seconds= 5 eureka.client.instance-info-replication-interval-seconds= 5 ## 配置集群中其他eureka实例，用于本eureka实例的注册方。 eureka.client.region=beijing eureka.client.availability-zones.beijing=zone-2,zone-3 eureka.client.service-url.zone-2=http://mask:111111@CentOSB:1111/eureka/ eureka.client.service-url.zone-3=http://mask:111111@CentOSC:1111/eureka/ spring.security.user.name=mask spring.security.user.password=111111 application-eureka-2.properties server.port=6666 ## 指定当前注册中心的服务名称 spring.application.name=eurekaregistry ## 启用注册中心主动失效，并且每次主动失效检测间隔为5s 默认值60s eureka.server.eviction-interval-timer-in-ms= 5000 ## 设置eureka注册中心的响应更新时间 eureka.server.responseCacheUpdateIntervalMs=3000 eureka.server.responseCacheAutoExpirationInSeconds=60 ## 配置注册中心的主机名 eureka.instance.instance-id = eureka-2 eureka.instance.hostname = CentOSB ## 服务刷新时间配置，每隔这个时间会主动心跳一次 eureka.instance.lease-renewal-interval-in-seconds= 5 ## 服务提供者被认定为丢失心跳超时，失效多久后被删除 eureka.instance.lease-expiration-duration-in-seconds=15 ## 配置定时获取|抓取注册中心的数据时间 eureka.client.registry-fetch-interval-seconds= 5 eureka.client.instance-info-replication-interval-seconds= 5 ## 配置集群中其他eureka实例，用于本eureka实例的注册方。 eureka.client.region=beijing eureka.client.availability-zones.beijing=zone-1,zone-3 eureka.client.service-url.zone-1=http://mask:111111@CentOSA:1111/eureka/ eureka.client.service-url.zone-3=http://mask:111111@CentOSC:1111/eureka/ spring.security.user.name=mask spring.security.user.password=111111 application-eureka-3.properties server.port=6666 ## 指定当前注册中心的服务名称 spring.application.name=eurekaregistry ## 启用注册中心主动失效，并且每次主动失效检测间隔为5s 默认值60s eureka.server.eviction-interval-timer-in-ms= 5000 ## 设置eureka注册中心的响应更新时间 eureka.server.responseCacheUpdateIntervalMs=3000 eureka.server.responseCacheAutoExpirationInSeconds=60 ## 配置注册中心的主机名 eureka.instance.instance-id = eureka-3 eureka.instance.hostname = CentOSC ## 服务刷新时间配置，每隔这个时间会主动心跳一次 eureka.instance.lease-renewal-interval-in-seconds= 5 ## 服务提供者被认定为丢失心跳超时，失效多久后被删除 eureka.instance.lease-expiration-duration-in-seconds=15 ## 配置定时获取|抓取注册中心的数据时间 eureka.client.registry-fetch-interval-seconds= 5 eureka.client.instance-info-replication-interval-seconds= 5 ## 配置集群中其他eureka实例，用于本eureka实例的注册方。 eureka.client.region=beijing eureka.client.availability-zones.beijing=zone-1,zone-2 eureka.client.service-url.zone-1=http://mask:111111@CentOSA:1111/eureka/ eureka.client.service-url.zone-2=http://mask:111111@CentOSB:1111/eureka/ spring.security.user.name=mask spring.security.user.password=111111 将三个项目打包分别上传至CentOSA、CentOSB、CentOSC运行 注：三台机器需关闭防火墙并互相做主机名与IP映射 访问http://CentOSA:6666/ ---CentOSA可换为ip ","link":"https://mask0407.github.io/springcloud03/"},{"title":"SpringCloud之熔断器-Hystrix","content":"Hystrix是一个延迟和容错库，旨在隔离对远程系统，服务和第三方库的访问点，停止级联故障，并在复杂的分布式系统中实现弹性，在这些系统中，故障是不可避免的。 依赖 &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-netflix-hystrix&lt;/artifactId&gt; &lt;/dependency&gt; SpringBootApplication入口中需要添加@EnableCircuitBreaker注解，此时Spring工厂会启动AOP方式对所有的方法上有@HystrixCommand的业务方法添加熔断策略。 @SpringBootApplication @EnableCircuitBreaker public class HystrixSpringBootApplication { public static void main(String[] args) { SpringApplication.run(HystrixSpringBootApplication.class,args); } } 在业务方法上添加@HystrixCommand注解实现熔断。 @HystrixCommand @Override public User queryUserById(Integer id) { System.out.println(Thread.currentThread().getId()); return new User(&quot;未熔断&quot;, new Date(), true, 11.11); } 线程隔离 默认该方法的执行会启动新的线程执行和主程序不在一个线程中，因此如果上下文中存在ThreadLocal变量，在该方法中就失效了。因此一般可以通过设置commandProperties注解属性，设置线程就可以了。 默认情况Service和Controller线程ID @HystrixCommand(commandProperties = { @HystrixProperty(name = &quot;execution.isolation.strategy&quot;,value = &quot;SEMAPHORE&quot;) }) @Override public User queryUserById(Integer id) { System.out.println(&quot;Service线程ID:&quot; + Thread.currentThread().getId()); return new User(&quot;未熔断&quot;, new Date(), true, 11.11); } 设置后Service和Controller线程ID execution.isolation.strategy该属性的可选值有两个THREAD和SEMAPHORE默认值是THREAD。①一般如果一个实例一秒钟有100个并发，此时因为频繁启动线程的开销过大此时一般考虑使用SEMAPHORE，②非网络调用。 Fallback 过在@HystrixCommand中声明fallbackMethod的名称可以实现优雅降级，如下所示： @HystrixCommand(fallbackMethod = &quot;fallbackMethodQueryUserById&quot;) @Override public User queryUserById(Integer id) { System.out.println(Thread.currentThread().getId()); int i=10/0; return new User(&quot;未熔断&quot;, new Date(), true, 11.11); } public User fallbackMethodQueryUserById(Integer id, Throwable e) { System.out.println(e.getMessage()); return new User(&quot;熔断降级后&quot;, new Date(), true, 11.11); } 调用结果： 注意要求fallbackMethod方法和目标方法必须在同一个类中，具有相同的参数（异常参数可选） Error Propagation 根据此描述，@ HystrixCommand能够指定应忽略的异常类型。如下所述ArithmeticException: / by zero将不会触发fallbackMethod方法。 // @HystrixCommand(fallbackMethod = &quot;fallbackMethodQueryUserById&quot;) // @HystrixCommand(commandProperties = { // @HystrixProperty(name = &quot;execution.isolation.strategy&quot;, value = &quot;SEMAPHORE&quot;) // }) @HystrixCommand(fallbackMethod = &quot;fallbackMethodQueryUserById&quot;,ignoreExceptions = {ArithmeticException.class}) @Override public User getUser(Integer id) { System.out.println(&quot;Service线程ID:&quot; + Thread.currentThread().getId()); int i = 10 / 0; return new User(&quot;未熔断&quot;, new Date(), true, 11.11); } public User fallbackMethodQueryUserById(Integer id, Throwable e) { System.out.println(e.getMessage()); return new User(&quot;熔断降级后&quot;, new Date(), true, 11.11); } 请求超时熔断 用户可以通过设置execution.isolation.thread.timeoutInMilliseconds属性设置一个方法最大请求延迟，系统会抛出HystrixTimeoutException @HystrixCommand(fallbackMethod = &quot;fallbackMethodQueryUserById&quot;,commandProperties = { @HystrixProperty(name=&quot;execution.isolation.thread.timeoutInMilliseconds&quot;,value=&quot;100&quot;) }) @Override public User getUser(Integer id) { System.out.println(&quot;Service线程ID:&quot; + Thread.currentThread().getId()); try { Thread.sleep(500); } catch (InterruptedException e) { e.printStackTrace(); } // int i = 10 / 0; return new User(&quot;未熔断&quot;, new Date(), true, 11.11); } public User fallbackMethodQueryUserById(Integer id, Throwable e) { System.out.println(e.getMessage()); return new User(&quot;超时熔断&quot;, new Date(), true, 11.11); } 执行结果： https://github.com/Netflix/Hystrix/tree/master/hystrix-contrib/hystrix-javanica https://github.com/Netflix/Hystrix/wiki/Configuration Hystrix Dashboard 依赖 &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-netflix-hystrix-dashboard&lt;/artifactId&gt; &lt;/dependency&gt; SpringBoot入口类添加@EnableHystrixDashboard注解 访问页面http://localhost:端口/hystrix ","link":"https://mask0407.github.io/springcloud02/"},{"title":"SpringCloud之注册中心-Nacos","content":"Nacos简介 Nacos是阿里巴巴开源的一款支持服务注册与发现，配置管理以及微服务管理的组件。用来取代以前常用的注册中心（zookeeper , eureka等等），以及配置中心（spring cloud config等等）。Nacos是集成了注册中心和配置中心的功能，做到了二合一。 Nacos gitHub : https://github.com/alibaba/nacos Nocos 文档 ：https://nacos.io/zh-cn/docs/what-is-nacos.html Nacos 支持几乎所有主流类型的“服务”的发现、配置和管理，如： Kubernetes Service gRPC &amp; Dubbo RPC Service Spring Cloud RESTful Service Nacos安装 下载 GitHub下载失败或速度过慢可以去 码云 搜索Nacos 以码云为例（1.1.4版本） 安装并启动服务 # 下载完毕后解压至本地 cd nacos/ # maven编译 mvn -Prelease-nacos clean install -U # 编译完成后启动服务 cd distribution/target/nacos/bin #Linux/Mac环境执行 sh startup.sh -m standalone # Windows环境执行 cmd startup.cmd 启动后查看日志 cd ../logs tail -f start.out Nacos端口默认为8848 访问地址：http://localhost:8848/nacos/ ","link":"https://mask0407.github.io/springcloud01/"},{"title":"SpringCloud之Ribbon-负载均衡","content":"负载均衡:Spring Cloud Ribbon Spring Cloud Ribbon 是一个基于Http和TCP的客服端负载均衡工具，它是基于Netflix Ribbon实现的。通过SpringCloud的自动配置使得项目可以自动的给RestTemplate添加拦截器，实现负载均衡的作用。 快速入门 pom.xml &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;com.mask&lt;/groupId&gt; &lt;artifactId&gt;cloud&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;properties&gt; &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt; &lt;java.version&gt;1.8&lt;/java.version&gt; &lt;/properties&gt; &lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;2.1.5.RELEASE&lt;/version&gt; &lt;/parent&gt; &lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-dependencies&lt;/artifactId&gt; &lt;version&gt;Greenwich.SR1&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;scope&gt;import&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;/dependencyManagement&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-netflix-ribbon&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;/project&gt; application.properties server.port=8080 #server.port=8088 USER-SERVICE.ribbon.listOfServers=localhost:8080,localhost:8088 USER-SERVICE.ribbon.NFLoadBalancerRuleClassName=com.netflix.loadbalancer.RandomRule UserController @RestController @RequestMapping(&quot;manager&quot;) public class UserController { @RequestMapping(value=&quot;user/{id}&quot;,method = {RequestMethod.GET}) public User queyUserById(@PathVariable(value = &quot;id&quot;) Integer id){ System.out.println(&quot;-------getUser--------&quot;); return new User(&quot;张三&quot;, new Date(), true, 12345.00); } @RequestMapping(value=&quot;user&quot;,method = {RequestMethod.POST}) public User saveUser(@RequestBody User user){ System.out.println(&quot;-------saveUser--------&quot;); return user; } @RequestMapping(value=&quot;user&quot;,method = {RequestMethod.PUT}) public void update(@RequestBody User user){ System.out.println(&quot;-------updateUser--------&quot;); } @RequestMapping(value=&quot;user/{pageNow}/{pageSize}&quot;,method = {RequestMethod.GET}) public List&lt;User&gt; queyUserByPage(@PathVariable(value = &quot;pageNow&quot;) Integer pageNow, @PathVariable(value = &quot;pageSize&quot;) Integer pageSize){ List&lt;User&gt; list = new ArrayList&lt;&gt;(); list.add(new User(&quot;张三&quot;, new Date(), false, 11111.00)); list.add(new User(&quot;李四&quot;, new Date(), true, 20000.00)); System.out.println(&quot;-------getUserPage--------&quot;); return list; } @RequestMapping(value=&quot;user/{ids}&quot;,method = {RequestMethod.DELETE}) public void deleteUserById(@PathVariable(value = &quot;ids&quot;) Integer[] ids){ System.out.println(&quot;-------deleteUsers--------&quot;); } } SpringRibbonApplication @SpringBootApplication public class SpringRibbonApplication { public static void main(String[] args) { SpringApplication.run(SpringRibbonApplication.class,args); } @Bean @LoadBalanced public RestTemplate restTemplate(){ return new RestTemplate(); } } User public class User implements Serializable { private Integer id; private String name; @DateTimeFormat(pattern = &quot;yyyy-MM-dd HH:mm:ss&quot;) @JsonFormat(pattern = &quot;yyyy-MM-dd HH:mm:ss&quot;) private Date birthDay; private Boolean sex; private Double salary; } RestTemplateTests @SpringBootTest(classes = SpringRibbonApplication.class) @RunWith(SpringRunner.class) public class RestTemplateTests { @Autowired private RestTemplate restTemplate; @Test public void testQueryUserById(){ String url=&quot;http://USER-SERVICE/manager/user/1&quot;; User user = restTemplate.getForObject(url, User.class); System.out.println(user); } @Test public void testQueryUserByPage(){ String url=&quot;http://USER-SERVICE/manager/user/1/10&quot;; List&lt;User&gt; users = restTemplate.getForObject(url, List.class); for (int i = 0; i &lt; users.size(); i++) { System.out.println(users.get(i)); } } @Test public void testSaveUser(){ String url=&quot;http://USER-SERVICE/manager/user&quot;; User user = restTemplate.postForObject(url, new User(&quot;李四&quot;, new Date(), true, 15000.0), User.class); System.out.println(user); } @Test public void testUpdateUser(){ String url=&quot;http://USER-SERVICE/manager/user&quot;; User user = new User(&quot;李四&quot;, new Date(), true, 18000.0); user.setId(3); restTemplate.put(url,user); } @Test public void testDeleteUser(){ String url=&quot;http://USER-SERVICE/manager/user/1,2,3&quot;; restTemplate.delete(url); } } 除此自外，SpringCloud提供了一种基于配置文件的配置方式 SpringRibbonApplication @SpringBootApplication @RibbonClient(name = &quot;USER-SERVICE&quot;,configuration = {UserSerivceRibbonConfigure.class}) public class SpringRibbonApplication { public static void main(String[] args) { SpringApplication.run(SpringRibbonApplication.class,args); } @Bean @LoadBalanced public RestTemplate restTemplate(){ return new RestTemplate(); } } UserSerivceRibbonConfigure @Configuration public class UserSerivceRibbonConfigure { @Bean public ServerList&lt;Server&gt; ribbonServerList(){ Server server1 = new Server(&quot;localhost&quot;, 8080); server1.setZone(&quot;beijing&quot;); Server server2 = new Server(&quot;localhost&quot;, 9090); server2.setZone(&quot;shanghai&quot;); return new StaticServerList&lt;Server&gt;(server1,server2); } @Bean public IRule ribbonRule(){ return new RandomRule(); } @Bean public ZonePreferenceServerListFilter ribbonServerListFilter(){ ZonePreferenceServerListFilter filter = new ZonePreferenceServerListFilter(); filter.setZone(&quot;beijing&quot;); return filter; } } 用户可以直接使用LoadBalancerClient查询服务列表 @Autowired private LoadBalancerClient loadBalancer; @Test public void testChoose(){ for (Integer i=0;i&lt;10;i++){ ServiceInstance instance = loadBalancer.choose(&quot;stores&quot;); URI storesUri = URI.create(String.format(&quot;http://%s:%s&quot;, instance.getHost(), instance.getPort())); System.out.println(storesUri); } } 测试结果 ","link":"https://mask0407.github.io/springcloud00/"},{"title":"Elasticsearch入门篇","content":"ElasticSearch ElasticSearch:简称为ES，基于Lucene全文检索引擎服务，支持分布式集群（数据横向扩展、分布式计算） 应用场景：1. 全文检索或者搜索服务 2. NOSQL数据库(ES中的数据单元为JSON) 3. ELK数据分析平台 NRT (near real time) 接近实时 ES中的一条数据写入后大概会有1s的延迟才能被检索到 9300端口:Es节点之间通讯使用 9200:Es节点和外部通讯使用 Es概念介绍: 名词 说明 额外补充 索引(index) 类似于数据库中的database 类型(type) 相当于sql中的table Es6.0以后删除了类型的概念,6.0还可以设置类型,但只能设置一个 文档(Document) 相当于sql中的一行记录 分片(Shard) 每个索引都有1到多个分片, 每个分片都是一个luncene索引 片的好处: 分摊索引的搜索压力, 分片还支持水平的拓展和拆分以及分布式的操作, 可以提高搜索和其他处理的效率 备份/复制(replicas) 拷贝一个分片就完成了分片的备份 备份的好处: 当主分片失败或者挂掉, 备份就可以代替分片进行操作, 进而提高了es的可用性, 备份的分片还可以进行搜索操作, 以分摊搜索的压力. 映射(Mapping) 类似于Table的Schema(模式) 例如:create table person_info(name varchar(20),age tinyint)创建一张表,person后的括号是定义表中的字段,即为Schema analyzer（分析器）介绍 analyzer（分析器）是一个包，这个包由三部分组成，分别是：character filters （字符过滤器）、tokenizer（分词器）、token filters（token过滤器）。 一个analyzer可以有0个或多个character filters 一个analyzer有且只能有一个tokenizer 一个analyzer可以有0个或多个token filters character filter 是做字符转换的，它接收的是文本字符流，输出也是字符流 tokenizer 是做分词的，它接收字符流，输出token流（文本拆分后变成一个一个单词，这些单词叫token） token filter 是做token过滤的，它接收token流，输出也是token流 由此可见，整个analyzer要做的事情就是将文本拆分成单个单词，文本 ----&gt; 字符 ----&gt; token 分析器的任务是分析（Analyze）文本数据，分析是分词，规范化文本的意思 ES在创建索引时, 默认创建5个分片(shard), 每个分片有一份备份/复制分片(replica shard), 可以修改, 分片的数量只能在创建索引的时候指定, 索引创建后就不能修改分片的数量了, 而备份是可以动态修改的 反向索引又叫倒排索引，是根据文章内容中的关键字建立索引 Keyword 类型是不会分词的，直接根据字符串内容建立反向索引，Text 类型在存入 Elasticsearch 的时候，会先分词（指定分词器会按指定分词器分词，未指定按默认分词器分词）），然后根据分词后的内容建立反向索引 java REST api是通过http访问，走9200端口（java api是9300端口）。 虽然es带有java api，但是会引起版本兼容性的问题，以及微弱到可以忽略的性能提升，并且java api在未来的es版本会放弃，官方推荐使用java REST api 每个elasticsaerch分片都是一个Lucene 索引。在单个索引中你最多可以存储2147483519 (= Integer.MAX_VALUE - 128) 个文档。你可以使用 _cat/shards api去监控分片的的大小。 Es配置文件 vim elasticsearch.yml #集群名字，es启动后会将具有相同集群名字的节点放到一个集群下。 cluster.name: elasticsearch #节点名字 node.name: &quot;es-node1&quot; #指定集群中的节点中有几个有master资格的节点。 #对于大集群可以写3个以上。 discovery.zen.minimum_master_nodes: 2 #设置集群中自动发现其它节点时ping连接超时时间，默认是3s， #为避免因为网络差而导致启动报错，设成了40s。 discovery.zen.ping.timeout: 40s #设置是否打开多播发现节点，默认是true discovery.zen.ping.multicast.enabled: false #ip地址 network.host: 192.168.137.100 #指明集群中其它可能为master的节点ip,以防es启动后发现不了集群中的其他节点。 discovery.zen.ping.unicast.hosts:[&quot;节点1的 ip&quot;,&quot;节点2 的ip&quot;,&quot;节点3的ip&quot;] 一般测试时，只需要改一下cluster.name、node.name、network.host即可，使用默认也可以 Es启动: 启动:# bin/elasticsearch 发生以下错误:Caused by: java.lang.RuntimeException: can not run elasticsearch as root(不能用root用户启动) 原因:root用户权限过大 解决方案： useradd es (添加一个es用户) passwd es (设置用户es密码) chown -R es:es * (给es用户权限) 切换到上面新添加的es用户再次执行启动命令:此时可能会出现3个错误 ERROR: [3] bootstrap checks failed [1]: max file descriptors [4096] for elasticsearch process is too low, increase to at least [65536] 解决(切换到root用户)： vim /etc/security/limits.conf # 添加以下内容(*也是,代表所有用户):增大内存和硬盘 * soft nofile 65536 * hard nofile 131072 * soft nproc 2048 * hard nproc 4096 [2]:max number of threads [3802] for user [es] is too low, increase to at least [4096] 解决: vim /etc/security/limits.d/90-nproc.conf(用户最大线程数) 修改以下内容 * soft nproc 4096(原先是1024) [3]:max virtual memory areas vm.max_map_count [65530] is too low, increase to at least [262144] 解决: vim /etc/sysctl.conf(配置最大线程数) # 添加以下内容 vm.max_map_count=655360 再次切换到es用户执行启动命令成功启动 Kibana工具 当es启动后，我们在命令行直接敲es命令是比较麻烦的，因此此时用到kibana插件，kibana是es的一个可视化视图工具，当然es也有其他的插件，比如elasticsearch-head，ElasticHD等等 kibana的安装 kibana下载地址：https://www.elastic.co/cn/downloads/kibana，注意：下载的版本要和es使用的版本要对应 kibana的配置文件 vim kibana.yml #对外服务监听端口 server.port: 5601 #绑定可以访问5601端口服务的IP地址，0.0.0.0表示任何地址在没有防火墙限制的情况下都可以访问，生产环境别这样设置，不安全。 server.host: &quot;0.0.0.0&quot; #默认值为主机名称，表示kibana实例绑定的主机，可以是IP地址或者主机名称. server.name: &quot;192.168.1.11&quot; #用来处理ES请求的服务URL elasticsearch.hosts: [&quot;http://192.168.1.11:9200&quot;,&quot;http://192.168.1.12:9200&quot;] #用来控制证书的认证，可选的值为full，none，certificate。此处由于没有证书，所以设置为null，否则启动会提示错误. elasticsearch.ssl.verificationMode: none #kibana搜索数据请求超时时间 elasticsearch.requestTimeout: 90000 更多配置信息查看：https://www.elastic.co/guide/en/kibana/5.6/settings.html 启动命令:kibana文件夹bin/kibana Es集群相关操作 查看集群健康信息 GET /_cat/health? 查看集群中节点信息 GET /_cat/nodes?v 查看集群中的索引信息 GET /_cat/indices?v 可以看到我们集群叫“elasticsearch”，运行状态是green。每当我们查询集群健康情况时，接口可能会返回green，yellow或red状态。green意味着一切良好（集群所有的功能都正常）。 yellow意味着所有的数据都是可用的，但是一些复制分片可能没有正确分发（集群的所有功能还是正常的）。red意味着因为某些原因导致有些数据不能使用。 注意，即使集群状态是red，它仍然可以运行一部分的功能。（例如，它依然可以从一些可用的分片处理搜索请求）但你应该尽快去修复它，因为这样会使搜索结果丢失一些数据 索引相关操作 简单操作: 创建: put/索引名 删除:delete/索引名 put、get、post、delete操作 添加数据 put 索引名/类型名/文档(id) {json数据} 不指定类型(id)时,会随机生成id值,但只能用post post 索引名/类型名 {json数据} 获取数据 get 索引名/类型名/文档(id) 测试文档是否存在 HEAD 索引名/类型名/文档(id) 200 - OK 200:该文档存在 404 - Not Found 404:该文档不存在 批量获取 主要是Get (索引名/类型名)_mget {json数据} 具体分类看保存的图片 批处理操作 POST /索引名/类型/_bulk # 批量插入多个document {&quot;index&quot;:{}} {&quot;name&quot;:&quot;ww&quot;,&quot;title&quot;:&quot;王五&quot;,&quot;age&quot;:18,&quot;created&quot;:&quot;2018-12-27&quot;} {&quot;index&quot;:{}} {&quot;name&quot;:&quot;zl&quot;,&quot;title&quot;:&quot;赵六&quot;,&quot;age&quot;:25,&quot;created&quot;:&quot;2018-12-27&quot;} POST /索引名/类型名/_bulk # 批量操作（包含修改和删除） {&quot;update&quot;:{&quot;_id&quot;:&quot;KrOP6WcBVEuCC3JS8V9K&quot;}} # 修改 {&quot;doc&quot;:{&quot;title&quot;:&quot;王小五&quot;}} {&quot;delete&quot;:{&quot;_id&quot;:&quot;K7OP6WcBVEuCC3JS8V9K&quot;}} # 删除 es更新文档的原理为：先找到这个文档，删除旧的文档内容执行更新，更新完后再索引最新的文档 过滤器 注意： 过滤查询运行时先执行过滤语句，后执行普通查询 过滤器的类型 1. term 、 terms Filter term、terms的含义与查询时一致。term用于精确匹配、terms用于多词条匹配 2. ranage filter 3. exists filter 4. ids filter Query和Filter更详细的对比可参考：https://blog.csdn.net/laoyang360/article/details/80468757 Mapping Type： 1. 简单类型： text , keyword , date , long , double , boolean or ip 2. 其它类型： object , geo_point , geo_shape 等 查看类型mapping GET /索引名/_mapping/类型名 GET 索引名/_mapping?pretty 查看某个字段的分词结果 GET your_index/your_type/your_id/_termvectors?fields=your_fieldsName IK分词器 elasticSearch默认的分词器对中文不是很友好，会将中文单个字的建立索引，因此可以使用ik分词器 ik分词器下载地址：https://github.com/medcl/elasticsearch-analysis-ik/releases 同样，版本尽量和Es相对应。 下载完以后在Es的plugins文件夹中创建一个名为IK的文件夹，将下载的ik分词器解压到该文件夹下， 重新启动Es,出现plugin [analysis-ik]说明ik分词器被加载。 Ik分词器的两种方式: 智能模式和细粒度模式（智能：对应es的IK插件的ik_smart，细粒度：对应es的IK插件的ik_max_word） 细粒度分词，包含每一种切分可能(更全)；而智能模式，只包含各种切分路径中最可能的一种。 ","link":"https://mask0407.github.io/elasticsearch00/"},{"title":"Storm 运行jar出错:org.apache.storm.thrift.TApplicationException: getLeader failed: unknownre result","content":"编写完storm的java代码准备在集群环境运行测试，结果报错： 起初以为是代码的错误，结果检查半天也没错误 后来去UIServer看集群状态，发现有节点宕掉了。 最后：重启机群，重新执行 storm jar storm-lowlevel-1.0-SNAPSHOT.jar com.msk.demo01.WordCountTopolpgy 错误解决 如果重启集群出错，先检查zookeeper集群状态，确保zookeeper集群可用 ","link":"https://mask0407.github.io/storm00/"},{"title":"一篇文章读懂Kafka消息队列","content":" 消息队列的概念 使用消息队列的场景分析 异步消息发送： 系统间解耦合 Kafka 架构 Kafka集群安装 准备工作 安装Zookeeper集群确保Kafka集群的正常运行 Kafka安装步骤 启动服务 测试 Topic 和 日志 生产者 消费者 Topic管理篇（DDL） 创建Tocpic Topic详细信息 删除Topic Topic列表 Kafka API实战(JDK1.8+) 快速入门 Maven依赖 引入log4j.properies 在Windos配置主机名和IP映射关系 生产者 消费者 读取数据偏移量控制 指定消费分区 Kafka发送/接收Object 生产者幂等性 生产者批量发送 生产者事务 只有生产者 生产者&amp;消费者 SpringBoot整合Kafka (Kafka 基础篇) 消息队列的概念 可以用于系统间通讯的一个组件-middle ware（中间件），该组件可以用于做消息缓冲的中间件（持久化）解决一些 并发处理、数据库缓冲等实现对高并发的业务场景的削峰填谷。 使用消息队列的场景分析 异步消息发送： 使用Kafka MQ功能实现模块间异步通信，把一些费时的操作交给额外的服务或者设备去执行，这样可以提升系统运行效率，加速连接释放的速度，例如：用户注册模块，在用户注册成功后，业务系统需要给用户发送一个通知短信，通知用户登录邮箱去激活刚注册的用户信息。这种业务场景如图所示，因为短信通知和邮件发送是一个比较耗时的操作，所以在这里没必要将短信和邮件发送作为注册模块的流程，使用Message Queue功能可以将改业务和主业务注册分离，这样可以缩短用户浏览器和服务建立的链接时间，同时也能满足发送短信和邮件的业务。 系统间解耦合 ①在某些高吞吐的业务场景下，可能会出现在某一个时间段系统负载写入的负载压力比较大，短时间有大量的数据需要持久化到数据库中，但是由于数据的持久化需要数据库提供服务，由于传统的数据库甚至一些NoSQL产品也不能很好的解决高并发写入，因为数据库除去要向用户提供链接之外，还需要对新来的数据做持久化，这就需要一定的时间才能将数据落地到磁盘。因此在高并发写入的场景，就需要用户集成Message Queue在数据库前作为缓冲队列。在队列的另一头只需要程序有条不紊的将数据写入到数据库即可，这就保证无论外界写入压力有多么大都可以借助于Message Queue缓解数据库的压力。 ②Message Queue除了解决对数据缓冲的压力之外，还可以充当业务系统的中间件（Middleware）作为系统服务间解耦的组件存在，例如上图所示订单模块和库存模块中就可以使用Message Queue作为缓冲队列实现业务系统服务间的解耦，也就意味着即使服务在运行期间库存系统宕机也并不会影响订单系统的正常运行。 Kafka 架构 Kafka集群以Topic形式负责管理集群中的Record，每一个Record属于一个Topic。底层Kafka集群通过日志分区形式持久化Record。在Kafka集群中，Topic的每一个分区都一定会有1个Borker担当该分区的Leader，其他的Broker担当该分区的follower（取决于分区的副本因子）。一旦对应分区的Lead宕机，kafka集群会给当前的分区指定新的Borker作为该分区的Leader。分区的Leader的选举是通过Zookeeper一些特性实现的，这里就不在概述了。Leader负责对应分区的读写操作，Follower负责数据备份操作。 Kafka集群安装 准备工作 准备三台主机名分别为CentOSA|CentOSB|CentOSC的Linux系统主机 分别关闭防火墙、相互做主机名映射、校对物理时钟、安装配置JDK8 安装Zookeeper集群确保Kafka集群的正常运行 tar -zxf zookeeper-3.4.6.tar.gz -C /usr/ mkdir /root/zkdata #分别在三台机器执行以下命令 echo 1 &gt;&gt; /root/zkdata/myid echo 2 &gt;&gt; /root/zkdata/myid echo 3 &gt;&gt; /root/zkdata/myid touch /usr/zookeeper-3.4.6/conf/zoo.cfg vim /usr/zookeeper-3.4.6/conf/zoo.cfg zoo.cfg tickTime=2000 dataDir=/root/zkdata clientPort=2181 initLimit=5 syncLimit=2 server.1=CentOSA:2887:3887 server.2=CentOSB:2887:3887 server.3=CentOSC:2887:3887 启动zookeeper|查看zookeeper当前状态 /usr/zookeeper-3.4.6/bin/zkServer.sh start zoo.cfg /usr/zookeeper-3.4.6/bin/zkServer.sh status zoo.cfg Kafka安装步骤 下载Kafka服务安装包http://archive.apache.org/dist/kafka/2.2.0/kafka_2.11-2.2.0.tgz tar -zxf kafka_2.11-2.2.0.tgz -C /usr vim /usr/kafka_2.11-2.2.0/config/server.properties ############################# Server Basics ############################# broker.id=[0|1|2] #三台机器分别 0/1/2 ############################# Socket Server Settings ############################# listeners=PLAINTEXT://CentOS[A|B|C]:9092 #三台机器分别A、B、C ############################# Log Basics ############################# # A comma separated list of directories under which to store log files log.dirs=/usr/kafka-logs ############################# Zookeeper ############################# zookeeper.connect=CentOSA:2181,CentOSB:2181,CentOSC:2181 注：此配置只能使用主机名访问 如需IP访问 将listeners=PLAINTEXT://CentOS[A|B|C]:9092 #三台机器分别A、B、C 改为 advertised.listeners=PLAINTEXT://x.x.x.x:9092 启动服务 cd /usr/kafka_2.11-2.2.0/ ./bin/kafka-server-start.sh -daemon config/server.properties 测试 创建topic ./bin/kafka-topics.sh --zookeeper CentOSA:2181,CentOSB:2181,CentOSC:2181 --create --topic topic01 --partitions 3 --replication-factor 3 消费者 ./bin/kafka-console-consumer.sh --bootstrap-server CentOSA:9092,CentOSB:9092,CentOSC:9092 --topic topic01 生产者 ./bin/kafka-console-producer.sh --broker-list CentOSA:9092,CentOSB:9092,CentOSC:9092 --topic topic01 Topic 和 日志 Kafka集群是通过日志形式存储Topic中的Record，Record会根据分区策略计算得到的分区数存储到相应分区的文件中。每个分区都是一个有序的，不可变的记录序列，不断附加到结构化的commit-log中。每个分区文件会为Record进去分区的顺序进行编排。每一个分区中的Record都有一个id，该id标示了该record进入分区的先后顺序，通常将该id称为record在分区中的offset偏移量从0开始，依次递增。 Kafka集群持久地保留所有已发布的记录 - 无论它们是否已被消耗 - 使用可配置的保留时间。例如，如果保留策略设置为2天，则在发布记录后的2天内，它可供使用，之后将被丢弃以释放空间。Kafka的性能在数据大小方面实际上是恒定的，因此长时间存储数据不是问题。 事实上，基于每个消费者保留的唯一元数据是该消费者在日志中的偏移或位置。这种offset由消费者控制：通常消费者在读取记录时会线性地增加其偏移量，但事实上，由于消费者控制位置，它可以按照自己喜欢的任何顺序消费记录。例如，消费者可以重置为较旧的偏移量以重新处理过去的数据，或者跳到最近的记录并从“现在”开始消费。 生产者 生产者负责发送Record到Kafka集群中的Topic中。在发布消息的时候，首先先计算Record分区计算方案有三种： ①如果用户没有指定分区但是指定了key信息，生产者会根据hash（key）%分区数计算该Record所属分区信息。 ②如果生产者在发送消息的时候并没有key，也没有指定分区数，生产者会使用轮训策略选择分区信息。 ③如果指定了分区信息，就按照指定的分区信息选择对应的分区；当分区参数确定以后生产者会找到相应分区的Leader节点将Record记录写入到Topic日志存储分区中。 消费者 消费者作为消息的消费放，消费者对Topic中消息的消费方式是以Group为单位进行消费，Kafka服务器会自动的按照组内和组间对消费者消费的分区进行协调。 组内均分分区，确保一个组内的消费者不可重复消费分区中的数据，一般来说一个组内的消费者实例对的数目应该小于或者等于分区数目。 组间广播形式消费，确保所有组都可以拿到当前Record。组间数据之间可以保证对数据的独立消费。 Topic管理篇（DDL） 创建Tocpic ./bin/kafka-topics.sh --zookeeper CentOSA:2181,CentOSB:2181,CentOSC:2181 --create --topic topic01 --partitions 3 --replication-factor 3 Topic详细信息 ./bin/kafka-topics.sh --describe --zookeeper CentOSA:2181,CentOSB:2181,CentOSC:2181 --topic topic01 删除Topic ./bin/kafka-topics.sh --zookeeper CentOSA:2181,CentOSB:2181,CentOSC:2181 --delete --topic topic01 如果用户没有配置delete.topic.enable=true，则Topic删除不起作用。 Topic列表 ./bin/kafka-topics.sh --zookeeper CentOSA:2181,CentOSB:2181,CentOSC:2181 --list Kafka API实战(JDK1.8+) 快速入门 Maven依赖 &lt;dependency&gt; &lt;groupId&gt;org.apache.kafka&lt;/groupId&gt; &lt;artifactId&gt;kafka-clients&lt;/artifactId&gt; &lt;version&gt;2.2.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.slf4j&lt;/groupId&gt; &lt;artifactId&gt;slf4j-api&lt;/artifactId&gt; &lt;version&gt;1.7.25&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;log4j&lt;/groupId&gt; &lt;artifactId&gt;log4j&lt;/artifactId&gt; &lt;version&gt;1.2.17&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.slf4j&lt;/groupId&gt; &lt;artifactId&gt;slf4j-log4j12&lt;/artifactId&gt; &lt;version&gt;1.7.5&lt;/version&gt; &lt;/dependency&gt; 引入log4j.properies ### set log levels ### log4j.rootLogger = info,stdout ### 输出到控制台 ### log4j.appender.stdout = org.apache.log4j.ConsoleAppender log4j.appender.stdout.Target = System.out log4j.appender.stdout.layout = org.apache.log4j.PatternLayout log4j.appender.stdout.layout.ConversionPattern =%p %d %c %m %n 在Windos配置主机名和IP映射关系 192.168.111.128 CentOSA 192.168.111.129 CentOSB 192.168.111.130 CentOSC 生产者 package com.msk.demo01; import org.apache.kafka.clients.producer.KafkaProducer; import org.apache.kafka.clients.producer.ProducerConfig; import org.apache.kafka.clients.producer.ProducerRecord; import java.text.DecimalFormat; import java.util.Properties; public class KafkaProducerDemo { public static void main(String[] args) { //1.配置生产者了连接属性 Properties props = new Properties(); props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG,&quot;CentOSA:9092,CentOSB:9092,CentOSC:9092&quot;); props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG,&quot;org.apache.kafka.common.serialization.StringSerializer&quot;); props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG,&quot;org.apache.kafka.common.serialization.StringSerializer&quot;); //2.创建Kafka生产者 KafkaProducer&lt;String, String&gt; producer = new KafkaProducer&lt;String, String&gt;(props); //3.构建ProducerRecord for (int i=0;i&lt;10;i++){ DecimalFormat decimalFormat = new DecimalFormat(&quot;000&quot;); ProducerRecord&lt;String, String&gt; record = new ProducerRecord&lt;String, String&gt;(&quot;topic04&quot;, decimalFormat.format(i), &quot;value&quot; + i); //4.发送消息 producer.send(record); } //5.清空缓冲区 producer.flush(); //6.关闭生产者 producer.close(); } } 消费者 package com.msk.demo01; import org.apache.kafka.clients.consumer.ConsumerConfig; import org.apache.kafka.clients.consumer.ConsumerRecord; import org.apache.kafka.clients.consumer.ConsumerRecords; import org.apache.kafka.clients.consumer.KafkaConsumer; import java.time.Duration; import java.util.Arrays; import java.util.Properties; public class KafkaConsumerDemo { public static void main(String[] args) { //1.配置生产者了连接属性 Properties props = new Properties(); props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG,&quot;CentOSA:9092,CentOSB:9092,CentOSC:9092&quot;); props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, &quot;org.apache.kafka.common.serialization.StringDeserializer&quot;); props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG,&quot;org.apache.kafka.common.serialization.StringDeserializer&quot;); props.put(ConsumerConfig.GROUP_ID_CONFIG,&quot;group1&quot;); //2.创建Kafka消费者 KafkaConsumer&lt;String, String&gt; consumer = new KafkaConsumer&lt;String, String&gt;(props); //3.订阅topics consumer.subscribe(Arrays.asList(&quot;topic01&quot;)); //4.死循环读取消息 while(true){ ConsumerRecords&lt;String, String&gt; records = consumer.poll(Duration.ofSeconds(1)); if(records!=null &amp;&amp; !records.isEmpty()){ for (ConsumerRecord&lt;String, String&gt; record : records) { int partition = record.partition(); long offset = record.offset(); long timestamp = record.timestamp(); String key = record.key(); String value = record.value(); System.out.println(partition+&quot;\\t&quot;+offset+&quot;\\t&quot;+timestamp+&quot;\\t&quot;+key+&quot;\\t&quot;+value); } } } } } 读取数据偏移量控制 默认当用户使用subscribe方式订阅topic消息， 默认首次offset策略是latest。当用户第一次订阅topic在消费者订阅之前的数据是无法消费到 消息的。用户可以配置消费端参数auto.offset.reset控制kafka消费者行为。 Properties props = new Properties(); props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG,&quot;CentOSA:9092,CentOSB:9092,CentOSC:9092&quot;); props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, &quot;org.apache.kafka.common.serialization.StringDeserializer&quot;); props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG,&quot;org.apache.kafka.common.serialization.StringDeserializer&quot;); props.put(ConsumerConfig.GROUP_ID_CONFIG,&quot;group1&quot;); props.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG,&quot;earliest&quot;);//默认值 latest 因为消费端在使用consumer.poll数据的时候，底层会定时的向Kafka服务器提交消费的偏移量。默认消费端的offset是自动提交的，用户如果不希望自动提交偏移量可以配置如下参数 注意如果用户使用subscribe方式订阅topic，在消费端必须指定group.id，这样Kafka才能够实现消费&gt;端负载均衡以及实现组内均分组件广播。（推荐方式） 默认配置 enable.auto.commit = true auto.commit.interval.ms = 5000 props.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG,&quot;false&quot;); 手动提交偏移量 public class KafkaConsumerDemo { public static void main(String[] args) { //1.配置生产者了连接属性 Properties props = new Properties(); props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG,&quot;CentOSA:9092,CentOSB:9092,CentOSC:9092&quot;); props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, &quot;org.apache.kafka.common.serialization.StringDeserializer&quot;); props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG,&quot;org.apache.kafka.common.serialization.StringDeserializer&quot;); props.put(ConsumerConfig.GROUP_ID_CONFIG,&quot;group1&quot;); props.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG,false); //2.创建Kafka消费者 KafkaConsumer&lt;String, String&gt; consumer = new KafkaConsumer&lt;String, String&gt;(props); //3.订阅topics consumer.subscribe(Arrays.asList(&quot;topic01&quot;)); //4.死循环读取消息 while(true){ ConsumerRecords&lt;String, String&gt; records = consumer.poll(Duration.ofSeconds(1)); if(records!=null &amp;&amp; !records.isEmpty()){ Map&lt;TopicPartition, OffsetAndMetadata&gt; offsetMeta=new HashMap&lt;&gt;(); for (ConsumerRecord&lt;String, String&gt; record : records) { int partition = record.partition(); long offset = record.offset(); long timestamp = record.timestamp(); String key = record.key(); String value = record.value(); System.out.println(partition+&quot;\\t&quot;+offset+&quot;\\t&quot;+timestamp+&quot;\\t&quot;+key+&quot;\\t&quot;+value); TopicPartition part = new TopicPartition(&quot;topic03&quot;, partition); OffsetAndMetadata oam=new OffsetAndMetadata(offset+1);//设置下一次读取起始位置 offsetMeta.put(part,oam); } consumer.commitSync(offsetMeta); } } } } 指定消费分区 通过assign方式kafka对消费者的组管理策略失效。也就是说用户可以无需配置组ID。 public class KafkaConsumerDemo { public static void main(String[] args) { //1.配置生产者了连接属性 Properties props = new Properties(); props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG,&quot;CentOSA:9092,CentOSB:9092,CentOSC:9092&quot;); props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, &quot;org.apache.kafka.common.serialization.StringDeserializer&quot;); props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG,&quot;org.apache.kafka.common.serialization.StringDeserializer&quot;); //2.创建Kafka消费者 KafkaConsumer&lt;String, String&gt; consumer = new KafkaConsumer&lt;String, String&gt;(props); //3.指定分区 consumer.assign(Arrays.asList(new TopicPartition(&quot;topic01&quot;,1))); consumer.seek(new TopicPartition(&quot;topic01&quot;,1),1); //4.死循环读取消息 while(true){ ConsumerRecords&lt;String, String&gt; records = consumer.poll(Duration.ofSeconds(1)); if(records!=null &amp;&amp; !records.isEmpty()){ for (ConsumerRecord&lt;String, String&gt; record : records) { int partition = record.partition(); long offset = record.offset(); long timestamp = record.timestamp(); String key = record.key(); String value = record.value(); System.out.println(partition+&quot;\\t&quot;+offset+&quot;\\t&quot;+timestamp+&quot;\\t&quot;+key+&quot;\\t&quot;+value); } } } } } Kafka发送/接收Object 生产Object public interface Serializer&lt;T&gt; extends Closeable { void configure(Map&lt;String, ?&gt; configs, boolean isKey); //重点实现serialize byte[] serialize(String topic, T data); default byte[] serialize(String topic, Headers headers, T data) { return serialize(topic, data); } @Override void close(); } 消费Object public interface Deserializer&lt;T&gt; extends Closeable { void configure(Map&lt;String, ?&gt; configs, boolean isKey); //重点实现方法 T deserialize(String topic, byte[] data); default T deserialize(String topic, Headers headers, byte[] data) { return deserialize(topic, data); } @Override void close(); } 实现序列化和反序列化 public class ObjectCodec implements Deserializer&lt;Object&gt;, Serializer&lt;Object&gt; { @Override public void configure(Map&lt;String, ?&gt; configs, boolean isKey) { } @Override public byte[] serialize(String topic, Object data) { return SerializationUtils.serialize((Serializable) data); } @Override public Object deserialize(String topic, byte[] data) { return SerializationUtils.deserialize(data); } @Override public void close() { } } 生产者幂等性 幂等:多次操作最终的影响等价与一次操作称为幂等性操作,所有的读操作一定是幂等的.所有的写操作一定不是幂等的.当 生产者和broker默认有acks应答机制,如果当生产者发送完数据给broker之后如果没有在规定的时间内收到应答,生产者可以考虑重发数据.可以通过一下配置参数提升生产者的可靠性. acks = all // 0 无需应答 n 应答个数 -1所有都需要 retries = 3 // 表示重试次数 request.timeout.ms = 3000 //等待应答超时时间 enable.idempotence = true //开启幂等性 public class KafkaProducerDemo { public static void main(String[] args) { //1.配置生产者了连接属性 Properties props = new Properties(); props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG,&quot;CentOSA:9092,CentOSB:9092,CentOSC:9092&quot;); props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG,&quot;org.apache.kafka.common.serialization.StringSerializer&quot;); props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG,&quot;org.apache.kafka.common.serialization.StringSerializer&quot;); props.put(ProducerConfig.ACKS_CONFIG,&quot;all&quot;);//等待所有从机应答 props.put(ProducerConfig.RETRIES_CONFIG,3);//重试3次 props.put(ProducerConfig.REQUEST_TIMEOUT_MS_CONFIG,3000);//等待3s应答 props.put(ProducerConfig.ENABLE_IDEMPOTENCE_CONFIG,true);//开启幂等性 //2.创建Kafka生产者 KafkaProducer&lt;String, String&gt; producer = new KafkaProducer&lt;String, String&gt;(props); //3.构建ProducerRecord for (int i=15;i&lt;20;i++){ DecimalFormat decimalFormat = new DecimalFormat(&quot;000&quot;); User user = new User(i, &quot;name&quot; + i, i % 2 == 0); ProducerRecord&lt;String, String&gt; record = new ProducerRecord&lt;String, String&gt;(&quot;topic01&quot;, decimalFormat.format(i), &quot;user&quot;+i); //4.发送消息 producer.send(record); } //5.清空缓冲区 producer.flush(); //6.关闭生产者 producer.close(); } 生产者批量发送 生产者会尝试缓冲record，实现批量发送，通过一下配置控制发送时机，记住如果开启可batch，一定在关闭producer之前需要flush。 batch.size = 16384 //16KB 缓冲16kb数据本地 linger.ms = 2000 //默认逗留时间 public static void main(String[] args) { //1.配置生产者了连接属性 Properties props = new Properties(); props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG,&quot;CentOSA:9092,CentOSB:9092,CentOSC:9092&quot;); props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG,&quot;org.apache.kafka.common.serialization.StringSerializer&quot;); props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG,&quot;org.apache.kafka.common.serialization.StringSerializer&quot;); props.put(ProducerConfig.ACKS_CONFIG,&quot;all&quot;); props.put(ProducerConfig.RETRIES_CONFIG,3); props.put(ProducerConfig.REQUEST_TIMEOUT_MS_CONFIG,3000); props.put(ProducerConfig.ENABLE_IDEMPOTENCE_CONFIG,true); props.put(ProducerConfig.BATCH_SIZE_CONFIG,1024);//1kb缓冲区 props.put(ProducerConfig.LINGER_MS_CONFIG,1000);//设置逗留时常 //2.创建Kafka生产者 KafkaProducer&lt;String, String&gt; producer = new KafkaProducer&lt;String, String&gt;(props); //3.构建ProducerRecord for (int i=15;i&lt;20;i++){ DecimalFormat decimalFormat = new DecimalFormat(&quot;000&quot;); User user = new User(i, &quot;name&quot; + i, i % 2 == 0); ProducerRecord&lt;String, String&gt; record = new ProducerRecord&lt;String, String&gt;(&quot;topic01&quot;, decimalFormat.format(i), &quot;user&quot;+i); //4.发送消息 producer.send(record); } //5.清空缓冲区 producer.flush(); //6.关闭生产者 producer.close(); } 生产者事务 kafka生产者事务指的是在发送多个数据的时候，保证多个Record记录发送的原子性。如果有一条发送失败就回退，但是需要注意在使用kafka事务的时候需要调整消费者的事务隔离级别设置为read_committed因为kafka默认的事务隔离策略是read_uncommitted 开启事务 transactional.id=transaction-1 //必须保证唯一 enable.idempotence=true //开启kafka的幂等性 只有生产者 public class KafkaProducerDemo { public static void main(String[] args) { //1.创建Kafka生产者 KafkaProducer&lt;String, String&gt; producer = buildKafkaProducer(); //2.初始化事务和开启事务 producer.initTransactions(); producer.beginTransaction(); try { for (int i=5;i&lt;10;i++){ DecimalFormat decimalFormat = new DecimalFormat(&quot;000&quot;); User user = new User(i, &quot;name&quot; + i, i % 2 == 0); ProducerRecord&lt;String, String&gt; record = new ProducerRecord&lt;String, String&gt;(&quot;topic07&quot;, decimalFormat.format(i), &quot;user&quot;+i); producer.send(record); } producer.flush(); //3.提交事务] producer.commitTransaction(); } catch (Exception e) { System.err.println(e.getMessage()); //终止事务 producer.abortTransaction(); } //5.关闭生产者 producer.close(); } private static KafkaProducer&lt;String, String&gt; buildKafkaProducer() { //0.配置生产者了连接属性 Properties props = new Properties(); props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG,&quot;CentOSA:9092,CentOSB:9092,CentOSC:9092&quot;); props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG,&quot;org.apache.kafka.common.serialization.StringSerializer&quot;); props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG,&quot;org.apache.kafka.common.serialization.StringSerializer&quot;); props.put(ProducerConfig.ACKS_CONFIG,&quot;all&quot;); props.put(ProducerConfig.RETRIES_CONFIG,3); props.put(ProducerConfig.REQUEST_TIMEOUT_MS_CONFIG,3000); props.put(ProducerConfig.ENABLE_IDEMPOTENCE_CONFIG,true); props.put(ProducerConfig.BATCH_SIZE_CONFIG,1024);//1kb缓冲区 props.put(ProducerConfig.LINGER_MS_CONFIG,1000);//设置逗留时常 //开启事务 props.put(ProducerConfig.TRANSACTIONAL_ID_CONFIG,&quot;transaction-&quot;+UUID.randomUUID().toString()); return new KafkaProducer&lt;String, String&gt;(props); } } 消费者那方需要将事务隔离级别设置为`read_committed public class KafkaConsumerDemo { public static void main(String[] args) { //1.创建Kafka消费者 KafkaConsumer&lt;String, String&gt; consumer = buildKafkaConsumer(); //2.订阅topics consumer.subscribe(Arrays.asList(&quot;topic07&quot;)); //3.死循环读取消息 while(true){ ConsumerRecords&lt;String, String&gt; records = consumer.poll(Duration.ofSeconds(1)); if(records!=null &amp;&amp; !records.isEmpty()){ for (ConsumerRecord&lt;String, String&gt; record : records) { int partition = record.partition(); long offset = record.offset(); long timestamp = record.timestamp(); String key = record.key(); String value = record.value(); System.out.println(partition+&quot;\\t&quot;+offset+&quot;\\t&quot;+timestamp+&quot;\\t&quot;+key+&quot;\\t&quot;+value); } } } } private static KafkaConsumer&lt;String, String&gt; buildKafkaConsumer() { Properties props = new Properties(); props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG,&quot;CentOSA:9092,CentOSB:9092,CentOSC:9092&quot;); props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, &quot;org.apache.kafka.common.serialization.StringDeserializer&quot;); props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG,&quot;org.apache.kafka.common.serialization.StringDeserializer&quot;); props.put(ConsumerConfig.GROUP_ID_CONFIG,&quot;group1&quot;); props.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG,&quot;earliest&quot;); props.put(ConsumerConfig.ISOLATION_LEVEL_CONFIG,&quot;read_committed&quot;); return new KafkaConsumer&lt;String, String&gt;(props); } } 生产者&amp;消费者 package com.msk.demo08; import org.apache.kafka.clients.consumer.ConsumerConfig; import org.apache.kafka.clients.consumer.KafkaConsumer; import org.apache.kafka.clients.producer.KafkaProducer; import org.apache.kafka.clients.producer.ProducerConfig; import org.apache.kafka.common.serialization.Deserializer; import org.apache.kafka.common.serialization.Serializer; import java.util.Properties; import java.util.UUID; public class KafkaUtils { public static KafkaConsumer&lt;String, String&gt; buildKafkaConsumer(String servers, Class&lt;? extends Deserializer&gt; keyDeserializer, Class&lt;? extends Deserializer&gt; valueDeserializer,String group) { Properties props = new Properties(); props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG,servers); props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG,keyDeserializer); props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG,valueDeserializer); props.put(ConsumerConfig.GROUP_ID_CONFIG,group); props.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG,&quot;earliest&quot;); props.put(ConsumerConfig.ISOLATION_LEVEL_CONFIG,&quot;read_committed&quot;); props.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG,false);//设置为手动提交 return new KafkaConsumer&lt;String, String&gt;(props); } public static KafkaProducer&lt;String, String&gt; buildKafkaProducer(String servers, Class&lt;? extends Serializer&gt; keySerializer, Class&lt;? extends Serializer&gt; valueSerializer) { //1.配置生产者了连接属性 Properties props = new Properties(); props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG,servers); props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG,keySerializer); props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG,valueSerializer); props.put(ProducerConfig.ACKS_CONFIG,&quot;all&quot;); props.put(ProducerConfig.RETRIES_CONFIG,3); props.put(ProducerConfig.REQUEST_TIMEOUT_MS_CONFIG,3000); props.put(ProducerConfig.ENABLE_IDEMPOTENCE_CONFIG,true); props.put(ProducerConfig.BATCH_SIZE_CONFIG,1024);//1kb缓冲区 props.put(ProducerConfig.LINGER_MS_CONFIG,1000);//设置逗留时常 //开启事务 props.put(ProducerConfig.TRANSACTIONAL_ID_CONFIG,&quot;transaction-&quot;+ UUID.randomUUID().toString()); return new KafkaProducer&lt;String, String&gt;(props); } } KafkaProducerAndConsumer package com.msk.demo08; import com.msk.demo05.User; import org.apache.kafka.clients.consumer.ConsumerRecord; import org.apache.kafka.clients.consumer.ConsumerRecords; import org.apache.kafka.clients.consumer.KafkaConsumer; import org.apache.kafka.clients.consumer.OffsetAndMetadata; import org.apache.kafka.clients.producer.KafkaProducer; import org.apache.kafka.clients.producer.ProducerConfig; import org.apache.kafka.clients.producer.ProducerRecord; import org.apache.kafka.common.TopicPartition; import org.apache.kafka.common.serialization.StringDeserializer; import org.apache.kafka.common.serialization.StringSerializer; import java.text.DecimalFormat; import java.time.Duration; import java.util.*; public class KafkaProducerAndConsumer { public static void main(String[] args) { String servers = &quot;CentOSA:9092,CentOSB:9092,CentOSC:9092&quot;; String group=&quot;g1&quot;; //1.创建Kafka生产者 KafkaProducer&lt;String, String&gt; producer = KafkaUtils.buildKafkaProducer(servers, StringSerializer.class, StringSerializer.class); KafkaConsumer&lt;String, String&gt; consumer = KafkaUtils.buildKafkaConsumer(servers, StringDeserializer.class, StringDeserializer.class,group); consumer.subscribe(Arrays.asList(&quot;topic08&quot;)); //初始化事务 producer.initTransactions(); while (true) { producer.beginTransaction(); ConsumerRecords&lt;String, String&gt; records = consumer.poll(Duration.ofSeconds(1)); try { Map&lt;TopicPartition, OffsetAndMetadata&gt; commits = new HashMap&lt;TopicPartition, OffsetAndMetadata&gt;(); for (ConsumerRecord&lt;String, String&gt; record : records) { TopicPartition partition = new TopicPartition(record.topic(), record.partition()); OffsetAndMetadata offsetAndMetadata = new OffsetAndMetadata(record.offset() + 1); commits.put(partition, offsetAndMetadata); System.out.println(record); ProducerRecord&lt;String, String&gt; srecord = new ProducerRecord&lt;String, String&gt;(&quot;topic09&quot;, record.key(), record.value()); producer.send(srecord); } producer.flush(); //并没使用 consumer提交，而是使用producer帮助消费者提交偏移量 producer.sendOffsetsToTransaction(commits,group); //提交生产者的偏移量 producer.commitTransaction(); } catch (Exception e) { //System.err.println(e.getMessage()); producer.abortTransaction(); } } } } SpringBoot整合Kafka pom.xml &lt;properties&gt; &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt; &lt;java.version&gt;1.8&lt;/java.version&gt; &lt;kafka.version&gt;2.2.0&lt;/kafka.version&gt; &lt;/properties&gt; &lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;2.1.5.RELEASE&lt;/version&gt; &lt;/parent&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.kafka&lt;/groupId&gt; &lt;artifactId&gt;spring-kafka&lt;/artifactId&gt; &lt;version&gt;2.2.5.RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;!-- kafka client处理 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.kafka&lt;/groupId&gt; &lt;artifactId&gt;kafka-clients&lt;/artifactId&gt; &lt;version&gt;${kafka.version}&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; application.properties server.port=8888 # 生产者 spring.kafka.producer.bootstrap-servers=CentOSA:9092,CentOSB:9092,CentOSC:9092 spring.kafka.producer.acks=all spring.kafka.producer.retries=1 spring.kafka.producer.key-serializer=org.apache.kafka.common.serialization.StringSerializer spring.kafka.producer.value-serializer=org.apache.kafka.common.serialization.StringSerializer # 消费者 spring.kafka.consumer.bootstrap-servers=CentOSA:9092,CentOSB:9092,CentOSC:9092 spring.kafka.consumer.key-deserializer=org.apache.kafka.common.serialization.StringDeserializer spring.kafka.consumer.value-deserializer=org.apache.kafka.common.serialization.StringDeserializer 代码 @SpringBootApplication @EnableScheduling public class KafkaApplicationDemo { @Autowired private KafkaTemplate kafkaTemplate; public static void main(String[] args) { SpringApplication.run(KafkaApplicationDemo.class,args); } @Scheduled(cron = &quot;0/1 * * * * ?&quot;) public void send(){ String[] message=new String[]{&quot;this is a demo&quot;,&quot;hello world&quot;,&quot;hello boy&quot;}; ListenableFuture future = kafkaTemplate.send(&quot;topic07&quot;, message[new Random().nextInt(message.length)]); future.addCallback(o -&gt; System.out.println(&quot;send-消息发送成功：&quot; + message), throwable -&gt; System.out.println(&quot;消息发送失败：&quot; + message)); } @KafkaListener(topics = &quot;topic07&quot;,id=&quot;g1&quot;) public void processMessage(ConsumerRecord&lt;?, ?&gt; record) { System.out.println(&quot;record:&quot;+record); } } ","link":"https://mask0407.github.io/kafka00/"},{"title":"Flume 介绍与使用","content":" 介绍 架构 Flume环境搭建 配置文件结构 快速入门 启动 Avro Source Maven依赖 代码 Avro Source | memory channel| Kafka Sink Flume和log4j整合 依赖 log4j.properties 测试代码 Spring Boot logback整合 Flume SpringBoot项目组引入logback.xml 集成 Flume +logback 定制自己的Appender Flume对接HDFS (静态批处理) 拦截器&amp;通道选择器 Sink Processor (Apache Flume) 介绍 Flume是一种分布式，可靠且可用的服务，用于有效地收集，聚合和移动大量日志数据。Flume构建在日志流之上一个简单灵活的架构。它具有可靠的可靠性机制和许多故障转移和恢复机制，具有强大的容错性。使用Flume这套架构实现对日志流数据的实时在线分析。Flume支持在日志系统中定制各类数据发送方，用于收集数据；同时，Flume提供对数据进行简单处理，并写到各种数据接受方（可定制）的能力。 架构 Flume环境搭建 JDK1.8 tar -zxf apache-flume-1.9.0-bin.tar.gz -C /usr/ 配置文件结构 # 声明组件信息 &lt;Agent&gt;.sources = &lt;Source1&gt; &lt;Source2&gt; &lt;Agent&gt;.sinks = &lt;Sink1&gt; &lt;Sink1&gt; &lt;Agent&gt;.channels = &lt;Channel1&gt; &lt;Channel2&gt; # 组件配置 &lt;Agent&gt;.sources.&lt;Source&gt;.&lt;someProperty&gt; = &lt;someValue&gt; &lt;Agent&gt;.channels.&lt;Channel&gt;.&lt;someProperty&gt; = &lt;someValue&gt; &lt;Agent&gt;.sinks.&lt;Sink&gt;.&lt;someProperty&gt; = &lt;someValue&gt; # 链接组件 &lt;Agent&gt;.sources.&lt;Source&gt;.channels = &lt;Channel1&gt; &lt;Channel2&gt; ... &lt;Agent&gt;.sinks.&lt;Sink&gt;.channel = &lt;Channel1&gt; 快速入门 第一台目标机器 vi conf/demo01.properties ---------------------------------------------------------------------------- # 声明组件信息 a1.sources = s1 a1.sinks = sk1 a1.channels = c1 # 组件配置 a1.sources.s1.type = TAILDIR a1.sources.s1.filegroups = f1 a1.sources.s1.filegroups.f1 = /root/logs/userLoginFile.* a1.channels.c1.type = memory a1.sinks.sk1.type = avro a1.sinks.sk1.hostname = 192.168.111.133 a1.sinks.sk1.port = 44444 # 链接组件 a1.sources.s1.channels = c1 a1.sinks.sk1.channel = c1 第二台目标机器(192.168.111.133) vi conf/demo01.properties ----------------------------------------------------------------------------- # 声明组件信息 a1.sources = s1 a1.sinks = sk1 a1.channels = c1 # 组件配置 a1.sources.s1.type = avro a1.sources.s1.bind = 192.168.111.133 a1.sources.s1.port = 44444 a1.channels.c1.type = memory a1.sinks.sk1.type = file_roll a1.sinks.sk1.sink.directory = /root/file_roll a1.sinks.sk1.sink.rollInterval = 0 # 链接组件 a1.sources.s1.channels = c1 a1.sinks.sk1.channel = c1 启动 先启动第二台机器 ./bin/flume-ng agent --conf conf/ --conf-file conf/demo01.properties --name a1 再启动第一台机器 ./bin/flume-ng agent --conf conf/ --conf-file conf/demo01.properties --name a1 Avro Source 一般可以通过Avro Sink 将结果直接写入 Avro Source，这种情况，一般指的是通过flume采集本地的日志文件，架构一般如上图所示，一般情况下的应用服务器必须和agent部署在同一台物理主机。（服务器端日志采集） 用户调用Flume的暴露的SDK，直接将数据发送给Avro Source（移动端） Maven依赖 &lt;dependency&gt; &lt;groupId&gt;org.apache.flume&lt;/groupId&gt; &lt;artifactId&gt;flume-ng-sdk&lt;/artifactId&gt; &lt;version&gt;1.9.0&lt;/version&gt; &lt;/dependency&gt; 代码 Properties props = new Properties(); props.setProperty(RpcClientConfigurationConstants.CONFIG_CLIENT_TYPE, &quot;avro&quot;); props.put(&quot;client.type&quot;, &quot;default_loadbalance&quot;); props.put(&quot;hosts&quot;, &quot;h1 h2 h3&quot;); String host1 = &quot;192.168.111.133:44444&quot;; String host2 = &quot;192.168.111.133:44444&quot;; String host3 = &quot;192.168.111.133:44444&quot;; props.put(&quot;hosts.h1&quot;, host1); props.put(&quot;hosts.h2&quot;, host2); props.put(&quot;hosts.h3&quot;, host3); props.put(&quot;host-selector&quot;, &quot;random&quot;); // round_robin RpcClient client= RpcClientFactory.getInstance(props); Event event= EventBuilder.withBody(&quot;1 zhangsan true 28&quot;.getBytes()); client.append(event); client.close(); Avro Source | memory channel| Kafka Sink vi conf/demo02.properties # 声明组件信息 a1.sources = s1 a1.sinks = sk1 a1.channels = c1 # 组件配置 a1.sources.s1.type = avro a1.sources.s1.bind = 192.168.111.132 a1.sources.s1.port = 44444 a1.channels.c1.type = memory a1.sinks.sk1.type = org.apache.flume.sink.kafka.KafkaSink a1.sinks.sk1.kafka.bootstrap.servers = 192.168.111.132:9092 a1.sinks.sk1.kafka.topic = topic01 a1.sinks.sk1.flumeBatchSize = 20 a1.sinks.sk1.kafka.producer.acks = 1 a1.sinks.sk1.kafka.producer.linger.ms = 1 # 链接组件 a1.sources.s1.channels = c1 a1.sinks.sk1.channel = c1 启动 /bin/flume-ng agent --conf conf/ --conf-file conf/demo02.properties --name a1 注意 a1.sinks.sk1.flumeBatchSize官方写错了a1.sinks.sk1.kafka.flumeBatchSize Flume和log4j整合 依赖 &lt;dependency&gt; &lt;groupId&gt;org.apache.flume&lt;/groupId&gt; &lt;artifactId&gt;flume-ng-sdk&lt;/artifactId&gt; &lt;version&gt;1.9.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.flume.flume-ng-clients&lt;/groupId&gt; &lt;artifactId&gt;flume-ng-log4jappender&lt;/artifactId&gt; &lt;version&gt;1.9.0&lt;/version&gt; &lt;/dependency&gt; log4j.properties log4j.appender.flume = org.apache.flume.clients.log4jappender.LoadBalancingLog4jAppender log4j.appender.flume.Hosts = 192.168.111.132:44444 192.168.111.132:44444 192.168.111.132:44444 log4j.appender.flume.Selector = ROUND_ROBIN log4j.appender.flume.MaxBackoff = 30000 log4j.logger.com.mask = DEBUG,flume 测试代码 import org.apache.commons.logging.Log; import org.apache.commons.logging.LogFactory; public class TestLog { private static Log log= LogFactory.getLog(TestLog.class); public static void main(String[] args) { log.debug(&quot;你好！_debug&quot;); log.info(&quot;你好！_info&quot;); log.warn(&quot;你好！_warn&quot;); log.error(&quot;你好！_error&quot;); } } Spring Boot logback整合 Flume SpringBoot项目组引入logback.xml &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;configuration scan=&quot;true&quot; scanPeriod=&quot;60 seconds&quot; debug=&quot;false&quot;&gt; &lt;appender name=&quot;STDOUT&quot; class=&quot;ch.qos.logback.core.ConsoleAppender&quot; &gt; &lt;encoder&gt; &lt;pattern&gt;%p %c#%M %d{yyyy-MM-dd HH:mm:ss} %m%n&lt;/pattern&gt; &lt;charset&gt;UTF-8&lt;/charset&gt; &lt;/encoder&gt; &lt;/appender&gt; &lt;appender name=&quot;FILE&quot; class=&quot;ch.qos.logback.core.rolling.RollingFileAppender&quot;&gt; &lt;rollingPolicy class=&quot;ch.qos.logback.core.rolling.TimeBasedRollingPolicy&quot;&gt; &lt;fileNamePattern&gt;logs/userLoginFile-%d{yyyyMMdd}.log&lt;/fileNamePattern&gt; &lt;maxHistory&gt;30&lt;/maxHistory&gt; &lt;/rollingPolicy&gt; &lt;encoder&gt; &lt;pattern&gt;%p %c#%M %d{yyyy-MM-dd HH:mm:ss} %m%n&lt;/pattern&gt; &lt;charset&gt;UTF-8&lt;/charset&gt; &lt;/encoder&gt; &lt;/appender&gt; &lt;!-- 控制台输出日志级别 --&gt; &lt;root level=&quot;ERROR&quot;&gt; &lt;appender-ref ref=&quot;STDOUT&quot; /&gt; &lt;/root&gt; &lt;!--additivity 为false，日志不会再父类appender中输出--&gt; &lt;logger name=&quot;com.mask.tests&quot; level=&quot;INFO&quot; additivity=&quot;false&quot;&gt; &lt;appender-ref ref=&quot;FILE&quot; /&gt; &lt;appender-ref ref=&quot;STDOUT&quot; /&gt; &lt;/logger&gt; &lt;/configuration&gt; 集成 Flume +logback 在github 上找到https://github.com/gilt/logback-flume-appender 将项目源代码拷贝到项目工程： 在SpringBoot工程中添加当前版本flume的sdk &lt;dependency&gt; &lt;groupId&gt;org.apache.flume&lt;/groupId&gt; &lt;artifactId&gt;flume-ng-sdk&lt;/artifactId&gt; &lt;version&gt;1.9.0&lt;/version&gt; &lt;/dependency&gt; 在项目的logback.xml中添加flume的appender实现 &lt;appender name=&quot;flume&quot; class=&quot;com.gilt.logback.flume.FlumeLogstashV1Appender&quot;&gt; &lt;flumeAgents&gt; 192.168.111.132:44444, 192.168.111.132:44444, 192.168.111.132:44444 &lt;/flumeAgents&gt; &lt;flumeProperties&gt; connect-timeout=4000; request-timeout=8000 &lt;/flumeProperties&gt; &lt;batchSize&gt;1&lt;/batchSize&gt; &lt;reportingWindow&gt;1000&lt;/reportingWindow&gt; &lt;additionalAvroHeaders&gt; myHeader=myValue &lt;/additionalAvroHeaders&gt; &lt;application&gt;smapleapp&lt;/application&gt; &lt;layout class=&quot;ch.qos.logback.classic.PatternLayout&quot;&gt; &lt;pattern&gt;%p %c#%M %d{yyyy-MM-dd HH:mm:ss} %m%n&lt;/pattern&gt; &lt;/layout&gt; &lt;/appender&gt; 定制自己的Appender public class BZFlumeLogAppender extends UnsynchronizedAppenderBase&lt;ILoggingEvent&gt; { private String flumeAgents; protected Layout&lt;ILoggingEvent&gt; layout; private static RpcClient rpcClient; @Override protected void append(ILoggingEvent eventObject) { String body= layout!= null? layout.doLayout(eventObject):eventObject.getFormattedMessage(); if(rpcClient==null){ rpcClient=buildRpcClient(); } Event event= EventBuilder.withBody(body,Charset.forName(&quot;UTF-8&quot;)); try { rpcClient.append(event); } catch (EventDeliveryException e) { e.printStackTrace(); } } public void setFlumeAgents(String flumeAgents) { this.flumeAgents = flumeAgents; } public void setLayout(Layout&lt;ILoggingEvent&gt; layout) { this.layout = layout; } private RpcClient buildRpcClient(){ Properties props = new Properties(); int i = 0; for (String agent : flumeAgents.split(&quot;,&quot;)) { String[] tokens = agent.split(&quot;:&quot;); props.put(&quot;hosts.h&quot; + (i++), tokens[0] + ':' + tokens[1]); } StringBuffer buffer = new StringBuffer(i * 4); for (int j = 0; j &lt; i; j++) { buffer.append(&quot;h&quot;).append(j).append(&quot; &quot;); } props.put(&quot;hosts&quot;, buffer.toString()); if(i &gt; 1) { props.put(&quot;client.type&quot;, &quot;default_loadbalance&quot;); props.put(&quot;host-selector&quot;, &quot;round_robin&quot;); } props.put(&quot;backoff&quot;, &quot;true&quot;); props.put(&quot;maxBackoff&quot;, &quot;10000&quot;); return RpcClientFactory.getInstance(props); } } &lt;appender name=&quot;bz&quot; class=&quot;com.mask.flume.BZFlumeLogAppender&quot;&gt; &lt;flumeAgents&gt; 192.168.111.132:44444,192.168.111.132:44444 &lt;/flumeAgents&gt; &lt;layout class=&quot;ch.qos.logback.classic.PatternLayout&quot;&gt; &lt;pattern&gt;%p %c#%M %d{yyyy-MM-dd HH:mm:ss} %m&lt;/pattern&gt; &lt;/layout&gt; &lt;/appender&gt; Flume对接HDFS (静态批处理) 将一个目录下的日志文件，采集到HDFS中，并且删除采集完成的日志文件（批处理作业中） spooldir source、jdbc channel、HDFS Sink # 声明组件信息 a1.sources = s1 a1.sinks = sk1 a1.channels = c1 # 组件配置 a1.sources.s1.type = spooldir a1.sources.s1.spoolDir = /root/spooldir a1.sources.s1.deletePolicy = immediate a1.sources.s1.includePattern = ^.*\\.log$ a1.channels.c1.type = jdbc a1.sinks.sk1.type = hdfs a1.sinks.sk1.hdfs.path= hdfs:///flume/%y-%m-%d/ a1.sinks.sk1.hdfs.filePrefix = events- a1.sinks.sk1.hdfs.useLocalTimeStamp = true a1.sinks.sk1.hdfs.rollInterval = 0 a1.sinks.sk1.hdfs.rollSize = 0 a1.sinks.sk1.hdfs.rollCount = 0 a1.sinks.sk1.hdfs.fileType = DataStream # 链接组件 a1.sources.s1.channels = c1 a1.sinks.sk1.channel = c1 拦截器&amp;通道选择器 # 声明组件信息 a1.sources = s1 a1.sinks = sk1 sk2 a1.channels = c1 c2 # 组件配置 a1.sources.s1.type = avro a1.sources.s1.bind = 192.168.111.132 a1.sources.s1.port = 44444 # 拦截器 a1.sources.s1.interceptors = i1 i2 a1.sources.s1.interceptors.i1.type = regex_filter a1.sources.s1.interceptors.i1.regex = .*UserController.* a1.sources.s1.interceptors.i1.excludeEvents = false a1.sources.s1.interceptors.i2.type = regex_extractor a1.sources.s1.interceptors.i2.regex = .*(EVALUATE|SUCCESS).* a1.sources.s1.interceptors.i2.serializers = s1 a1.sources.s1.interceptors.i2.serializers.s1.name = type a1.channels.c1.type = memory a1.channels.c2.type = memory a1.sinks.sk1.type = org.apache.flume.sink.kafka.KafkaSink a1.sinks.sk1.kafka.bootstrap.servers = 192.168.111.132:9092 a1.sinks.sk1.kafka.topic = evaluatetopic a1.sinks.sk1.flumeBatchSize = 20 a1.sinks.sk1.kafka.producer.acks = 1 a1.sinks.sk1.kafka.producer.linger.ms = 1 a1.sinks.sk2.type = org.apache.flume.sink.kafka.KafkaSink a1.sinks.sk2.kafka.bootstrap.servers = 192.168.111.132:9092 a1.sinks.sk2.kafka.topic = usertopic a1.sinks.sk2.flumeBatchSize = 20 a1.sinks.sk2.kafka.producer.acks = 1 a1.sinks.sk2.kafka.producer.linger.ms = 1 # 通道选择器分流 a1.sources.s1.selector.type = multiplexing a1.sources.s1.selector.header = type a1.sources.s1.selector.mapping.EVALUATE = c1 a1.sources.s1.selector.mapping.SUCCESS = c2 a1.sources.s1.selector.default = c2 # 链接组件 a1.sources.s1.channels = c1 c2 a1.sinks.sk1.channel = c1 a1.sinks.sk2.channel = c2 Sink Processor # 声明组件 a1.sources = s1 a1.sinks = sk1 sk2 a1.channels = c1 # 将看 k1 k2 归纳一个组 a1.sinkgroups = g1 a1.sinkgroups.g1.sinks = sk1 sk2 a1.sinkgroups.g1.processor.type = load_balance a1.sinkgroups.g1.processor.backoff = true a1.sinkgroups.g1.processor.selector = round_robin # 配置source属性 a1.sources.s1.type = avro a1.sources.s1.bind = 192.168.111.132 a1.sources.s1.port = 44444 # 配置sink属性 a1.sinks.sk1.type = org.apache.flume.sink.kafka.KafkaSink a1.sinks.sk1.kafka.bootstrap.servers = 192.168.111.132:9092 a1.sinks.sk1.kafka.topic = evaluatetopic a1.sinks.sk1.flumeBatchSize = 20 a1.sinks.sk1.kafka.producer.acks = 1 a1.sinks.sk1.kafka.producer.linger.ms = 1 a1.sinks.sk2.type = org.apache.flume.sink.kafka.KafkaSink a1.sinks.sk2.kafka.bootstrap.servers = 192.168.111.132:9092 a1.sinks.sk2.kafka.topic = usertopic a1.sinks.sk2.flumeBatchSize = 20 a1.sinks.sk2.kafka.producer.acks = 1 a1.sinks.sk2.kafka.producer.linger.ms = 1 # 配置channel属性 a1.channels.c1.type = memory a1.channels.c1.transactionCapacity = 1 # 将source连接channel a1.sources.s1.channels = c1 a1.sinks.sk1.channel = c1 a1.sinks.sk2.channel = c1 ","link":"https://mask0407.github.io/flume01/"},{"title":"Hadoop生态圈-Hive","content":" Hive引言 Hive的运行原理 Hive环境搭建 Hive基本操作 MetaStore的替换问题 Hive基础语法 1.HQL 2.表操作 1）管理表 (MANAGED_TABLE) 2)外部表 3) 分区表【优化查询】 4）桶表 5）临时表 3. 数据的导入 1). 基本导入 2). 通过as关键完成数据的导入 3). 通过insert的方式导入数据 4). hdfs导入数据 5). 导入数据过程中数据的覆盖 6). 通过HDFS的API完成文件的上传 4. 数据的导出 1). sqoop 2). insert的方式 3). 通过HDFS的API完成文件的下载 4). 命令行脚本的方式 5. Hive提供导入，导出的工具 6.与MR相关的配置 (Hive) Hive引言 什么是Hive hive是facebook开源，并捐献给了apache组织，作为apache组织的顶级项目。 hive.apache.org hive是一个基于大数据技术的数据仓库技术 DataWareHouse (数仓) 数据库 DataBase 数据量级小，数据价值高 数据仓库 DataWareHouse 数据体量大，数据价值低 底层依附是HDFS,MapReduce Hive的好处 Hive让程序员应用时，书写SQL语句，最终由Hive把SQL语句转换成MapReduce运行，这样简化了程序员的工作。 Hive的运行原理 Hive是将大多数Hive SQL语句底层转换为MapReduce 运行Job作业来进行数据的处理 Hive环境搭建 1. linux服务器 ip 映射 主机名 关闭防火墙 关闭selinux ssh免密登陆 jdk 2. 搭建hadoop环境 3. 安装Hive 3.1 解压缩hive 3.2 hive_home/conf/hive-env.sh [改名] HADOOP_HOME=/opt/install/hadoop-2.5.2 export HIVE_CONF_DIR=/opt/install/apache-hive-0.13.1-bin/conf 3.2 hdfs创建2个目录 /tmp /user/hive/warehouse bin/hdfs dfs -mkdir /tmp bin/hdfs dfs -mkdir /user/hive/warehouse 3.3 启动hive bin/hive 3.4 jps runjar Hive基本操作 # 创建数据库 create database [if not exists] test; # 查看所有数据库 show databases; # 使用数据库 use db_name; # 删除空数据库 drop database db_name; drop database db_name cascade; # 查看数据库的本质 hive中的数据库 本质是 hdfs的目录 /user/hive/warehouse/test.db # 查看当前数据库下的所有表 show tables; # 建表语句 create table t_user( id int , name string )row format delimited fields terminated by '\\t'; # 查看表的本质 hive中的表 本质是 hdfs的目录 /user/hive/warehouse/test.db/t_user # 删除表 drop table t_user; # hive中向表导入数据 load data local inpath '/root/hive/data' into table t_user; # hive导入数据的本质 load data local inpath '/root/hive/data' into table t_user; 1. 导入数据 本质本质上就是 hdfs 上传文件 bin/hdfs dfs -put /root/hive/data /user/hive/warehouse/test.db/t_user; 2. 上传了重复数据，hive导数据时，会自动修改文件名 3. 查询某一个张表时，Hive会把表中这个目录下所有文件的内容，整合查询出来 # SQL(类SQL 类似于SQL HQL Hive Query Language) select * from t_user; select id from t_user; 1. Hive把SQL转换成MapReduce (如果清洗数据 没有Reduce) 2. Hive在绝大多数情况下运行MR,但是在* limit操作时不运行MR MetaStore的替换问题 Hive中的MetaStore把HDFS对应结构，与表对应结果做了映射（对应）。但是默认情况下hive的metaStore应用的是derby数据库，只支持一个client访问。 Hive中元数据库Derby替换成MySQL(Oracle) 0. 删除hdfs /user/hive/warehouse目录，并重新建立 1. linux mysql yum -y install mysql-server 2. 启动mysql服务并设置管理员密码 service mysqld start /usr/bin/mysqladmin -u root password '123456' 3. 打开mysql远程访问权限 GRANT ALL PRIVILEGES ON *.* TO 'root'@'%' IDENTIFIED BY '123456'; flush privileges; use mysql delete from user where host like 'hadoop%'; delete from user where host like 'l%'; delete from user where host like '1%'; service mysqld restart 4. 创建conf/hive-site.xml mv hive-default.xml.template hive-site.xml hive-site.xml &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt; &lt;value&gt;jdbc:mysql://CentOSA:3306/metastore?createDatabaseIfNotExist=true&lt;/value&gt; &lt;description&gt;the URL of the MySQL database&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt; &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt; &lt;description&gt;Driver class name for a JDBC metastore&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt; &lt;value&gt;root&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt; &lt;value&gt;123456&lt;/value&gt; &lt;/property&gt; 5. hive_home/lib 上传mysql driver jar包 Hive基础语法 1.HQL 1. 基本查询 select * from table_name # 不启动mr select id from table_name # 启动mr 2. 条件查询 where select id,name from t_users where name = 'mask1'; 2.1 比较查询 = ！= &gt;= &lt;= select id,name from t_users where age &gt; 20; 2.2 逻辑查询 and or not select id,name,age from t_users where name = 'mask' or age&gt;30; 2.3 谓词运算 between and select name,salary from t_users where salary between 100 and 300; in select name,salary from t_users where salary in (100,300); is null select name,salary from t_users where salary is null; like select name,salary from t_users where name like 'mask%'; select name,salary from t_users where name like 'mask__'; select name,salary from t_users where name like 'mask%' and length(name) = 6; 3. 排序 order by [底层使用的是 map sort group sort compareto] select name,salary from t_users order by salary desc; 4. 去重 distinct select distinct(age) from t_users; 5. 分页 [Mysql可以定义起始的分页条目，但是Hive不可以] select * from t_users limit 3; 6. 聚合函数（分组函数） count() avg() max() min() sum() count(*) count(id) 区别 7. group by select max(salary) from t_users group by age; 规矩： select 后面只能写 分组依据和聚合函数 （Oracle报错，Mysql不报错，结果不对） 8. having 分组后，聚合函数的条件判断用having select max(salary) from t_users group by age having max(salary) &gt; 800; 9. hive不支持子查询 10. hive内置函数 show functions length(column_name) 获得列中字符串数据长度 substring(column_name,start_pos,total_count) concat(col1,col2) to_data('yyyy-mm-dd') year(data) 获得年份 month(data) date_add .... select year(to_date('1999-10-11')) ; 11. 多表操作 inner join select e.name,e.salary,d.dname from t_emp as e inner join t_dept as d on e.dept_id = d.id; select e.name,e.salary,d.dname from t_emp as e left join t_dept as d on e.dept_id = d.id; select e.name,e.salary,d.dname from t_emp as e right join t_dept as d on e.dept_id = d.id; select e.name,e.salary,d.dname [mysql 不支持] from t_emp as e full join t_dept as d on e.dept_id = d.id; 2.表操作 1）管理表 (MANAGED_TABLE) 1. 基本管理表的创建 create table if not exists table_name( column_name data_type, column_name data_type )row format delimited fields terminated by '\\t' [location 'hdfs_path'] 2. as 关键字创建管理表 create table if not exists table_name as select id,name from t_users [location '']; 表结构 由 查询的列决定，同时会把查询结果的数据 插入新表中 3. like 关键字创建管理表 create table if not exists table_name like t_users [location 'hdfs_path']; 表结构 和 like关键字后面的表 一致，但是没有数据是空表 细节操作 1. 数据类型 int string varchar char double float boolean 2. location hdfs_path 定制创建表的位置，默认是 /user/hive/warehouse/db_name.db/table_name create table t_mask( id,int name,string )row format delimited fields terminated by '\\t' stored as textfile location /xiaohei ; 启示：日后先有hdfs目录，文件，在创建表进行操作。 3. 查看hive表结构的命令 desc table_name describe table_name desc extended table_name desc formatted table_name 2)外部表 1. 基本 create external table if not exists table_name( id int, name string ) row delimited fields terminated by '\\t' stored as textfile [location 'hdfs_path']; 2. as create external table if not exists table_name as select id,name from t_users [location '']; 3. like create external table if not exists table_name like t_users [location 'hdfs_path']; 4. 管理表和外部表的区别 drop table t_users_as; 删除管理表时，直接删除metastore,同时删除hdfs的目录和数据文件 drop table t_user_ex; 删除外部表时，删除metastore的数据。 5. 外部表与管理表使用方式的区别 3) 分区表【优化查询】 分区表是为了提高条件查询时的效率 create table t_user_part( id int, name string, age int, salary int)partitioned by (data string) row format delimited fields terminated by '\\t'; load data local inpath '/root/data15' into table t_user_part partition (date='15'); load data local inpath '/root/data16' into table t_user_part partition (date='16'); select * from t_user_part 全表数据进行的统计 select id from t_user_part where data='15' and age&gt;20; 4）桶表 5）临时表 3. 数据的导入 1). 基本导入 load data local inpath 'local_path' into table table_name 2). 通过as关键完成数据的导入 建表的同时，通过查询导入数据 create table if not exists table_name as select id,name from t_users 3). 通过insert的方式导入数据 #表格已经建好，通过查询导入数据。 create table t_users_like like t_users; insert into table t_users_like select id,name,age,salary from t_users; 4). hdfs导入数据 load data inpath 'hdfs_path' into table table_name 5). 导入数据过程中数据的覆盖 load data inpath 'hdfs_path' overwrite into table table_name 本质 把原有表格目录的文件全部删除，再上传新的 6). 通过HDFS的API完成文件的上传 bin/hdfs dfs -put /xxxx /user/hive/warehouse/db_name.db/table_name 4. 数据的导出 1). sqoop hadoop的一种辅助工具 HDFS/Hive &lt;------&gt; RDB (MySQL,Oracle) 2). insert的方式 #xiaohei一定不能存在，自动创建 insert overwrite 【local】 directory '/root/xiaohei' select name from t_user; 3). 通过HDFS的API完成文件的下载 bin/hdfs dfsd -get /user/hive/warehouse/db_name.db/table_name /root/xxxx 4). 命令行脚本的方式 bin/hive --database 'test' -f /root/hive.sql &gt; /root/result 5. Hive提供导入，导出的工具 1. export 导出 export table tb_name to 'hdfs_path' 2. import 导入 import table tb_name from 'hdfs_path' 6.与MR相关的配置 #与MR相关的参数 Map --&gt; Split ---&gt; Block #reduce相关个数 mapred-site.xml &lt;property&gt; &lt;name&gt;mapreduce.job.reduces&lt;/name&gt; &lt;value&gt;1&lt;/value&gt; &lt;/property&gt; hive-site.xml &lt;!--1G--&gt; &lt;property&gt; &lt;name&gt;hive.exec.reducers.bytes.per.reducer&lt;/name&gt; &lt;value&gt;1000000000&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hive.exec.reducers.max&lt;/name&gt; &lt;value&gt;999&lt;/value&gt; &lt;/property&gt; ","link":"https://mask0407.github.io/hive01/"},{"title":"Hive安装(超详细)","content":" 前提环境 Hive安装 解压文件 修改配置文件 HDFS创建Hive工作空间 配置Hive环境变量 Hive启动 替换Hive原生MetaStore (Hive安装) 前提环境 Linux基础设置以及Hadoop环境安装请看上一篇文章 Hive安装 解压文件 [root@localhost ~]# tar -zxvf apache-hive-1.2.2-bin.tar.gz -C /opt/install/ 修改配置文件 [root@localhost apache-hive-1.2.2-bin]# cd /opt/install/apache-hive-1.2.2-bin/conf hive-env.sh 拷贝创建hive-env.sh文件 [root@localhost conf]# cp hive-env.sh.template hive-env.sh 加入配置 HADOOP_HOME=/opt/install/hadoop-2.9.2 export HIVE_CONF_DIR=/opt/install/apache-hive-1.2.2-bin/conf HDFS创建Hive工作空间 hdfs创建2个目录 hadoop dfs -mkdir /tmp hadoop dfs -mkdir -p /user/hive/warehouse 查看是否创建成功 [root@localhost conf]# hadoop fs -ls / 配置Hive环境变量 [root@localhost apache-hive-1.2.2-bin]# vim /etc/profile # 加入 export HIVE_HOME=/opt/install/apache-hive-1.2.2-bin export PATH=$PATH:$JAVA_HOME/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$HIVE_HOME/bin # 刷新 [root@localhost apache-hive-1.2.2-bin]# source /etc/profile Hive启动 进入Hive交互界面 hive hive基本操作指令参考之前博客 替换Hive原生MetaStore 未完待续。。。。 ","link":"https://mask0407.github.io/hive00/"},{"title":"Hbase 伪分布式及高可用集群搭建","content":" Hbase的引言 什么是HBase NoSQL特点 NoSQL分类 Hbase存储的逻辑结构 Hbase伪分布式环境的搭建 Hbase的shell命令 HBase 集群搭建 1.时间同步集群 2.Hadoop集群搭建 3. Zookeeper集群 4. HBase集群 HBase JAVA API Maven依赖 方法 HBase中的过滤器 HBase中列簇相关的属性 HBase 体系结构 1.RegionServer 2. HMaster作用 3. Zookeeper作用 【重点】 4. HBase compact 和 split RowKey设计 (HBase) Hbase的引言 什么是HBase hbase是Apache 组织开源的顶级项目 distributed, scalable, big data store 产品 hbase是基于Hadoop的一个NoSQL产品 Column类型的NoSQL hbase是Google BigTable的开源实现, 爬虫爬取的网页 hbase运行亿级数据查询时，效率可达到秒级，毫秒级 在线处理 实时的处理 NoSQL特点 1. 部分NoSQL In-Memory 内存型 （Redis) 2. Schema-Less NoSchema 弱格式 无格式 3. 杜绝表连接 4. 弱化事务，没有事务 （Redis有事务，MongoDB(4.x 没事务 4.x后有事务) 5. 搭建集群方便 NoSQL分类 1. key value 类型 redis 2. document 类型 mongodb 3. column 类型 HBase Cassandra 4. 图 类型 neo4j (金融 知识图谱) Hbase存储的逻辑结构 mysql t_user HBase 多版本的好处：修改速度快，直接新增一条新数据，给旧数据标记墓碑 Hbase伪分布式环境的搭建 Hmaster、 Hregionserver 1. linux服务器 ip 主机名 主机映射 防火墙 selinux ssh免密 jdk 2. hadoop安装 2.1 解压缩 2.2 6个配置文件 2.3 格式化 2.4 启动进程 3. 安装zookeeper 3.1 解压缩 3.2 配置conf/zoo.cfg 3.3 创建临时目录 data ---&gt; myid文件（集群） 3.4 启动服务 4. hbase的安装 4.1 解压缩hbase 4.2 hdfs上创建 /hbase文件夹 hbase_home/data/tmp文件夹 4.3 修改hbase相关的配置文件 env.sh export HBASE_MANAGES_ZK=false export JAVA_HOME=/usr/java/jdk1.7.0_71 hbase-site.xml &lt;property &gt; &lt;name&gt;hbase.tmp.dir&lt;/name&gt; &lt;value&gt;/opt/install/hbase-0.98.6-hadoop2/data/tmp&lt;/value&gt; &lt;/property&gt; &lt;property &gt; &lt;name&gt;hbase.rootdir&lt;/name&gt; &lt;value&gt;hdfs://CentOSA:8020/hbase&lt;/value&gt; &lt;/property&gt; &lt;property &gt; &lt;name&gt;hbase.cluster.distributed&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hbase.zookeeper.quorum&lt;/name&gt; &lt;value&gt;CentOSA&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hbase.zookeeper.property.dataDir&lt;/name&gt; &lt;value&gt;/opt/install/zookeeper-3.4.6&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hbase.unsafe.stream.capability.enforce&lt;/name&gt; &lt;value&gt;false&lt;/value&gt; &lt;/property&gt; 4.4 修改 regionservers文件 CentOSA 4.5 替换hbase相关hadoop的jar 4.6 启动hbase bin/hbase-daemon.sh start master bin/hbase-daemon.sh start regionserver 4.7 网络访问 http://CentOS:60010 bin/hbase shell Hbase的shell命令 1. help 帮助命令 help '命令名字' 2. hbase中数据库的概念 namespace 2.1 显示所有的数据库 list_namespace 默认 default hbase 2.2 显示当前数据库中所有的表 list_namespace_tables 'hbase' 2.3 创建一个数据库 create_namespace 'test' 2.4 描述数据库 describe_namespace 'test' 2.5 修改数据库 alter_namespace 2.6 删除数据库 drop_namespace 'test' 3. 创建hbase中的表【重点】 3.1 基本的建表方式 【默认default库中】 create 't1','cf1' 3.2 创建多个列簇【默认default库中】 create 't1','cf1','cf2' 3.3 指定表所属的数据库 create 'test:t1','cf1' 3.4 详细描述列簇的相关属性 create 'test:t2',{NAME=&gt;'cf1',VERSIONS=&gt;2},{NAME=&gt;'cf2'} 4. 描述表 describe 'test:t1' 5. 修改表 alter 'test:t2',{NAME=&gt;'cf2',VERSIONS=&gt;2} 6. 删除表 disable 'test:t2' drop 'test:t2' 7. 失效 生效表相关命令 enable disable enable_all disable_all is_enable is_disable 8. 判断表是否存在 exists 9. 表的查找命令 list 'namespace:t.*' 10. 插入数据 put 't1'，’rowkey‘,'family:qualify','value' put 'namespace:t1'，’rowkey‘,'family:qualify','value' 11. 删除数据 delete 'namespace:t1' ,'rowkey','family:qualify','timestamp' 12. 全表扫描 scan '表名' scan 'namespace:tb1', {STARTROW =&gt; '20170521_10001',STOPROW =&gt; '20170521_10003'} scan 'test:user',{STARTROW=&gt;'001',STOPROW=&gt;'004'} 不包括stoprow的值 13. 某条数据的查询 get 'test:user','001' get 'test:user','001','base:name' HBase 集群搭建 1.时间同步集群 CentOS 作为时间同步服务器 主节点 1. yum install ntp 三台机器 2. service ntpd start 三台机器 chkconfig ntpd on 3. 服务器节点 主节点 ntpdate -u 202.112.10.36 vi /etc/ntp.conf restrict 192.168.111.0 mask 255.255.255.0 nomodify notrap # 中国这边最活跃的时间服务器 : http://www.pool.ntp.org/zone/cn server 210.72.145.44 perfer # 中国国家受时中心 server 202.112.10.36 # 1.cn.pool.ntp.org server 59.124.196.83 # 0.asia.pool.ntp.org # 允许上层时间服务器主动修改本机时间 restrict 210.72.145.44 nomodify notrap noquery restrict 202.112.10.36 nomodify notrap noquery restrict 59.124.196.83 nomodify notrap noquery # 外部时间服务器不可用时，以本地时间作为时间服务 server 127.127.1.0 # local clock fudge 127.127.1.0 stratum 10 service ntpd restart 4. client端 vi /etc/ntp.conf server 192.168.111.41 #这里指的是ntp服务的ip 192.168.206.130 restrict 192.168.111.41 nomodify notrap noquery server 127.127.1.0 # local clock fudge 127.127.1.0 stratum 10 5. 三台机器 service ntpd restart 6. 从节点同步主节点时间 ntpdate -u 192.168.184.16 adjust time server 192.168.19.10 offset -0.017552 sec 5. date命令查看处理结果 2.Hadoop集群搭建 HDFS集群 YARN集群 3. Zookeeper集群 1. 解压缩 2. 创建数据文件夹 zookeeper_home/data 3. 修改配置文件 conf/zoo.cfg dataDir server.0 4. 在data文件夹中创建 myid文件 0 1 2 5. 启动服务 4. HBase集群 1. 准备：hbase_home data/tmp logs目录中的内容清空 hdfs 上面 hbase目录清空 2. 修改hbase_home/conf/hbase-env.sh export JAVA_HOME=/usr/java/jdk1.7.0_71 export HBASE_MANAGES_ZK=false 3. hbase-site.xml &lt;property &gt; &lt;name&gt;hbase.tmp.dir&lt;/name&gt; &lt;value&gt;/opt/install/hbase-0.98.6-hadoop2/data/tmp&lt;/value&gt; &lt;/property&gt; &lt;property &gt; &lt;name&gt;hbase.rootdir&lt;/name&gt; &lt;value&gt;hdfs://CentOSA:8020/hbase&lt;/value&gt; &lt;/property&gt; &lt;property &gt; &lt;name&gt;hbase.cluster.distributed&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hbase.zookeeper.quorum&lt;/name&gt; &lt;value&gt;CentOSA,CentOSB,CentOSC&lt;/value&gt; &lt;/property&gt; 4. regionservers CentOSA CentOSB CentOSC 5. 启动hbase bin/hbase-daemon.sh start master bin/hbase-daemon.sh start regionserver HBase JAVA API # java访问HBase的核心API Configruation HBase相关的配置 Htable HBase中的表 Put 插入数据 Get 查询数据 Scan 扫描数据 BytesUtil 字节处理 Maven依赖 &lt;!-- https://mvnrepository.com/artifact/org.apache.hbase/hbase-client --&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hbase&lt;/groupId&gt; &lt;artifactId&gt;hbase-client&lt;/artifactId&gt; &lt;version&gt;0.98.6-hadoop2&lt;/version&gt; &lt;/dependency&gt; &lt;!-- https://mvnrepository.com/artifact/org.apache.hbase/hbase-server --&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hbase&lt;/groupId&gt; &lt;artifactId&gt;hbase-server&lt;/artifactId&gt; &lt;version&gt;0.98.6-hadoop2&lt;/version&gt; &lt;/dependency&gt; 方法 对象 作用 基本用法 Configruation Hbase相关的配置文件 conf.set(&quot;key&quot;,&quot;value&quot;); HTable Hbase中的表 表相关的操作都是HTable完成 Put HBase中插入数据 Put put = new Put(rowkey)put.addHTable.put(put) Delete HBase中的删除操作 Delete delete = new Delete(rowkey)HTable.delete(delete) Get HBase查询单条数据 Get get = net Get(rowkey)HTable.get(get) ---&gt; Result Result 单行数据 Result -- Cells --- Cell --- cloneFamilycloneQualifycloneValue Scan 表的扫描 ResultScanner ---&gt; Result HBase中的过滤器 行键相关的过滤器 1.比较行键值的大小 Filter filter1 = new RowFilter(CompareFilter.CompareOp.GREATER, new BinaryComparator(Bytes.toBytes(&quot;0003&quot;))); scan.setFilter(filter1); 2. 比较行键按照特定的规则设计 Filter filter1 = new PrefixFilter(Bytes.toBytes(&quot;000&quot;)); scan.setFilter(filter1); 列簇相关的筛选 1. 只要base列簇相关的数据 Filter filter1 = new FamilyFilter(CompareFilter.CompareOp.EQUAL, new BinaryComparator(Bytes.toBytes(&quot;base&quot;))); scan.setFilter(filter1); 限定符相关筛选 Filter filter1 = new QualifierFilter(CompareFilter.CompareOp.EQUAL, new BinaryComparator(Bytes.toBytes(&quot;age&quot;))); scan.setFilter(filter1); 值的筛选 Filter filter = new ValueFilter(CompareFilter.CompareOp.EQUAL, new SubstringComparator(&quot;1&quot;) ); scan.setFilter(filter); 列簇中的数据进行筛选 SingleColumnValueFilter filter = new SingleColumnValueFilter( Bytes.toBytes(&quot;base&quot;), Bytes.toBytes(&quot;sex&quot;), CompareFilter.CompareOp.EQUAL, new SubstringComparator(&quot;male&quot;)); filter.setFilterIfMissing(true); //filter.setFilterIfMissing(false); //符合要求的数据 password=123456 //column 中不包含password限定符 查询出来 scan.setFilter(filter); //同时要排除password 限定符 SingleColumnValueExcludeFilter filter = new SingleColumnValueExcludeFilter( Bytes.toBytes(&quot;base&quot;), Bytes.toBytes(&quot;password&quot;), CompareFilter.CompareOp.NOT_EQUAL, new SubstringComparator(&quot;66666&quot;)); filter.setFilterIfMissing(true); //filter.setFilterIfMissing(false); //符合要求的数据 password=123456 //column 中不包含password限定符 查询出来 scan.setFilter(filter); FilterList 设置多个过滤器 同时生效 FilterList filterList = new FilterList(); Filter filter1 = new PrefixFilter(Bytes.toBytes(&quot;000&quot;)); SingleColumnValueFilter filter2 = new SingleColumnValueFilter( Bytes.toBytes(&quot;base&quot;), Bytes.toBytes(&quot;password&quot;), CompareFilter.CompareOp.EQUAL, new SubstringComparator(&quot;123456&quot;)); filter2.setFilterIfMissing(true); filterList.addFilter(filter1); filterList.addFilter(filter2); scan.setFilter(filterList); HBase中列簇相关的属性 1. 创建HBase表 create 'table_name',{NAME=&gt;'',VERSIONS=&gt;''} 2. Hbase列簇的常见属性 # 列簇的名字 NAME='xxxxx' # 列簇对应限定符 能存几个版本的数据 VERSIONS =&gt; '1' # TTL Time To Live 指定的是cell中的数据，存储在HBase中的存活时间 'FOREVER' TTL =&gt; 100 # 指定HBase上存储的数据 是否 启动压缩 COMPRESSION =&gt; 'NONE' COMPRESSION =&gt; 'snappy' # 列簇中的数据，存储在内存中，提高查询效率 （默认关闭） IN_MEMORY =&gt; 'false’ # 缓存 列簇部分数据，从而提高查询效率 BLOCKCACHE =&gt; 'true' # Block是列簇中存储数据的最小单位 BLOCKSIZE =&gt; '65536' 调整大 顺序查询 需求高 调整小 随机查询 需求高 # 提高查询效率 BLOOMFILTER 布隆过滤 HBase 体系结构 1.RegionServer 2. HMaster作用 1. HRegionServer 集群是否健康 2. Region---RegionServer分配 3. 新Region加入后，负载均衡 3. Zookeeper作用 【重点】 1. 管理HMaster的高可用 2. 存储了HBase中非常重要的信息 meta信息 rowkey 范围 ---- region ---- RegionServer(健康) 4. HBase compact 和 split RowKey设计 1. HBase相关的查询操作，95%上都是对RowKey查询。 2. 设计过程 2.1 复合 2.2 查询内容作为rowkey组成 3. rowkey 64K 10--100字节唯一 4. rowkey结合自己的实际需求 4.1 区域查询多，建议 rowkey 连续 4.4 区域查询少，散列 hash ---&gt; 加密、UUID System.out.println(UUID.randomUUID().toString()); String rowkey = &quot;xx_male_001&quot;; String result = DigestUtils.md5Hex(rowkey); System.out.println(result); ","link":"https://mask0407.github.io/hbase01/"},{"title":"Hbase单节点伪分布式搭建","content":"Hbase伪分布式搭建 引言 Hbase伪分布式搭建是指在一台机器上同时运行 Hmaster、 Hregionserver，该文章基于Hadoop伪分布式搭建 Hadoop伪分布式搭建传送门 搭建过程 Zookeeper安装 将Zookeeper压缩包上传至服务器 解压缩Zookeeper二进制文件 [root@CentOS ~]# tar -zxvf zookeeper-3.4.6.tar.gz -C /opt/install/c 进入Zookeeper根目录 [root@CentOS ~]# cd /opt/install/zookeeper-3.4.6/ 修改配置文件 # 创建zoo.cfg配置文件 mkdir /root/zkdata echo 1 &gt;&gt; /root/zkdata/myid [root@CentOS zookeeper-3.4.6]# cp conf/zoo_sample.cfg conf/zoo.cfg # 修改 dataDir=/root/zkdata # 加入 server.1=CentOS:2888:3888 启动Zookeeper [root@CentOS zookeeper-3.4.6]# bin/zkServer.sh start conf/zoo.cfg 查看Zookeeper状态 [root@CentOS zookeeper-3.4.6]# bin/zkServer.sh status 伪分布式单节点ZK为Standalone模式 Hbase安装 将Hbase压缩包上传至服务器 解压缩Hbase二进制文件 [root@CentOS ~]# tar -zxvf hbase-2.2.5-bin.tar.gz -C /opt/install/ 进入Hbase根目录 [root@CentOS ~]# cd /opt/install/hbase-2.2.5 修改配置文件 hbase.env.sh [root@CentOS hbase-2.2.5]# vi conf/hbase-env.sh # 修改 export JAVA_HOME=/opt/install/jdk1.8.0_144 # 不使用内置Zookeeper export HBASE_MANAGES_ZK=false hbase-site.xml [root@CentOS hbase-2.2.5]# vi conf/hbase-site.xml &lt;configuration&gt; &lt;property&gt; &lt;name&gt;hbase.tmp.dir&lt;/name&gt; &lt;value&gt;/opt/install/hbase-2.2.5/data/tmp&lt;/value&gt; &lt;/property&gt; &lt;property &gt; &lt;name&gt;hbase.rootdir&lt;/name&gt; &lt;value&gt;hdfs://CentOS:8020/hbase&lt;/value&gt; &lt;/property&gt; &lt;property &gt; &lt;name&gt;hbase.cluster.distributed&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hbase.zookeeper.quorum&lt;/name&gt; &lt;value&gt;CentOS&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hbase.zookeeper.property.dataDir&lt;/name&gt; &lt;value&gt;/opt/install/zookeeper-3.4.6&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hbase.unsafe.stream.capability.enforce&lt;/name&gt; &lt;value&gt;false&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; regionservers [root@CentOS hbase-2.2.5]# vi conf/regionservers # 修改为： CentOS Hbase启动命令 [root@CentOS hbase-2.2.5]# bin/hbase-daemon.sh start master [root@CentOS hbase-2.2.5]# bin/hbase-daemon.sh start regionserver ","link":"https://mask0407.github.io/hbase00/"},{"title":"HDFS 去重功能","content":"本地上传文件到HDFS利用Redis去重 前言： HDFS存储数据块的同时还会存储数据的MD5加密校验和用来判断该数据是否完整 下面代码就是模仿这个特性做了一个小功能进行上传时内容去重 技术选型 springboot Redis Hadoop JSP Maven pom.xml &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;1.5.7.RELEASE&lt;/version&gt; &lt;relativePath/&gt; &lt;!-- lookup parent from repository --&gt; &lt;/parent&gt; &lt;groupId&gt;com.msk&lt;/groupId&gt; &lt;artifactId&gt;springboot-hadoop&lt;/artifactId&gt; &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt; &lt;name&gt;springboot-hadoop&lt;/name&gt; &lt;description&gt;Demo project for Spring Boot&lt;/description&gt; &lt;properties&gt; &lt;java.version&gt;1.8&lt;/java.version&gt; &lt;/properties&gt; &lt;dependencies&gt; &lt;!-- Springboot的web支持 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!-- Springboot的测试支持 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;!-- 只在test测试里面运行 --&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;!-- springboot于jsp整合 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.tomcat.embed&lt;/groupId&gt; &lt;artifactId&gt;tomcat-embed-jasper&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!-- 对Redis的依赖支持 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-redis&lt;/artifactId&gt; &lt;version&gt;1.5.8.RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;commons-codec&lt;/groupId&gt; &lt;artifactId&gt;commons-codec&lt;/artifactId&gt; &lt;version&gt;1.10&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-common&lt;/artifactId&gt; &lt;version&gt;2.5.2&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-hdfs&lt;/artifactId&gt; &lt;version&gt;2.5.2&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-client&lt;/artifactId&gt; &lt;version&gt;2.5.2&lt;/version&gt; &lt;/dependency&gt; &lt;!-- https://mvnrepository.com/artifact/org.springframework.boot/spring-boot-starter-data-redis --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-redis&lt;/artifactId&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; &lt;/project&gt; application.yml server: port: 8888 context-path: /hadoop spring: http: multipart: max-file-size: 10MB max-request-size: 100MB redis: host: 192.168.227.100 port: 7000 JSP页面 &lt;%@ page contentType=&quot;text/html;charset=UTF-8&quot; language=&quot;java&quot; isELIgnored=&quot;false&quot; %&gt; &lt;html&gt; &lt;body&gt; &lt;form action=&quot;${pageContext.request.contextPath}/upload/upload.do&quot; method=&quot;post&quot; enctype=&quot;multipart/form-data&quot;&gt; &lt;input type=&quot;file&quot; name=&quot;file&quot; value=&quot;点击上传&quot;&gt; &lt;input type=&quot;submit&quot; value=&quot;提交&quot;&gt; &lt;/form&gt; &lt;/body&gt; &lt;/html&gt; Java代码 package com.msk.controller; import org.apache.commons.codec.digest.DigestUtils; import org.apache.hadoop.conf.Configuration; import org.apache.hadoop.fs.FSDataOutputStream; import org.apache.hadoop.fs.FileSystem; import org.apache.hadoop.fs.Path; import org.apache.hadoop.io.IOUtils; import org.springframework.beans.factory.annotation.Autowired; import org.springframework.data.redis.core.SetOperations; import org.springframework.data.redis.core.StringRedisTemplate; import org.springframework.web.bind.annotation.RequestMapping; import org.springframework.web.bind.annotation.RestController; import org.springframework.web.multipart.MultipartFile; import java.io.File; import java.io.FileInputStream; import java.io.IOException; @RestController @RequestMapping(&quot;/upload&quot;) public class Upload { @Autowired StringRedisTemplate redisTemplate; @RequestMapping(value = &quot;/upload&quot;, produces = &quot;application/json; charset=utf-8&quot;) public String upload(MultipartFile file) throws Exception { String filename = file.getOriginalFilename(); // 创建流 File f = new File(&quot;E:/&quot; + filename); // 加密去重 String md5Hex = DigestUtils.md5Hex(file.getBytes()); if (&quot;&quot;.equals(filename)) { return &quot;空的！&quot;; } Boolean b = isBoolean(md5Hex); if (b) { setRedis(md5Hex, filename); return &quot;已上传相同内容文件！！！&quot;; } else { // 存入本地 file.transferTo(f); FileInputStream inputStream = new FileInputStream(f); // 存入HDFS FileSystem filesystem = getFilesystem(); FSDataOutputStream fsDataOutputStream = filesystem.create(new Path(&quot;/data/&quot;+filename)); IOUtils.copyBytes(inputStream, fsDataOutputStream, 1024, true); setRedis(md5Hex, filename); return &quot;存好了，下一位&quot;; } } // 获取FileSystem对象 private FileSystem getFilesystem() throws IOException { Configuration configuration = new Configuration(); configuration.set(&quot;fs.defaultFS&quot;, &quot;hdfs://hadoop1.msk.com:8020&quot;); return FileSystem.get(configuration); } // 将加密数据存入redis private void setRedis(String key, String vlue) { SetOperations&lt;String, String&gt; set = redisTemplate.opsForSet(); set.add(key, vlue); } // 去重判断 private Boolean isBoolean(String md5Hex) { return redisTemplate.hasKey(md5Hex); } } ","link":"https://mask0407.github.io/hadoop03/"},{"title":"Hadoop HA 高可用集群搭建","content":" 环境及准备 zookeeper集群的搭建 HA-HDFS分布式集群搭建 (HDFS分布式集群搭建【高级版】) HDFS集群要保证NameNode的高可用性，为了让NameNode更安全，这里选择用ZooKeeper集群来保证 环境及准备 同上篇普通版 zookeeper集群的搭建 下载并解压zookeeper 在zookeeper根目录下创建data文件夹 进入conf文件夹修改配置 3.1 修改zoo_sample.cfg 名字为 zoo.cfg 3.2 编辑 zoo.cfg dataDir=/opt/install/zookeeper-3.4.5/data server.0=hadoop1.msk.com:2888:3888 server.1=hadoop2.msk.com:2888:3888 server.2=hadoop3.msk.com:2888:3888 在zookeeper/data下创建myid文件 第一台节点myid里面填0 第二台 1 以此类推（三台机器分别为 0,1,2） 用主节点分别ssh免密登录三台机器（包括主节点自身) zookeeper的启停命令 bin/zkServer.sh start | stop | restart | status zookeeper客户端命令 注：在zookeeper的主节点运行 bin/zkCli.sh HA-HDFS分布式集群搭建 如果使用的是以前的普通集群建议先清空data/tmp，如果是新环境可以参考以往基础版集群文章搭建基础环境 配置文件的修改 core-site.xml &lt;!-- 这里的ns随意 只是入口 --&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://ns&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/opt/install/hadoop-2.5.2/data/tmp&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;ha.zookeeper.quorum&lt;/name&gt; &lt;value&gt;hadoop1.msk.com:2181,hadoop2.msk.com:2181,hadoop3.msk.com:2181&lt;/value&gt; &lt;/property&gt; **hdfs-site.xml ** &lt;property&gt; &lt;name&gt;dfs.permissions.enabled&lt;/name&gt; &lt;value&gt;false&lt;/value&gt; &lt;/property&gt; &lt;!--指定hdfs的nameservice为ns，需要和core-site.xml中的保持一致 --&gt; &lt;property&gt; &lt;name&gt;dfs.nameservices&lt;/name&gt; &lt;value&gt;ns&lt;/value&gt; &lt;/property&gt; &lt;!-- ns下面有两个NameNode，分别是nn1，nn2 --&gt; &lt;property&gt; &lt;name&gt;dfs.ha.namenodes.ns&lt;/name&gt; &lt;value&gt;nn1,nn2&lt;/value&gt; &lt;/property&gt; &lt;!-- nn1的RPC通信地址 --&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.rpc-address.ns.nn1&lt;/name&gt; &lt;value&gt;hadoop1.msk.com:8020&lt;/value&gt; &lt;/property&gt; &lt;!-- nn1的http通信地址 --&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.http-address.ns.nn1&lt;/name&gt; &lt;value&gt;hadoop1.msk.com:50070&lt;/value&gt; &lt;/property&gt; &lt;!-- nn2的RPC通信地址 --&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.rpc-address.ns.nn2&lt;/name&gt; &lt;value&gt;hadoop2.msk.com:8020&lt;/value&gt; &lt;/property&gt; &lt;!-- nn2的http通信地址 --&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.http-address.ns.nn2&lt;/name&gt; &lt;value&gt;hadoop2.msk.com:50070&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定NameNode的元数据在JournalNode上的存放位置 --&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.shared.edits.dir&lt;/name&gt; &lt;value&gt;qjournal://hadoop1.msk.com:8485;hadoop2.msk.com:8485;hadoop3.msk.com:8485/ns&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定JournalNode在本地磁盘存放数据的位置 --&gt; &lt;property&gt; &lt;name&gt;dfs.journalnode.edits.dir&lt;/name&gt; &lt;value&gt;/opt/install/hadoop-2.5.2/journal&lt;/value&gt; &lt;/property&gt; &lt;!-- 开启NameNode故障时自动切换 --&gt; &lt;property&gt; &lt;name&gt;dfs.ha.automatic-failover.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;!-- 配置失败自动切换实现方式 --&gt; &lt;property&gt; &lt;name&gt;dfs.client.failover.proxy.provider.ns&lt;/name&gt; &lt;value&gt;org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider&lt;/value&gt; &lt;/property&gt; &lt;!-- 配置隔离机制，如果ssh是默认22端口，value直接写sshfence即可 --&gt; &lt;property&gt; &lt;name&gt;dfs.ha.fencing.methods&lt;/name&gt; &lt;value&gt;sshfence&lt;/value&gt; &lt;/property&gt; &lt;!-- 使用隔离机制时需要ssh免登陆 --&gt; &lt;property&gt; &lt;name&gt;dfs.ha.fencing.ssh.private-key-files&lt;/name&gt; &lt;value&gt;/root/.ssh/id_rsa&lt;/value&gt; &lt;/property&gt; yarn-env.sh export JAVA_HOME=/usr/java/jdk1.7.0_71 首先启动zookeeper集群（三台都要执行启动zookeeper服务指令） 在主NameNode节点格式化zkfc bin/hdfs zkfc -formatZK 在每个journalnode节点用如下命令启动journalnode sbin/hadoop-daemon.sh start journalnode 在主namenode节点格式化namenode和journalnode目录 bin/hdfs namenode -format ns 在主namenode节点启动namenode进程 sbin/hadoop-daemon.sh start namenode 在备namenode节点执行第一行命令，这个是把备namenode节点的目录格式化并把元数据从主namenode节点copy过来，并且这个命令不会把journalnode目录再格式化了！然后用第二个命令启动备namenode进程 bin/hdfs namenode -bootstrapStandby sbin/hadoop-daemon.sh start namenode 在两个namenode节点都执行以下命令 sbin/hadoop-daemon.sh start zkfc 在所有datanode节点都执行以下命令启动datanode sbin/hadoop-daemon.sh start datanode 日常启停命令 sbin/start-dfs.sh sbin/stop-dfs.sh ","link":"https://mask0407.github.io/hadoop02/"},{"title":"Hadoop 基础分布式集群搭建及使用","content":" 环境：以CentOS6.5为例 准备 配置文件修改 配置Hadoop环境变量 NameNode格式化 启动|停止hadoop【在NameNode节点运行】 shell访问HDFS 浏览器访问HDFC (HDFS分布式集群搭建【基础版】) 环境：以CentOS6.5为例 防火墙关闭并关闭自启 selinux设置关闭 设置主机名 设置主机映射 jdk1.7 ssh免密登陆 hadoop-2.5.2 准备 解压hadoop，并在hadoop根目录下创建 data/tmp目录 配置文件修改 配置文件位置：hadoop目录下的etc/hadoop hadoop-env.sh export JAVA_HOME=/usr/java/jdk1.7.0_71 #jdk路径 core-site.xml &lt;!-- 用于设置namenode并且作为Java程序的访问入口 --&gt; &lt;!-- hadoop1.msk.com 为主机名 --&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://hadoop1.msk.com:8020&lt;/value&gt; &lt;/property&gt; &lt;!-- 存储NameNode持久化的数据，DataNode块数据 --&gt; &lt;!-- 手工创建$HADOOP_HOME/data/tmp --&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/opt/install/hadoop-2.5.2/data/tmp&lt;/value&gt; &lt;/property&gt; hdfs-site.xml &lt;!-- 设置副本数量 默认是3 可自行根据需求更改 --&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;3&lt;/value&gt; &lt;/property&gt; &lt;!-- 权限，可省略 --&gt; &lt;property&gt; &lt;name&gt;dfs.permissions.enabled&lt;/name&gt; &lt;value&gt;false&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.http.address&lt;/name&gt; &lt;value&gt;hadoop1.mask.com:50070&lt;/value&gt; &lt;/property&gt; mapred-site.xml &lt;!-- yarn 与 MapReduce相关 --&gt; &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt; yarn-site.xml &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt; slaves 这里配置DataNode的主机名 机器有限，这里节点1即为NameNode也为DataNode hadoop1.msk.com 配置Hadoop环境变量 在环境变量文件/etc/profile加入 export HADOOP_HOME=(你的Hadoop安装路径) export path=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin NameNode格式化 目的作用：格式化hdfs系统，并且生成存储数据块的目录 hadoop namenode -format 启动|停止hadoop【在NameNode节点运行】 sbin/start-all.sh sbin/stop-all.sh 想检验是否启动成功可用jps命令查看进程 shell访问HDFS 查看目录结构 hadoop fs -ls 路径 创建文件夹 hadoop fs -mkdir /a hadoop fs -mkdir -p /a/b 本地上传文件到hdfs中 hadoop fs -put local_path hdfs_path 查看文件内容 hadoop fs dfs -text /a/c hadoop fs dfs -cat /a/c 删除 hadoop fs dfs -rm /a/c 删除非空文件夹 hadoop fs dfs -rmr /a 从hdfs下载文件到本地 hadoop fs -get hdfs_path local_path 浏览器访问HDFC http://hadoop1.msk.com:50070 访问 hdfs http://hadoop1.msk.com:8088 访问 yarn ","link":"https://mask0407.github.io/hadoop01/"},{"title":"Hadoop 伪分布式搭建(超详细)","content":" 虚拟机准备阶段操作 安全设置 防火墙相关指令 关闭关闭selinux IP设置 查看机器IP 修改主机名 修改IP及主机名映射 SSH免密登陆 Hadoop伪分布式搭建 JDK配置 解压 配置环境变量 Hadoop配置 解压文件 修改配置文件 配置Hadoop环境变量 验证环境变量是否配置成功 格式化NameNode Hadoop起停命令 查看WebUI界面 (Hadoop伪分布式搭建) 虚拟机准备阶段操作 本文是基于CentOS 7 系统搭建 相关资源下载 链接:https://pan.baidu.com/s/1FW228OfyURxEgnXW0qqpmA 密码:18uc 安全设置 防火墙相关指令 # 查看防火墙状态 firewall-cmd --state # 停止防火墙 [root@localhost ~]# systemctl stop firewalld.service # 禁止防火墙开机自启 [root@localhost ~]# systemctl disable firewalld.service 关闭关闭selinux [root@localhost ~]# vi /etc/selinux/config 将 SELINUX=enforcing改为 SELINUX=disabled IP设置 查看机器IP [root@localhost ~]# ifconfig 修改主机名 [root@localhost ~]# vi /etc/hostname 修改IP及主机名映射 [root@localhost ~]# vi /etc/hosts SSH免密登陆 [root@localhost ~]# ssh-keygen -t rsa # 生产密钥 # 连续三次回车 # 将密钥发送给需要登陆本机的机器，这里只有一台机器 所以发给自己 [root@localhost ~]# ssh-copy-id root@CentOS # 测试ssh [root@localhost ~]# ssh root@CentOS Hadoop伪分布式搭建 创建 install文件夹 [root@localhost ~]# mkdir /opt/install/ JDK配置 这里选用JDK8 解压 [root@localhost ~]# tar -zxvf jdk-8u144-linux-x64.tar.gz -C /opt/install/ 配置环境变量 [root@localhost jdk1.8.0_144]# vi /etc/profile # 加入配置 加入位置如下图所示 export JAVA_HOME=/opt/install/jdk1.8.0_144 export PATH=$PATH:$JAVA_HOME/bin # 保存后刷新环境变量 [root@localhost jdk1.8.0_144]# source /etc/profile # 刷新完 执行命令验证JDK是否安装成功 [root@localhost jdk1.8.0_144]# java -version 成功界面 Hadoop配置 解压文件 [root@localhost ~]# tar -zxvf hadoop-2.9.2.tar.gz -C /opt/install/ 修改配置文件 [root@localhost ~]# cd /opt/install/hadoop-2.9.2/etc/hadoop hadoop-env.sh export JAVA_HOME=/opt/install/jdk1.8.0_144 core-site.xml &lt;!-- 用于设置namenode并且作为Java程序的访问入口 --&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://CentOS:8020&lt;/value&gt; &lt;/property&gt; &lt;!-- 存储NameNode持久化的数据，DataNode块数据 --&gt; &lt;!-- 手工创建$HADOOP_HOME/data/tmp --&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/opt/install/hadoop-2.9.2/data/tmp&lt;/value&gt; &lt;/property&gt; hdfs-site.xml &lt;!-- 设置副本数量 默认是3 可自行根据需求更改 --&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;3&lt;/value&gt; &lt;/property&gt; &lt;!-- 权限，可省略 --&gt; &lt;property&gt; &lt;name&gt;dfs.permissions.enabled&lt;/name&gt; &lt;value&gt;false&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.http.address&lt;/name&gt; &lt;value&gt;CentOS:50070&lt;/value&gt; &lt;/property&gt; mapred-site.xml 首先拷贝一个mapred-site.xml [root@localhost hadoop]# cp mapred-site.xml.template mapred-site.xml &lt;!-- yarn 与 MapReduce相关 --&gt; &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt; yarn-site.xml &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt; slaves 这里配置DataNode的主机名 伪分布式情况下这里NameNode也充当DataNode CentOS 配置Hadoop环境变量 [root@localhost hadoop-2.9.2]# vim /etc/profile # 加入 export HADOOP_HOME=/opt/install/hadoop-2.9.2 export PATH=$PATH:$JAVA_HOME/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin # 刷新环境变量 [root@localhost hadoop-2.9.2]# source /etc/profile 验证环境变量是否配置成功 [root@localhost hadoop-2.9.2]# hadoop version 格式化NameNode 目的作用：格式化hdfs系统，并且生成存储数据块的目录 [root@localhost hadoop-2.9.2]# hadoop namenode -format 格式化成功后如图显示 Hadoop起停命令 start-all.sh stop-all.sh 启动成后 jps查看进程 查看WebUI界面 http://CentOS:50070 访问 hdfs http://CentOS:8088 访问 yarn ","link":"https://mask0407.github.io/hadoop00/"}]}