<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" >

<title>Spark入门(四)——Spark RDD算子使用方法 | 个人博客</title>

<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.12.0/css/all.min.css">
<link rel="shortcut icon" href="https://mask0407.github.io/favicon.ico?v=1593400910740">
<link rel="stylesheet" href="https://mask0407.github.io/styles/main.css">





<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/aos@2.3.4/dist/aos.css" />
<script src="https://cdn.jsdelivr.net/npm/vue/dist/vue.js"></script>



    <meta name="description" content="Spark RDD算子
RDD算子实战
转换算子
map(function)
传入的集合元素进行RDD[T]转换 def map(f: T =&gt; U): org.apache.spark.rdd.RDD[U]
scala&gt;  s..." />
    <meta name="keywords" content="Spark,大数据" />
  </head>
  <body>
    <div id="app" class="main">

      <div class="sidebar" :class="{ 'full-height': menuVisible }">
  <div class="top-container" data-aos="fade-right">
    <div class="top-header-container">
      <a class="site-title-container" href="https://mask0407.github.io">
        <img src="https://mask0407.github.io/images/avatar.png?v=1593400910740" class="site-logo">
        <h1 class="site-title">个人博客</h1>
      </a>
      <div class="menu-btn" @click="menuVisible = !menuVisible">
        <div class="line"></div>
      </div>
    </div>
    <div>
      
        
          <a href="/" class="site-nav">
            首页
          </a>
        
      
        
          <a href="/archives" class="site-nav">
            归档
          </a>
        
      
        
          <a href="/tags" class="site-nav">
            标签
          </a>
        
      
        
          <a href="/post/about" class="site-nav">
            关于
          </a>
        
      
    </div>
  </div>
  <div class="bottom-container" data-aos="flip-up" data-aos-offset="0">
    <div class="social-container">
      
        
      
        
      
        
      
        
      
        
      
        
      
    </div>
    <div class="site-description">
      温故而知新
    </div>
    <div class="site-footer">
      Powered by <a href="https://github.com/getgridea/gridea" target="_blank">Gridea</a> | <a class="rss" href="https://mask0407.github.io/atom.xml" target="_blank">RSS</a>
    </div>
  </div>
</div>


      <div class="main-container">
        <div class="content-container" data-aos="fade-up">
          <div class="post-detail">
            <h2 class="post-title">Spark入门(四)——Spark RDD算子使用方法</h2>
            <div class="post-date">2020-06-29</div>
            
            <div class="post-content" v-pre>
              <h1 id="spark-rdd算子">Spark RDD算子</h1>
<h2 id="rdd算子实战">RDD算子实战</h2>
<h3 id="转换算子">转换算子</h3>
<h4 id="mapfunction">map(function)</h4>
<p>传入的集合元素进行RDD[T]转换 <code>def map(f: T =&gt; U): org.apache.spark.rdd.RDD[U]</code></p>
<pre><code class="language-java">scala&gt;  sc.parallelize(List(1,2,3,4,5),3).map(item =&gt; item*2+&quot; &quot; )
res1: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[2] at map at &lt;console&gt;:25

scala&gt;  sc.parallelize(List(1,2,3,4,5),3).map(item =&gt; item*2+&quot; &quot; ).collect
res2: Array[String] = Array(&quot;2 &quot;, &quot;4 &quot;, &quot;6 &quot;, &quot;8&quot;, &quot;10 &quot;)
</code></pre>
<h4 id="filterfunc"><strong>filter</strong>(<em>func</em>)</h4>
<p>将满足条件结果记录 <code>def filter(f: T=&gt; Boolean): org.apache.spark.rdd.RDD[T]</code></p>
<pre><code class="language-java">scala&gt;  sc.parallelize(List(1,2,3,4,5),3).filter(item=&gt; item%2==0).collect
res3: Array[Int] = Array(2, 4)
</code></pre>
<p><strong>flatMap</strong>(<em>func</em>)</p>
<p>将一个元素转换成元素的数组，然后对数组展开。<code>def flatMap[U](f: T=&gt; TraversableOnce[U]): org.apache.spark.rdd.RDD[U]</code></p>
<pre><code class="language-java">scala&gt;  sc.parallelize(List(&quot;ni hao&quot;,&quot;hello spark&quot;),3).flatMap(line=&gt;line.split(&quot;\\s+&quot;)).collect
res4: Array[String] = Array(ni, hao, hello, spark)
</code></pre>
<h4 id="mappartitionsfunc"><strong>mapPartitions</strong>(<em>func</em>)</h4>
<p>与map类似，但在RDD的每个分区（块）上单独运行，因此当在类型T的RDD上运行时，func必须是<code>Iterator &lt;T&gt; =&gt; Iterator &lt;U&gt;</code>类型</p>
<p><code>def mapPartitions[U](f: Iterator[Int] =&gt; Iterator[U],preservesPartitioning: Boolean): org.apache.spark.rdd.RDD[U]</code></p>
<pre><code class="language-java">scala&gt;  sc.parallelize(List(1,2,3,4,5),3).mapPartitions(items=&gt; for(i&lt;-items;if(i%2==0)) yield i*2 ).collect()
res7: Array[Int] = Array(4, 8)

</code></pre>
<h4 id="mappartitionswithindexfunc"><strong>mapPartitionsWithIndex</strong>(<em>func</em>)</h4>
<p>与mapPartitions类似，但也为func提供了表示分区索引的整数值，因此当在类型T的RDD上运行时，func必须是类型<code>（Int，Iterator &lt;T&gt;）=&gt; Iterator &lt;U&gt;</code>。</p>
<p><code>def mapPartitionsWithIndex[U](f: (Int, Iterator[T]) =&gt; Iterator[U],preservesPartitioning: Boolean): org.apache.spark.rdd.RDD[U]</code></p>
<pre><code class="language-java">scala&gt;  sc.parallelize(List(1,2,3,4,5),3).mapPartitionsWithIndex((p,items)=&gt; for(i&lt;-items) yield (p,i)).collect
res11: Array[(Int, Int)] = Array((0,1), (1,2), (1,3), (2,4), (2,5))
</code></pre>
<h4 id="samplewithreplacement-fraction-seed"><strong>sample</strong>(<em>withReplacement</em>, <em>fraction</em>, <em>seed</em>)</h4>
<p>对数据进行一定比例的采样，使用withReplacement参数控制是否允许重复采样。</p>
<pre><code class="language-java">def sample(withReplacement: Boolean,fraction: Double,seed: Long): org.apache.spark.rdd.RDD[T]

scala&gt;  sc.parallelize(List(1,2,3,4,5,6,7),3).sample(false,0.7,1L).collect
res13: Array[Int] = Array(1, 4, 6, 7)
</code></pre>
<h4 id="unionotherdataset"><strong>union</strong>(<em>otherDataset</em>)</h4>
<p>返回一个新数据集，其中包含源数据集和参数中元素的并集。</p>
<p><code>def union(other: org.apache.spark.rdd.RDD[T]): org.apache.spark.rdd.RDD[T]</code></p>
<pre><code class="language-java">scala&gt; var rdd1=sc.parallelize(Array((&quot;张三&quot;,1000),(&quot;李四&quot;,100),(&quot;赵六&quot;,300)))
scala&gt; var rdd2=sc.parallelize(Array((&quot;张三&quot;,1000),(&quot;王五&quot;,100),(&quot;温七&quot;,300)))
scala&gt; rdd1.union(rdd2).collect
res16: Array[(String, Int)] = Array((张三,1000), (李四,100), (赵六,300), (张三,1000), (王五,100), (温七,300))
</code></pre>
<h4 id="intersectionotherdataset"><strong>intersection</strong>(<em>otherDataset</em>)</h4>
<p>返回包含源数据集和参数中元素交集的新RDD。</p>
<p><code>def intersection(other: org.apache.spark.rdd.RDD[T],numPartitions: Int): org.apache.spark.rdd.RDD[T]</code></p>
<pre><code class="language-java">scala&gt; var rdd1=sc.parallelize(Array((&quot;张三&quot;,1000),(&quot;李四&quot;,100),(&quot;赵六&quot;,300)))
scala&gt; var rdd2=sc.parallelize(Array((&quot;张三&quot;,1000),(&quot;王五&quot;,100),(&quot;温七&quot;,300)))
scala&gt; rdd1.intersection(rdd2).collect
res17: Array[(String, Int)] = Array((张三,1000))

</code></pre>
<h4 id="distinctnumpartitions"><strong>distinct</strong>([<em>numPartitions</em>]))</h4>
<p>返回包含源数据集的不同元素的新数据集。</p>
<pre><code class="language-java">scala&gt;  sc.parallelize(List(1,2,3,3,5,7,2),3).distinct.collect
res19: Array[Int] = Array(3, 1, 7, 5, 2)
</code></pre>
<h4 id="groupbykeynumpartitions"><strong>groupByKey</strong>([<em>numPartitions</em>])</h4>
<p>在（K，V）对的数据集上调用时，返回<code>（K，Iterable &lt;V&gt;）</code>对的数据集。 注意：如果要对每个键执行聚合（例如总和或平均值）进行分组，则使用reduceByKey或aggregateByKey将产生更好的性能。 注意：默认情况下，输出中的并行级别取决于父RDD的分区数。您可以传递可选的numPartitions参数来设置不同数量的任务。</p>
<pre><code class="language-java">scala&gt;  sc.parallelize(List(&quot;ni hao&quot;,&quot;hello spark&quot;),3).flatMap(line=&gt;line.split(&quot;\\s+&quot;)).map(word=&gt;(word,1)).groupByKey(3).map(tuple=&gt;(tuple._1,tuple._2.sum)).collect
</code></pre>
<h4 id="reducebykeyfunc-numpartitions"><strong>reduceByKey</strong>(<em>func</em>, [<em>numPartitions</em>])</h4>
<p>当调用（K，V）对的数据集时，返回（K，V）对的数据集，其中使用给定的reduce函数func聚合每个键的值，该函数必须是类型（V，V）=&gt; V.</p>
<pre><code class="language-java">scala&gt;  sc.parallelize(List(&quot;ni hao&quot;,&quot;hello spark&quot;),3).flatMap(line=&gt;line.split(&quot;\\s+&quot;)).map(word=&gt;(word,1)).reduceByKey((v1,v2)=&gt;v1+v2).collect()
res33: Array[(String, Int)] = Array((hao,1), (hello,1), (spark,1), (ni,1))

scala&gt;  sc.parallelize(List(&quot;ni hao&quot;,&quot;hello spark&quot;),3).flatMap(line=&gt;line.split(&quot;\\s+&quot;)).map(word=&gt;(word,1)).reduceByKey(_+_).collect()
res34: Array[(String, Int)] = Array((hao,1), (hello,1), (spark,1), (ni,1))
</code></pre>
<h4 id="aggregatebykeyzerovalueseqop-combop-numpartitions"><strong>aggregateByKey</strong>(<em>zeroValue</em>)(<em>seqOp</em>, <em>combOp</em>, [<em>numPartitions</em>])</h4>
<p>当调用（K，V）对的数据集时，返回（K，U）对的数据集，其中使用给定的组合函数和中性“零”值聚合每个键的值。允许与输入值类型不同的聚合值类型，同时避免不必要的分配。</p>
<pre><code class="language-java">scala&gt;  sc.parallelize(List(&quot;ni hao&quot;,&quot;hello spark&quot;),3).flatMap(line=&gt;line.split(&quot;\\s+&quot;)).map(word=&gt;(word,1)).aggregateByKey(0L)((z,v)=&gt;z+v,(u1,u2)=&gt;u1+u2).collect
res35: Array[(String, Long)] = Array((hao,1), (hello,1), (spark,1), (ni,1))
</code></pre>
<h4 id="sortbykeyascending-numpartitions"><strong>sortByKey</strong>([<em>ascending</em>], [<em>numPartitions</em>])</h4>
<p>当调用K实现Ordered的（K，V）对数据集时，返回按键升序或降序排序的（K，V）对数据集，如布尔升序参数中所指定。</p>
<pre><code class="language-java">scala&gt;  sc.parallelize(List(&quot;ni hao&quot;,&quot;hello spark&quot;),3).flatMap(line=&gt;line.split(&quot;\\s+&quot;)).map(word=&gt;(word,1)).aggregateByKey(0L)((z,v)=&gt;z+v,(u1,u2)=&gt;u1+u2).sortByKey(false).collect()
res37: Array[(String, Long)] = Array((spark,1), (ni,1), (hello,1), (hao,1))

</code></pre>
<h4 id="sortbyfuncascending-numpartitions"><strong>sortBy</strong>(func,[<em>ascending</em>], [<em>numPartitions</em>])**</h4>
<p>对（K，V）数据集调用sortBy时，用户可以通过指定func指定排序规则，T =&gt; U 要求U必须实现Ordered接口</p>
<pre><code class="language-java">scala&gt;  sc.parallelize(List(&quot;ni hao&quot;,&quot;hello spark&quot;),3).flatMap(line=&gt;line.split(&quot;\\s+&quot;)).map(word=&gt;(word,1)).aggregateByKey(0L)((z,v)=&gt;z+v,(u1,u2)=&gt;u1+u2).sortBy(_._2,true,2).collect
res42: Array[(String, Long)] = Array((hao,1), (hello,1), (spark,1), (ni,1))
</code></pre>
<h4 id="join">join</h4>
<p>当调用类型（K，V）和（K，W）的数据集时，返回（K，（V，W））对的数据集以及每个键的所有元素对。通过leftOuterJoin，rightOuterJoin和fullOuterJoin支持外连接。</p>
<pre><code class="language-java">scala&gt; var rdd1=sc.parallelize(Array((&quot;001&quot;,&quot;张三&quot;),(&quot;002&quot;,&quot;李四&quot;),(&quot;003&quot;,&quot;王五&quot;)))
scala&gt; var rdd2=sc.parallelize(Array((&quot;001&quot;,(&quot;apple&quot;,18.0)),(&quot;001&quot;,(&quot;orange&quot;,18.0))))
scala&gt; rdd1.join(rdd2).collect
res43: Array[(String, (String, (String, Double)))] = Array((001,(张三,(apple,18.0))), (001,(张三,(orange,18.0))))

</code></pre>
<h4 id="cogroup">cogroup</h4>
<p>当调用类型（K，V）和（K，W）的数据集时，返回（K，（Iterable ，Iterable ））元组的数据集。此操作也称为groupWith。</p>
<pre><code class="language-java">scala&gt; var rdd1=sc.parallelize(Array((&quot;001&quot;,&quot;张三&quot;),(&quot;002&quot;,&quot;李四&quot;),(&quot;003&quot;,&quot;王五&quot;)))
scala&gt; var rdd2=sc.parallelize(Array((&quot;001&quot;,&quot;apple&quot;),(&quot;001&quot;,&quot;orange&quot;),(&quot;002&quot;,&quot;book&quot;)))
scala&gt; rdd1.cogroup(rdd2).collect()
res46: Array[(String, (Iterable[String], Iterable[String]))] = Array((001,(CompactBuffer(张三),CompactBuffer(apple, orange))), (002,(CompactBuffer(李四),CompactBuffer(book))), (003,(CompactBuffer(王五),CompactBuffer())))
</code></pre>
<h4 id="cartesian">cartesian</h4>
<p>当调用类型为T和U的数据集时，返回（T，U）对的数据集（所有元素对）。</p>
<pre><code class="language-java">scala&gt; var rdd1=sc.parallelize(List(&quot;a&quot;,&quot;b&quot;,&quot;c&quot;))
scala&gt; var rdd2=sc.parallelize(List(1,2,3,4))
scala&gt; rdd1.cartesian(rdd2).collect()
res47: Array[(String, Int)] = Array((a,1), (a,2), (a,3), (a,4), (b,1), (b,2), (b,3), (b,4), (c,1), (c,2), (c,3), (c,4))
</code></pre>
<h4 id="coalescenumpartitions">coalesce(numPartitions)</h4>
<p>将RDD中的分区数减少为numPartitions。过滤大型数据集后，可以使用概算子减少分区数。</p>
<pre><code class="language-java">scala&gt;  sc.parallelize(List(&quot;ni hao&quot;,&quot;hello spark&quot;),3).coalesce(1).partitions.length
res50: Int = 1

scala&gt;  sc.parallelize(List(&quot;ni hao&quot;,&quot;hello spark&quot;),3).coalesce(1).getNumPartitions
res51: Int = 1

</code></pre>
<h4 id="repartition">repartition</h4>
<p>随机重新调整RDD中的数据以创建更多或更少的分区。</p>
<pre><code class="language-java">scala&gt; sc.parallelize(List(&quot;a&quot;,&quot;b&quot;,&quot;c&quot;),3).mapPartitionsWithIndex((index,values)=&gt;for(i&lt;-values) yield (index,i) ).collect
res52: Array[(Int, String)] = Array((0,a), (1,b), (2,c))

scala&gt; sc.parallelize(List(&quot;a&quot;,&quot;b&quot;,&quot;c&quot;),3).repartition(2).mapPartitionsWithIndex((index,values)=&gt;for(i&lt;-values) yield (index,i) ).collect
res53: Array[(Int, String)] = Array((0,a), (0,c), (1,b))
</code></pre>
<h3 id="动作算子">动作算子</h3>
<h4 id="collect">collect</h4>
<p>用在测试环境下，通常使用collect算子将远程计算的结果拿到Drvier端，注意一般数据量比较小，用于测试。</p>
<pre><code class="language-java">scala&gt; var rdd1=sc.parallelize(List(1,2,3,4,5),3).collect().foreach(println)
</code></pre>
<h4 id="saveastextfile">saveAsTextFile</h4>
<p>将计算结果存储在文件系统中，一般存储在HDFS上</p>
<pre><code class="language-java">scala&gt;  sc.parallelize(List(&quot;ni hao&quot;,&quot;hello spark&quot;),3).flatMap(_.split(&quot;\\s+&quot;)).map((_,1)).reduceByKey(_+_).sortBy(_._2,false,3).saveAsTextFile(&quot;hdfs:///wordcounts&quot;)
</code></pre>
<h4 id="foreach">foreach</h4>
<p>迭代遍历所有的RDD中的元素，通常是将foreach传递的数据写到外围系统中，比如说可以将数据写入到Hbase中。</p>
<p>scala&gt;  sc.parallelize(List(&quot;ni hao&quot;,&quot;hello spark&quot;),3).flatMap(<em>.split(&quot;\s+&quot;)).map((</em>,1)).reduceByKey(<em>+</em>).sortBy(_._2,false,3).foreach(println)<br>
(hao,1)<br>
(hello,1)<br>
(spark,1)<br>
(ni,1)</p>
<blockquote>
<p>注意如果使用以上代码写数据到外围系统，会因为不断创建和关闭连接影响写入效率，一般推荐使用foreachPartition</p>
</blockquote>
<pre><code class="language-java">val lineRDD: RDD[String] = sc.textFile(&quot;file:///E:/demo/words/t_word.txt&quot;)
lineRDD.flatMap(line=&gt;line.split(&quot; &quot;))
    .map(word=&gt;(word,1))
    .groupByKey()
    .map(tuple=&gt;(tuple._1,tuple._2.sum))
    .sortBy(tuple=&gt;tuple._2,false,3)
    .foreachPartition(items=&gt;{
        //创建连接
        items.foreach(t=&gt;println(&quot;存储到数据库&quot;+t))
        //关闭连接
    })
</code></pre>
<h3 id="共享变量">共享变量</h3>
<h4 id="变量广播">变量广播</h4>
<p>通常情况下，当一个RDD的很多操作都需要使用driver中定义的变量时，每次操作，driver都要把变量发送给worker节点一次，如果这个变量中的数据很大的话，会产生很高的传输负载，导致执行效率降低。使用广播变量可以使程序高效地将一个很大的<code>只读</code>数据发送给多个worker节点，而且对每个worker节点只需要传输一次，每次操作时executor可以直接获取本地保存的数据副本，不需要多次传输。</p>
<pre><code class="language-java">val conf = new SparkConf().setAppName(&quot;demo&quot;).setMaster(&quot;local[2]&quot;)
val sc = new SparkContext(conf)

val userList = List(
    &quot;001,张三,28,0&quot;,
    &quot;002,李四,18,1&quot;,
    &quot;003,王五,38,0&quot;,
    &quot;004,zhaoliu,38,-1&quot;
)
val genderMap = Map(&quot;0&quot; -&gt; &quot;女&quot;, &quot;1&quot; -&gt; &quot;男&quot;)
val bcMap = sc.broadcast(genderMap)

sc.parallelize(userList,3)
.map(info=&gt;{
    val prefix = info.substring(0, info.lastIndexOf(&quot;,&quot;))
    val gender = info.substring(info.lastIndexOf(&quot;,&quot;) + 1)
    val genderMapValue = bcMap.value
    val newGender = genderMapValue.getOrElse(gender, &quot;未知&quot;)
    prefix + &quot;,&quot; + newGender
}).collect().foreach(println)

sc.stop() 
</code></pre>
<h4 id="累加器">累加器</h4>
<p>Spark提供的Accumulator，主要用于多个节点对一个变量进行共享性的操作。Accumulator只提供了累加的功能。但是确给我们提供了多个task对一个变量并行操作的功能。但是task只能对Accumulator进行累加操作，不能读取它的值。只有Driver程序可以读取Accumulator的值。</p>
<pre><code class="language-java">scala&gt; var count=sc.longAccumulator(&quot;count&quot;)
scala&gt; sc.parallelize(List(1,2,3,4,5,6),3).foreach(item=&gt; count.add(item))
scala&gt; count.value
res1: Long = 21
</code></pre>

            </div>
            
              <div class="tag-container">
                
                  <a href="https://mask0407.github.io/aAuqPKSPG/" class="tag">
                    Spark
                  </a>
                
                  <a href="https://mask0407.github.io/1ffDVERZml/" class="tag">
                    大数据
                  </a>
                
              </div>
            
            
              <div class="next-post">
                <div class="next">下一篇</div>
                <a href="https://mask0407.github.io/spark02/">
                  <h3 class="post-title">
                    Spark入门(三)——SparkRDD剖析(面试点)
                  </h3>
                </a>
              </div>
            

            

          </div>

        </div>
      </div>
    </div>

    <script src="https://cdn.jsdelivr.net/npm/aos@2.3.4/dist/aos.min.js"></script>
<script type="application/javascript">

AOS.init();

var app = new Vue({
  el: '#app',
  data: {
    menuVisible: false,
  },
})

</script>






  </body>
</html>
